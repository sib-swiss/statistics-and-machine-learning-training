{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content <a id='toc'></a>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2. Maximum Likelihood](#0)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[The link between OLS method and Maximum likelihood](#1.1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3. Generalized linear model](#1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Practical case 1 : Count of Galumna mites among moss samples](#3.1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Modeling process of the GLM](#3.3)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Poisson model GLM on the mites data](#3.4)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Practical case 2 : Relation between the presence of  kyphosis and and a few covariates](#3.2)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Bernouilli model on the kyphosis data (logistic regression)](#3.5)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[model testing with GLMs](#3.6)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Predicting kyphosis from our model](#3.7)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise on GLM :  a dose-response problem](#2)\n",
    "\n",
    "[Annex : is the Pearson chi2 adapted for the mites data](#A0)\n",
    "\n",
    "[Annex : Additionnal Practical case : exponential growth of Corona virus infection](#A1)\n",
    "\n",
    "[Annex : is the Pearson chi2 adapted for the infection data](#A2)\n",
    "\n",
    "[Annex : trying a negative binomial model on the infection data](#A3)\n",
    "\n",
    "[Annex : compute the confidence interval in GLMs - example with a gaussian distribution](#confint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a couple more configuration of the plotting engine\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 4, 4\n",
    "plt.rc(\"font\", size=10)\n",
    "\n",
    "\n",
    "plt.rc('xtick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('xtick.major', size=8, pad=12)\n",
    "plt.rc('xtick.minor', size=8, pad=12)\n",
    "\n",
    "plt.rc('ytick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('ytick.major', size=8, pad=12)\n",
    "plt.rc('ytick.minor', size=8, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before  we can delve into the large topic of **Generalized Linear Models**, we are going to introduce the more general framework of **Maximum Likelihood** upon which rely most of the fitting problems of GLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# 2. Maximum Likelihood <a id='0'></a>\n",
    "\n",
    "Maximum Likelihood is a method that is used to estimate parameters of a probablililty distribution. It is done by **maximizing the likelihood function**. In the case that we are interested in (i.e. independant identically distributed) this likelihood function is simply the product of  a density function values over the entire sample. **It is a parametric method** since it needs to have an a priori about the density function for it to work. \n",
    "\n",
    "Since it is a product, most of the time we would rather work with the log likelihood function which transforms this product into a sum.\n",
    "\n",
    "So we would like to maximize $l$, the loglikelihood function, by choosing a set of parameters $\\pmb\\Theta$.\n",
    "Where $l$ is of the form:\n",
    "\n",
    "$$l(\\pmb\\Theta;X)=\\sum_i ln(p(x_i|\\pmb\\Theta))$$\n",
    "\n",
    "Where $X$ is a random variable and $p()$ is the density function associated to $X$. So you want to find the following estimation for $\\pmb\\Theta$\n",
    "\n",
    "$$\\hat{\\pmb\\Theta}=argmax_{\\pmb\\Theta} l(\\pmb\\Theta;X)$$\n",
    "\n",
    "> Note : this formulation of the likelihood relies on the Bayes theorem. It puts forward that the likelihood of parameters given the observed data depends on the probability of the observed data given the parameters. Thus, in ML we search the parameters for which the observed data seems the most probable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the example of a gaussian where you would like to estimate the $\\sigma$ and the $\\mu$, given your data. As they are simulated data we chose that $\\mu=2$ and $\\sigma=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_small=np.random.randn(10)*0.5+2 # this is our observed data, with ( mean=2 , sd=0.5 )\n",
    "\n",
    "m=[2,0.5] # we will try 2 possible combinations of paramters ( mean=2 , sd=0.5 ) and ( mean=0.5 , sd=0.5 ) \n",
    "s=[0.5,0.5]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(m) , figsize = (14,7))\n",
    "\n",
    "X_small_=[[v,0] for v in X_small] # this will be used to plot segments\n",
    "\n",
    "x=np.arange(-2,4,0.005) # we will plot between -2 and 4\n",
    "\n",
    "for q in range(len(m)): # for each of the parameter combinations we want to try\n",
    "    ax[q].plot(X_small,[0]*len(X_small),marker='+', linewidth=0 , markersize = 15) # we plot the observed data as crosses\n",
    "\n",
    "    # we plot the distribution we are testing\n",
    "    ax[q].plot( x , stats.norm.pdf( x , loc = m[q] , scale = s[q] ),'k') \n",
    "    \n",
    "    Predicted = stats.norm.pdf( X_small , loc = m[q] , scale = s[q] )\n",
    "\n",
    "    Predicted_= [i for i in zip(X_small,Predicted)] # this is to plot segments\n",
    "    lc = mc.LineCollection(zip(X_small_,Predicted_) , \n",
    "                           colors='red',linewidths=2,alpha=0.7,\n",
    "                           label='Predicted likelihood')\n",
    "    ax[q].add_collection(lc)\n",
    "    ax[q].legend(loc='best',fontsize=10)\n",
    "    \n",
    "    # the log likelihood of this set of parameters is the sum of the log of the probability \n",
    "    # densities of the sample\n",
    "    sum_like=sum(np.log(Predicted))     \n",
    "    ax[q].set_title('$\\mu$ : {} - $\\sigma$: {:.2f} - log likelihood : {:.2f}'.format(m[q],\n",
    "                                                                                     s[q],\n",
    "                                                                                     sum_like) ,\n",
    "                    fontsize=13)\n",
    "    \n",
    "    ax[q].set_xlabel('X')\n",
    "    ax[q].set_ylabel('Likelihood')\n",
    "    \n",
    "    ## setting the yscale to log can make sense here\n",
    "    #ax[q].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying those red bars is exactly what the maximum likelihood does. \n",
    "\n",
    "Basically, you shift your theoritical distribution to the right or the left (trying different means), and you narrow it or widen it (trying different variances). \n",
    "\n",
    "For each of those try you multiply those red bars together, and the combination of parameters giving highest result is the one maximizing the likelihood of your data being produced by that distribution with those parameters.\n",
    "\n",
    "\n",
    "It is important to point out here that **even when our data are actually coming from a certain distribution, there will (almost) always be a difference between the theoretical distribution and the recovered one**, as to have perfect match you would need an infinite number of data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=np.random.randn(400)*0.5+2\n",
    "fig = plt.figure(figsize = (10,7)) \n",
    "sns.kdeplot(X,label='data probability\\ndensity function')\n",
    "x=np.arange(0,4,0.005)\n",
    "plt.plot(X, np.random.rand(len(X))*-0.1 ,'k+',label='data')\n",
    "plt.plot(x, stats.norm.pdf( x , loc = 2 , scale = 0.5 ) ,'r',label='generative probability\\ndensity function')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.xlabel('X')\n",
    "plt.legend(loc='best',fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test many combinations of possible means and standard deviations to see where our maximum of likelihood lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "def loglike_func(X,mu,sigma):\n",
    "    \"\"\"returns a loglikelihood of mu and sigma given data X\"\"\"\n",
    "    ll = sum( np.log(stats.norm.pdf(X,mu,sigma) ) ) \n",
    "    if np.isnan(ll) or ll < -10**4: \n",
    "        ll = -10**4 # we verify that no numerical error gave us an NaN or very small log value\n",
    "    return ll\n",
    "\n",
    "mu=np.linspace(0,4,250) # from 0 to 4\n",
    "sigma=np.linspace(0.1,2.1,250) # from 0.1 to 2.1\n",
    "\n",
    "mu,sigma=np.meshgrid(mu,sigma) # this useful function combines all possibles values for mu and sigma\n",
    "\n",
    "loglike_func_V = np.vectorize( lambda mu,sigma : loglike_func(X,mu,sigma))\n",
    "\n",
    "# we compute the log-likelihood for all tested parameters values \n",
    "zs= loglike_func_V(np.ravel( mu ), np.ravel( sigma ) )\n",
    "\n",
    "loglike=zs.reshape(mu.shape)\n",
    "\n",
    "bestMu = np.ravel(mu)[np.argmax(zs)]\n",
    "bestSigma = np.ravel(sigma)[np.argmax(zs)]\n",
    "bestLogLikelihood = np.max(zs)\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(x=mu,y=sigma,z=loglike),\n",
    "                      go.Scatter3d(x=[bestMu],y=[bestSigma],z=[bestLogLikelihood])])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely enought this maximum likelihood method allows us to recover the true parameters of the distribution.\n",
    "\n",
    "\n",
    "Let us look at the curvature of the loglikelihood space as it is bearing useful information. For that we are going to look at how the log likelihood function behave in function of $\\mu$ when $\\sigma=0.5$. Same for the behaviour of the likelihood function according to $\\sigma$ when $\\mu=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=len(m) , figsize = (14,7))\n",
    "mu=np.arange(0,4,0.1)\n",
    "sigma=np.arange(0.1,2.1,0.1)\n",
    "\n",
    "for q in range(2): \n",
    "    if q==0: # different values of mu for sigma=0.5\n",
    "        ax[q].plot(mu,[loglike_func(X,mu=v,sigma=0.5) for v in mu])\n",
    "        ax[q].plot([2],[loglike_func(X,mu=2,sigma=0.5)],'ro')\n",
    "        ax[q].set_xlabel('$\\mu$')\n",
    "        ax[q].set_ylabel('Log likelihood')\n",
    "        ax[q].set_title('log likelihood when $\\sigma=0.5$')\n",
    "    if q==1: # different values of sigma for mu=2\n",
    "        ax[q].plot(sigma,[loglike_func(X,sigma=v,mu=2) for v in sigma])\n",
    "        ax[q].plot([0.5],[loglike_func(X,sigma=0.5,mu=2)],'ro')\n",
    "        ax[q].set_xlabel('$\\sigma$')\n",
    "        ax[q].set_ylabel('Log likelihood')\n",
    "        ax[q].set_title('log likelihood when $\\mu=2$')\n",
    "    ax[q].set_ylim([-1000,0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would to draw a little bit of attention on the curvature of the loglikelihood function here. You can see here that the way the loglikelihood drops around the red dot is quite different for $\\mu$ and $\\sigma$. For $\\mu$ the steepness of the slopes around the maximum are important an well defined, whereas for $\\sigma$ it is way flatter. This has a direct consequence on how narrow your confidence interval around the estimated parameters are going to be. \n",
    "\n",
    "You can see that as the log likelihood function is flatter, there is a wider range of $\\sigma$ that have a loglikelihood equivalent to the maximum.\n",
    "\n",
    "I was talking about the \"steepness of the slopes\" before. In mathematical terms this corresponds to the second derivative of the loglikelihood function. The bigger this second derivative is the steeper are the slopes. You can put all those second derivatives in a matrix called the hessian matrix. In our case : \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial^2l}{\\partial\\mu^2} & \\frac{\\partial^2l}{\\partial\\mu\\partial\\sigma} \\\\\n",
    "    \\frac{\\partial^2l}{\\partial\\sigma\\partial\\mu} & \\frac{\\partial^2l}{\\partial\\sigma^2}\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You can have a geometrical intepretation of the confidence interval by seeing that it is related to the inverse of this second derivative or the inverse of that hessian matrix. This inverse is called radius of curvature, and the bigger the second derivative is going to be, the smaller that radius of curvature is going to be. \n",
    "\n",
    "In other words, when the highest value really pops up from the ones around it in the likelihood landscape, then the confidence interval around the best values of the parameters is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to ToC](#toc)\n",
    "\n",
    "## What is the link between OLS method and Maximum likelihood? What is the distribution we are trying to fit in the case of a regression? <a id='1.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now imagine that we try to fit the average of a Y, $\\bar{Y}$, along the curve $\\bar{Y}=\\beta X+c$ for which the noise around those averages is gaussian. \n",
    "\n",
    "Since we didn't put the noise in this equality, it really represents a fit of the average of Y with some gaussian noise around it. The equation representing the fitting of Y would be $Y=\\beta X+c+\\epsilon$. \n",
    "\n",
    "We could thus consider that we can switch to the following problem of distribution fitting, defined by the probability density function of a normal law $N(\\bar{y} , \\sigma)$:\n",
    "\n",
    "$$p(y_i|\\bar{y_i},\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}*\\exp(-\\frac{1}{2}\\frac{(y_i-\\bar{y_i})^2}{\\sigma^2})$$\n",
    "\n",
    "Note that the parameters you want to estimate are $\\bar{y_i}$ and $\\sigma$.\n",
    "\n",
    "By definition of the  likelihood function over $n$ individuals in a sample is:\n",
    "\n",
    "$$\\Pi_i \\frac{1}{\\sqrt{2\\pi\\sigma^2}}*\\exp(-\\frac{1}{2}\\frac{(y_i-\\bar{y_i})^2}{\\sigma^2})$$\n",
    "\n",
    "which transformed into the loglikelihood function:\n",
    "\n",
    "$$l(\\bar{y_i},\\sigma | Y) = \\sum_i -\\frac{1}{2}\\frac{(y_i-\\bar{y_i})^2}{\\sigma^2} + constant = -\\frac{1}{2 \\sigma^2} \\sum_i (y_i-\\bar{y_i})^2 + constant$$\n",
    "\n",
    "And we try to find the maximum of this expression.\n",
    "\n",
    "Now, in the frame of a linear model, we have $$\\bar{y_i}=\\beta x_i+c$$\n",
    "\n",
    "And in least square, we try to find the minimum of :\n",
    "$$\\sum_i (y_i- \\beta x_i+c )^2 = \\sum_i (y_i-\\bar{y_i})^2 $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You see now that maximizing $\\sum_i -(y_i-\\bar{y_i})^2$ over $\\beta$ (ML) is the same as minimizing $\\sum_i (y_i-\\bar{y_i})^2$ over $\\beta$ (OLS).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# 3. Generalized linear model <a id='1'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, previously we saw that under some hypothesis we could use the Oridnary Least Square method to fit a curve. We also saw that if we knew that the errors were normally distributed then this OLS method was equivalent to using another method called Maximum Likelihood. We also mentioned that if the OLS hypothesis were not verified, then we could use what is called a Generalized Linear Model. As you will see this GLM also rely on the Maximum Likelihood method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see a typical cases where our OLS is not going to work, and where I am pretty sure you will have some ideas about the underlying model we choose use instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Practical case 1 : Count of Galumna mites among moss samples <a id='3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset we look at 70 observations of moss and mite samples collected at the [Station de Biologie from the Université de Montréal](https://www.google.com/maps/place/Station+de+biologie+des+Laurentides/@45.9635944,-74.0372659,12.75z/data=!4m5!3m4!1s0x4ccf36477ea47e51:0x6a37c160e959433e!8m2!3d45.9871434!4d-74.0053923?shorturl=1), within the municipality of Saint-Hippolyte, Québec (Canada). \n",
    "\n",
    "Each sample includes 5 variables of environmental measurements and abundance for Galumna species for each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mites = pd.read_csv(\"data/mites.csv\")\n",
    "mites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different kind of Response variables:\n",
    " * **Galumna** : number of Galumna individuals in the sample\n",
    " * **pa** : presence/absence of Galumna\n",
    " * **totalabund** : total number of individuals\n",
    " * **prop** : fraction of individuals that are Galumna\n",
    " \n",
    "and a number of Predictive variables:\n",
    " * **SubsDens** : Substrate Density\n",
    " * **WatrCont** : Water Content\n",
    " * **Substrate** : Substrate type\n",
    " * **Shrub** : Amount of shrubs nearby\n",
    " * **Topo** : Topography\n",
    "\n",
    "We will focus only on the count (`Galumna`) and water content (`WatrCont`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( mites , y = 'Galumna' , x = \"WatrCont\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could go ahead and try our usual OLS approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = smf.ols(\"Galumna ~ WatrCont\" , data = mites)# we create the least square fit object\n",
    "model = model.fit()#we do the actual fit\n",
    "\n",
    "print( model.summary() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( mites , y = 'Galumna' , x = \"WatrCont\" )\n",
    "\n",
    "x = np.linspace(mites.WatrCont.min() , mites.WatrCont.max() , 100 )\n",
    "plt.plot( x ,  model.predict( {\"WatrCont\" : x } ) , color = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**micro-exercise: what are the problems there ?** (no need to draw additionnal plots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach could be to **transform y** before fitting.\n",
    "\n",
    "A classical transformation would be to compute the logarithm of y.\n",
    "\n",
    "Here we have many values at 0, so we typically do something like log(1+y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mites[\"logGalumna\"] = np.log( 1 + mites.Galumna )\n",
    "sns.scatterplot( mites , y = 'logGalumna' , x = \"WatrCont\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we would model would not be :\n",
    "\n",
    "$$ y = \\beta X $$\n",
    "\n",
    "but rather, (ignoring the +1 for now):\n",
    "\n",
    "$$ log(y) = \\beta_0 + \\beta X $$\n",
    "\n",
    "Which is equivalent to:\n",
    "\n",
    "$$ y =   e^{\\beta_0} * e^{\\beta X} $$\n",
    "\n",
    "$e^{\\beta_0}$ is a fixed term and can be noted $y_0$:\n",
    "\n",
    "$$ y =   y_0 * e^{\\beta X} $$\n",
    "\n",
    "\n",
    "So you can see that by transforing our response variable we have drastically changed the nature of our model.\n",
    "\n",
    "\n",
    "> with the +1 accounted for, we have : $ y =   y_0 * e^{\\beta X} - 1 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\"logGalumna ~ WatrCont\" , data = mites)# we create the least square fit object\n",
    "model = model.fit()#we do the actual fit\n",
    "\n",
    "print( model.summary() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( mites , y = 'Galumna' , x = \"WatrCont\" )\n",
    "x = np.linspace(mites.WatrCont.min() , mites.WatrCont.max() , 100 )\n",
    "log_y_pred = model.predict( {\"WatrCont\" : x } )\n",
    "y_pred = np.exp( log_y_pred ) - 1 ## transform back the data \n",
    "plt.plot( x , y_pred , color = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see it is a bit better but there are still outstanding issues.\n",
    "\n",
    "\n",
    "But as we will see, by **transforming the response variable** we have already taken a big step toward GLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## Let's define what is the modeling process behind GLM <a id = '3.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember, to use our Maximum Likelihood Estimator method, we spent a lot of time before saying that we were looking to solve a very specific kind of problem : one where we wanted to fit the mean of $\\pmb Y_i$ knowing some value $\\pmb X_i$ , and that there was some gaussian noise $\\mathcal{N}(0,\\sigma^2)$ around the mean of $\\pmb Y_i$.\n",
    "\n",
    "In mathematical term we were modeling our problem with the following distribution:\n",
    "\n",
    "$$P(\\pmb Y_i|\\pmb X_i)=\\mathcal{N}(\\pmb X_i^{T}\\pmb\\beta,\\sigma^2)$$\n",
    "\n",
    "On which we were using Maximum Likelihood Estimation \n",
    "\n",
    "Thus so far, we have restricted ourselves to purely straightforward linear relations between the mean of $\\pmb Y_i$ conditional to $\\pmb X_i$ i.e.:\n",
    "\n",
    "$$\\mu(\\pmb Y_i)=\\mathbb{E}(\\pmb Y_i|\\pmb X_i)=(\\pmb X_i^{T}\\pmb\\beta)$$\n",
    "\n",
    "\n",
    "We have also only looked at case where the noise was normaly distributed : hence the $\\mathcal{N}$ above. \n",
    "\n",
    "Under some conditions (i.e your random component is part of the exponential distribution family) you can relax those conditions.\n",
    "\n",
    "Remember, as we want to use linear models the only thing we can not change is our fitting using $\\pmb X_i^{T}\\pmb\\beta$. But this linear relationship doesn't have to apply to the conditional mean $\\mu(\\pmb X_i)$ but instead to a function of that conditional mean:\n",
    "\n",
    "$$g(\\mu(\\pmb Y_i))=\\pmb X_i^{T}\\pmb\\beta$$\n",
    "\n",
    "**$g$ is called the link function**, and we will investigate a little bit more what it can be and why it is useful. \n",
    "\n",
    "\n",
    "Another possibility is that rather than a normal distribution we could also look for a more general distribution to model the noise induced by the interaction between our $\\pmb Y_i$ and $\\pmb X_i$:\n",
    "\n",
    "$$P(\\pmb Y_i|\\pmb X_i)=\\mathcal{D}(g^{-1}(\\pmb X_i^{T}\\pmb\\beta),\\sigma^2)$$\n",
    "\n",
    "Where $\\mathcal{D}$ is a distribution that makes sense to model our $\\pmb Y$ random variable, and $g^{-1}$ the inverse of the link function.\n",
    "\n",
    "And on top of that we could still work with our Maximum Likelihood Estimator method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first-off, let us demonstrate **how the classical Linear Model (OLS) can be seen as a particular case of  Generalized Linear Model** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelOLS = smf.ols( \"logGalumna ~ WatrCont\" , data= mites ) # classical linear model OLS\n",
    "\n",
    "# a GLM with gaussian error model, and identity as a link function\n",
    "modelGLM = smf.glm(\"logGalumna ~ WatrCont\" , data= mites , \n",
    "                   family=sm.families.Gaussian( sm.families.links.Identity() ) )\n",
    "\n",
    "\n",
    "resultsGLM = modelGLM.fit()#we do the actual fit\n",
    "resultsOLS = modelOLS.fit()#we do the actual fit\n",
    "\n",
    "print(resultsOLS.summary())\n",
    "print(resultsGLM.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explain all these new metrics later, but you can see that :\n",
    " * the Log-Likelihood is the same\n",
    " * the parameter estimates are the same \n",
    "    \n",
    "**So an OLS is a GLM with a gaussian noise and an identity link**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see which models we would use to better model this data:\n",
    "\n",
    "**In our first example about count of Galumna individuals, the GLM would have perfom a maximum likelihood on the following distribution:**\n",
    "\n",
    "$$P(n( w ) | w )=\\mathcal{P}(n_0 * e^{\\beta w })$$\n",
    "\n",
    "Which is a poisson distribution ($\\mathcal{P}$) with mean $n(w)= n_0 e^{\\beta w}$.\n",
    "\n",
    "You can note that the function $g^{-1}$ is: $ \\pmb tw^{T}\\pmb\\beta\\rightarrow n_0 * e^{\\beta w}$, \n",
    "\n",
    "and $g:n(w) \\rightarrow log(n_0)+\\beta w$\n",
    "\n",
    "So the family is a **Poisson** distribution, and the link is the **log** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "## a Poisson model GLM on the mites data  <a id='3.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to do the same thing in `statsmodels` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelPoisson = smf.glm( \"Galumna ~ WatrCont\", data= mites,\n",
    "               family=sm.families.Poisson( link = sm.families.links.Log() ))# family=Poisson link=log\n",
    "modelPoisson = modelPoisson.fit()#we do the actual fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adding a couple of diagnostic plots\n",
    "fig, ax = plt.subplots(ncols=2,figsize=(14,6))\n",
    "ax[0].plot(modelPoisson.mu , modelPoisson.resid_pearson , 'bo')\n",
    "ax[0].set_xlabel(\"predicted values\")\n",
    "ax[0].set_ylabel(\"pearson residuals\")\n",
    "\n",
    "ax[1].plot(modelPoisson.mu , mites.Galumna , 'bo')\n",
    "ax[1].plot( [0,max(mites.Galumna)], [0,max(mites.Galumna)] )\n",
    "ax[1].set_xlabel(\"predicted values\")\n",
    "ax[1].set_ylabel(\"observed values\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing the fit summary\n",
    "res=modelPoisson.summary()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that , in this summary, you don't see those F-statistic and t-test reports. Of course, since the noise is not normally distributed anymore.\n",
    "\n",
    "An important point to make is that the stringent constraints of the classical OLS result in a number of statistical properties that allows a lot of inference to be made, in partcular with regards to testing the significance of parameter values and comparing (nested)  models.\n",
    "\n",
    "In GLMs, things are not so clear cut, and the tools we use to evaluate a model are a bit different.\n",
    "\n",
    "\n",
    "Instead of the t-statistic you get a **z-statistic instead, which is coming from a Wald test**, with the null hypothesis being that the estimate is 0. This Wald test relies on the computation of the curvature of the log-likelihood function to get the confidence interval on the estimators as well as it's standard error.\n",
    "\n",
    "You might have also noticed that there is no $\\pmb R^2$ anymore. \n",
    "Instead you have : \n",
    " * the **Deviance** : it is a way to generalize the concept of $\\pmb R^2$ and could be summarized as the log likelihood ratio between a fully saturated model (ie., a model with as many parameters as points) and our model.\n",
    "      $$2*(logL(saturated model)-logL(fitted model))$$ \n",
    "      \n",
    " * the **Pearson Chi2** : This corresponds to the sum of the squared **Pearson residuals**. Pearson's residuals correspond to the difference between the predicted and observed value, divided by the expected standard deviation of the predicted value. Contrary to normal residuals (also called *response residuals*), they account for the fact the noise function does not necessarily has a constant variance across the data.\n",
    "\n",
    "You want these two numbers to be as low as possible. By themselves, they are not always easy to interpret. \n",
    "\n",
    "When the sample size is large enough, the Pearson Chi2 should follow a Chi-square distribution with a number of degree of freedom equal to $n-p$ (hence its name). \n",
    "In this case you could use the Pearson Chi2 to perform a **goodness-of-fit test**.\n",
    "\n",
    "Here, unfortunately, the sample size is too low for this Chi-square distribution assumption (see the end of this notebook for how we know this).\n",
    "\n",
    "This is why we also have to rely on the **diagnostic plots** (pearson residuals against predicted values, and observed values against predicted values).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters interpretation**\n",
    "\n",
    "In this model, we can see that both parameters are significantly different from 0.\n",
    "Remember our model equation is $n(t)=n(0)e^{\\beta t}$.\n",
    "* $n_0$, the theoretical number of Galumna individual for a water content of 0,  corresponds to `np.exp(Intercept)` $\\sim 13.19$\n",
    "* $\\beta$, the growth/decay rate, corresponds to `WatrCont` $= -0.0076$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this new model along side the log-transformed OLS one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( mites , y = 'Galumna' , x = \"WatrCont\" )\n",
    "\n",
    "x = np.linspace(mites.WatrCont.min() , mites.WatrCont.max() , 100 )\n",
    "\n",
    "## log-transformed OLS\n",
    "log_y_pred = model.predict( {\"WatrCont\" : x } )\n",
    "y_pred = np.exp( log_y_pred ) - 1 ## transform back the data \n",
    "plt.plot( x , y_pred , color = 'orange', label = 'log(y+1) OLS')\n",
    "\n",
    "## poisson GLM\n",
    "plt.plot( x , modelPoisson.predict( {\"WatrCont\" : x } ) , color = 'green' , label = 'Poisson GLM')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new model, taking into account the Poisson statistic of our response variable is doing quite better ! \n",
    "\n",
    "\n",
    "We can see that the OLS gravely underestimate the number of individuals for low water content and can give non-sensical negative values later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, we can still see that for medium water content (water content ~300 to 450), our model does not necessarily reflect the high variance we observe.\n",
    "\n",
    "\n",
    "This can be due to :\n",
    " * another covariable influencing the results\n",
    " * a problem with our modelling process\n",
    "\n",
    "\n",
    "Indeed, **Poisson random variable have their mean and variance equal** : that is a fairly strong requirement !\n",
    "\n",
    "To investigate this, in the next pair of plots I used a sliding window centered around each point, to calculate the mean and the variance at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interv=10 #number of points to the right and to the left we look at to deduce the local mean and variance\n",
    "\n",
    "## number of F.occidentalis ordered by elevation\n",
    "n=np.array(mites.Galumna)[ np.argsort( modelPoisson.mu ) ] \n",
    "\n",
    "df = pd.DataFrame({'local_means' : [np.mean(n[v-interv:v+interv+1]) for v in range(interv,len(n)-interv,1)],\n",
    "'local_vars' : [np.var(n[v-interv:v+interv+1]) for v in range(interv,len(n)-interv,1)] })\n",
    "df.sort_values(by = \"local_means\" , inplace=True)\n",
    "\n",
    "df['local_means2'] = df['local_means']**2\n",
    "\n",
    "\n",
    "linearfit = smf.ols( \"local_vars ~ local_means\" , data=df ).fit().fittedvalues\n",
    "squarefit = smf.ols( \"local_vars ~ local_means + local_means2\" , data=df ).fit().fittedvalues\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "\n",
    "ax.plot(df.local_means,\n",
    "           df.local_vars,'ro')\n",
    "ax.plot(df.local_means , linearfit , label = 'linear')\n",
    "ax.plot(df.local_means , squarefit , label = 'square')\n",
    "ax.set_xlabel('Mean n')\n",
    "ax.set_ylabel('Var n')\n",
    "ax.set_title('mean-variance relationship')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear link between the mean and the variances seems OK, but clearly, there is not a 1:1 relationship between mean and variance here. There is a clear **overdispersion**.\n",
    "\n",
    "\n",
    "This sort of situation could be handled with a GLM with another family : [negative binomial regression](https://timeseriesreasoning.com/contents/negative-binomial-regression-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the ToC](#toc)\n",
    "\n",
    "## Practical case 2 : Relation between the presence of  kyphosis and and a few covariates <a id='3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to model the chance to develop kyphosis (abnormally excessive convex curvature of the spine) after undergoing a surgical operation that aims at preventing it, according to 3 features :\n",
    "* age of the patient in months, \n",
    "* the number of vertebrea involved in abnormal curvature, \n",
    "* where that abnormal curvature starts on the vertebrae. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kyphosis=pd.read_csv('data/kyphosis.csv')\n",
    "# replace present/absent with 1/0\n",
    "df_kyphosis[\"Kyphosis_0_1\"]=[1 if v==\"present\" else 0 for v in df_kyphosis[\"Kyphosis\"]] \n",
    "df_kyphosis.drop(columns=\"Kyphosis\",inplace=True)\n",
    "df_kyphosis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_kyphosis,hue=\"Kyphosis_0_1\",height=2,aspect=1)\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here **the variable we are interested in is either 0 or 1**, and could be seen as the outcome of a bernouilli experiment as it is a binary choice. \n",
    "\n",
    "So inherently you already run into a problem here : how do you write a continuous linear function that will give you values between 0 and 1 (close to zero when kyphosis is absent and close to one when kyphosis is present)...\n",
    "\n",
    "Moreover, again, since you have a bernouilli random variable your mean and your variance are dependant... How are we going to model the absence or presence of kyphosis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see how using OLS here makes no sense**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = smf.ols( \"Kyphosis_0_1 ~ Age + Number + Start\" , data = df_kyphosis  )\n",
    "results = model.fit()#we do the actual fit\n",
    "\n",
    "res=results.summary()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check what the predicted values are for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.hist( list(results.fittedvalues) , 10 )\n",
    "ax.set_xlabel(\"predicted Kyphosis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Those are some irrational values for Kyphosis_1_0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once again a way to start tackling your problem is to **transform your variable of interest**, here kyphosis being present or not. Let's write $p$ the probablity that kyphosis is present (Y=1) and thus $1-p$ is the probablity that kyphosis is absent (Y=0). In a bernouilli statistic this $p$ also stand for the average of having Y=1.\n",
    "\n",
    "The transformation we are going to look at is the log transform of the odds ratio (**logit**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( figsize=(12,6) )\n",
    "P = np.linspace(0.001,0.999,999)\n",
    "ax.plot( P , np.log( P / (1-P) ) )\n",
    "ax.set_xlabel('P')\n",
    "ax.set_ylabel('logit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we say that :\n",
    "\n",
    "$$ln{\\frac{p_i}{1-p_i}}=\\beta_{0}+\\Sigma_{j=1}\\beta_{j}x_{j}=\\pmb X^T\\pmb\\beta$$\n",
    "\n",
    "which translates to\n",
    "\n",
    "$$p_i=\\frac{1}{1+e^{-(\\beta_{0}+\\Sigma_{j=1}\\beta_{j}x_{j})}}$$\n",
    "\n",
    "So this last equation shows you how we transformed our linear combination of covariables into an outcome between 0 and 1 that relates to the mean of a Bernouilli distribution (if you remember that before we work on relating linearly our covariable to the mean of a gaussian you might see where I am going with that, if not, no worries).\n",
    "\n",
    "\n",
    "You can see that we change quite a lot the variable of interest: now it's $p$ and it's not an integer just like our `Kyphosis_0_1`. \n",
    "Here more than before we drastically shift from modeling the variable to modeling its mean. \n",
    "Also, we cannot straight away go from `Kyphosis_0_1` to $p$ (but at least I can show you why it doesn't work). \n",
    "\n",
    "We already know that our variable is coming from a bernouilli experiment. So a good approach would be to fit a bernouilli distribution with parameter (mean) $p_i=\\frac{1}{1+e^{-(\\beta_{0}+\\Sigma^{n}_{j=1}\\beta_{j}x_{j})}}$, along the curve of our data (juste like what we did for the maximum likelihood with gaussian noise).\n",
    "\n",
    "As you will see this is exactly what GLM does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In our second example about kyphosis GLM would also simply perform maximum likelihood on the following distribution:**\n",
    "\n",
    "\n",
    "$$P(\\pmb K_i|\\pmb X_i)=\\mathcal{B}(\\frac{1}{1+e^{-(\\beta_{0}+\\Sigma^{n}_{j=1}\\beta_{j}x_{j})}})$$\n",
    "\n",
    "Where $\\mathcal{B}$ is the Bernouilli distribution, and $\\frac{1}{1+e^{-(\\beta_{0}+\\Sigma^{n}_{j=1}\\beta_{j}x_{j})}}$ corresponds to $p$ in the context of a log-odds-ratio transform.\n",
    "\n",
    "So the family is a particular **Binomial** distribution, and the link is the **logit** function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Backto to the ToC](#toc)\n",
    "\n",
    "## Let's apply this to the kyphosis data <a id='3.5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = smf.glm(\"Kyphosis_0_1 ~ Age + Number + Start\" , data = df_kyphosis,\n",
    "                family=sm.families.Binomial( link = sm.families.links.logit() ))\n",
    "# we use a Binomial here even though our cariable is Bernouilli. \n",
    "# Because a Binomial with number of try = 1 is a Bernouilli variable, \n",
    "# and since we are not providing a number of tries here, the function\n",
    "# by defafault understands that we want indeed Bernouilli\n",
    "\n",
    "results = model.fit()#we do the actual fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( results.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter interpretation**\n",
    "\n",
    "Our model is $log( \\frac{p_i}{(1-p_i)} ) = constant + Age * Age_i + Number * Number_i + Start * Start_i$\n",
    "\n",
    "So an increment of 1 in age, for instance, will result of an increment of $Age$ in the log odds-ratio, so a multiplication of the odds-ratio by $exp(Age)$.\n",
    "Note that change in odds-ratio are not that easy to interpret, but the direction and magnitude of the parameters still gives you an information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyOddsRatioFactor( p , factor):\n",
    "    OR0 = p/(1-p)\n",
    "    OR1 = OR0*factor\n",
    "    return OR1/(1+OR1)\n",
    "\n",
    "# let's compute the effect of a 10 months difference of age\n",
    "# 1 months age effect : 0.0109\n",
    "factor = np.exp(0.0109 * 10 ) \n",
    "print('factor' , factor)\n",
    "for p in np.linspace(0.1,0.9,9): \n",
    "    p1 = applyOddsRatioFactor( p , factor)\n",
    "    relativeRisk = p1/p\n",
    "    print( '{:.1f} -> {:.3f}\\t, RR: {:.2f}'.format( p , p1 , relativeRisk ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, given these parameter values, we are able to provide estimates for the probability of kiphosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab( results.fittedvalues > 0.5 , df_kyphosis.Kyphosis_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "sns.violinplot(y = results.fittedvalues, x=df_kyphosis.Kyphosis_0_1 , hue = df_kyphosis.Kyphosis_0_1 , \n",
    "               cut=0 , ax = ax ,  inner=\"stick\" )\n",
    "ax.set_xlabel(\"predicted Kyphosis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## model testing - demonstration on the kyphosis data-set <a id=\"3.6\"></a>\n",
    "\n",
    "The summary of the model for the kyphosis suggest that the parameter associated with `Number` is arguably not significantly from 0. \n",
    "So, **we may be tempted to exclude this measurment from our model**.\n",
    "\n",
    "This could be quite interesting if, for instance, obtaining this measurment was somewhat costly or painful to the patient. \n",
    "\n",
    "Furthermore, it is also informative for our undertanding of the underlying process to know whether or not this co-variable has a measurable effect on our ability to predict the variable of interest.\n",
    "\n",
    "Let's test how the model without the `Number` compares :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelReduced = smf.glm(\"Kyphosis_0_1 ~ Age \" , data = df_kyphosis,\n",
    "                family=sm.families.Binomial( link = sm.families.links.logit() ))\n",
    "\n",
    "\n",
    "resultsReduced = modelReduced.fit()#we do the actual fit\n",
    "\n",
    "print(resultsReduced.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the models are **nested**, so we will use a Likelihood Ratio Test to compare the reduced model with the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLkhDiff = 2*(results.llf - resultsReduced.llf ) \n",
    "# NB equivalent to : resultsReduced.deviance - results.deviance , in some cases\n",
    "dof = results.df_model - resultsReduced.df_model\n",
    "\n",
    "pval = 1-stats.chi2.cdf( logLkhDiff ,df=dof)\n",
    "print('log-likelihood difference:' , logLkhDiff )\n",
    "print('Degrees of freedom:',dof)\n",
    "print(\"\\tp-value for a chi-square distribution:\",pval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, it seems that the model including `Number` is a better fit than the model without it with a borderline significance level (p-value=0.048).\n",
    "\n",
    "> You may read about using the deviance to perform LRT. This is only valid is the particular cases where the scale of the model is fixed and not estimated. Given that statsmodels readily reports the log-likelihood we find it wiser to go for that directly rather than the deviance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## Predicting kyphosis  <a id=\"3.7\"></a>\n",
    "\n",
    "For each patient we have now a probability that this patient will develop Kyphosis. From here we can put a threshold that will tip our decision to **classify** our patient as prone to develop Kyphosis or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive approach : predict a kyphosis when the probability is >0.5\n",
    "predictedKyphosis = results.mu > 0.5\n",
    "pd.crosstab( predictedKyphosis , df_kyphosis.Kyphosis_0_1 ) # cross the results with the obersved data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what does this classification tells you about the ability of our model to predict Kyphosis? Would you trust it to make a decision on a patient?\n",
    "\n",
    "Bear in mind that this is the **performance on the data that was used to create the model**. We could ask how well we expect these results to translate with new data.\n",
    "\n",
    "Anyhow, notice the use of the word **classify**. Classification is a notion that we will develop next, from a **machine learning** perspective. But remember that in that particular case called classification by logistic regression, under the hood the problem is still a problem of regression through a  modelisation using Generalised Linear Model.\n",
    "\n",
    "In the next lessons, we will spend some time introducing key concepts of the classical machine learning framework, through **classification and regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise on GLM :  a dose-response problem <a id='2'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following dataset we would like to model the death rate of beetles due to some pesticide concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "np.var( stats.binom(  n = 600 , p = 0.217 ).rvs(10**4) / 600 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var( stats.binom(  n = 6 , p = 0.217 ).rvs(10**4) / 6 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beetles=pd.read_csv('data/beetle.csv' , index_col=0)\n",
    "df_beetles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1,figsize=(4,4))\n",
    "ax.scatter( df_beetles[\"dose\"], df_beetles[\"prop\"])\n",
    "ax.set_xlabel('dose')\n",
    "ax.set_ylabel('Proportion of dead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what You have learned, build a model of the number of dead and alive beetles \n",
    "depending on the insecticide dose.\n",
    "\n",
    "Here is a tentative path you may follow:\n",
    "1. decide on a relevant model family given the quantity you want to model\n",
    "2. create and fit your model\n",
    "3. inspect the results of your fitting\n",
    "4. plot the predictions of your model with the observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTES**\n",
    "\n",
    "* Here you can see that the number of bettle is not always exactly the same (from 56 to 63).\n",
    "* It is possible to have several predicted variable in your model.\n",
    "\n",
    "**option 1 : using different matrices for target / covariables**\n",
    "```\n",
    "y=df_beetles[[\"ndied\",\"nalive\"]]\n",
    "```\n",
    "**option 2 : R-style formula**:\n",
    "```\n",
    "\" ndied + nalive ~ ... \"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - reading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r -7 solutions/solution_02_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - setting up the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 8-15 solutions/solution_02_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - diagnostic plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 16-31 solutions/solution_02_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - plotting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 32- solutions/solution_02_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Code demonstration of why we should provide a the number of beetles and not just the proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will simulate some data.\n",
    "## for this I use the prediction of our previous model.\n",
    "ps = results.predict( df_beetles.dose )\n",
    "\n",
    "\n",
    "df_beetles2 = pd.DataFrame( columns = ['dose','nexp','ndied','nalive','prop'] , dtype= float)\n",
    "\n",
    "# two sets of experiments: one with 6 beetles, one with 600\n",
    "for n in [6,600]: \n",
    "\n",
    "    ## simulate data with a binomial\n",
    "    ndied = np.random.binomial( p = ps , n = n )\n",
    "\n",
    "    df_beetles2 = pd.concat([ df_beetles2 , \n",
    "                             pd.DataFrame({'dose':df_beetles.dose,\n",
    "                                           'nexp':n,\n",
    "                                           'ndied':ndied,\n",
    "                                           'nalive':n-ndied,\n",
    "                                           'prop':ndied/n\n",
    "                                          }) \n",
    "                            ],\n",
    "                            ignore_index=True)\n",
    "    \n",
    "df_beetles2 = df_beetles2.sort_values('dose')\n",
    "df_beetles2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( df_beetles2 , x = 'dose' , y = 'prop' , hue = 'nexp' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model with the number of beetles\n",
    "model_bin = smf.glm(\"ndied+nalive ~ dose\", data=df_beetles2,\n",
    "                family=sm.families.Binomial() )\n",
    "model_bin = model_bin.fit()\n",
    "\n",
    "print( model_bin.summary() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model with the proportion of beetles\n",
    "model_ber = smf.glm(\"prop ~ dose\", data=df_beetles2,\n",
    "                family=sm.families.Binomial() )\n",
    "model_ber = model_ber.fit()\n",
    "\n",
    "print( model_ber.summary() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the prediction\n",
    "p1 = model_ber.mu \n",
    "p2 = model_bin.mu\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1,figsize=(6,6))\n",
    "#ax.plot(df_beetles2[\"dose\"] , df_beetles2[\"prop\"] ,'bo',label='data')\n",
    "sns.scatterplot( df_beetles2 , x = 'dose' , y = 'prop' , hue = 'nexp' )\n",
    "ax.plot(df_beetles2[\"dose\"], p1,'r',label='prop')\n",
    "ax.plot(df_beetles2[\"dose\"], p2,'g',label='nalive+ndied')\n",
    "\n",
    "ax.set_xlabel('dose')\n",
    "ax.set_ylabel('proportion of dead')\n",
    "ax.legend(loc='best',fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model which is informed about the number of beetles **gives more importance to the samples obtained with 600 beetles than the ones obtained with 6 beetles**, as it should, because the latter are more noisy.\n",
    "\n",
    "Contrastingly, the model with the proportions only does not have this information and thus it gets overly influenced by the noisy samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Annex : is the Pearson chi2 adapted for the mites data <a id='A0'></a>\n",
    "\n",
    "The theory goes that if the model is correct,and if the sample size is large enough, then the sum of the squares of the **pearson residuals** (residuals scaled by the standard deviation of the predicted values) should follow a chi-square distribution with a number of degrees of freedom equal to $n-p$ (number of points - number of parameters).\n",
    "\n",
    "`statsmodels` reports this sum of squares as `Pearson Chi2`.\n",
    "\n",
    "The problem is \"*the sample size is large enough*\" does not have a clear cut limit. It is quite dependent on the model type and the values of this parameter. So to test this hypothesis we propose a simulation strategy.\n",
    "\n",
    "The idea is that we take the model, and use it to simulate a lot of data-set, and then compute the Pearson Chi2 for each of this simulated data-set.\n",
    "If the sample size is large enough, then the simulated Pearson Chi2 will follow the expected Chi-square distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "PEARSONCHI2 = []\n",
    "\n",
    "\n",
    "n = len(mites.WatrCont)\n",
    "\n",
    "\n",
    "for i in range(10**3):\n",
    "\n",
    "    ## simulate some WatrCont value (X)\n",
    "    x = np.random.uniform( mites.WatrCont.min() , mites.WatrCont.max() ,  size = n)\n",
    "\n",
    "    ## mu corresponding to the X according to our model\n",
    "    mus = modelPoisson.predict( {\"WatrCont\":x} )\n",
    "\n",
    "    ## simulate Y from this\n",
    "    y = np.random.poisson( mus )\n",
    "\n",
    "\n",
    "    pearsonResiduals = (mus - y)/np.sqrt(mus) \n",
    "    #NB: this formula works for for poisson models because the variance of prediction \n",
    "    #   is equal to its mean\n",
    "\n",
    "    pearsonChi2 = sum( np.power( pearsonResiduals , 2 ) )\n",
    "    \n",
    "    PEARSONCHI2.append( pearsonChi2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "fig, ax = plt.subplots(ncols=1,figsize=(8,5))\n",
    "ax.hist(PEARSONCHI2)\n",
    "ax.set_xlabel(\"simulated pearson chi2\")\n",
    "\n",
    "pvals = 1-stats.chi2.cdf(PEARSONCHI2,df=n-2)\n",
    "print( \"fraction of rejected tests at alpha=0.05 :\" , sum(pvals < 0.05)/len(pvals) )\n",
    "print(\"K-S test of observed distribution of deviance with a chi2 with n-2 df :\" , \n",
    "      stats.kstest( PEARSONCHI2 , lambda x : stats.chi2.cdf(x , df = n-2) ).pvalue )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this does not follow a Chi2, so the sample is not \"large enough\" for this property to be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Annex : Additionnal Practical case : exponential growth of Corona virus infection <a id='A1' ></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset we follow the spread of covid cases over time. We would like for example to understand what is the growth rate of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_corona_exp=pd.read_csv('data/exponential_covid.csv')\n",
    "df_corona_exp[\"Days\"]=df_corona_exp.index\n",
    "df_corona_exp.drop(columns=\"Time\",inplace=True)\n",
    "df_corona_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,figsize=(14,7))\n",
    "ax[0].plot(list(df_corona_exp[\"Days\"]),list(df_corona_exp[\"Infections\"]),'ro')\n",
    "ax[0].set_xlabel('days')\n",
    "ax[0].set_ylabel('Number of infected')\n",
    "\n",
    "ax[1].plot(list(df_corona_exp[\"Days\"]),list(df_corona_exp[\"Infections\"]),'ro')\n",
    "ax[1].set_xlabel('days')\n",
    "ax[1].set_ylabel('log( Number of infected ) ')\n",
    "ax[1].set_yscale('log')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here looking at the data, and with a little bit of domain knowledge we are looking at model of the kind $$n(t)=n(0)e^{\\beta t}$$ where $\\beta$ is the growth rate.  \n",
    "\n",
    "With this kind of function it seems impossible to to be able to write $\\bar{n(t)}=\\pmb t^T\\pmb\\beta$, as we would in a ordinary linear model. \n",
    "\n",
    "Moreover, here $n(t)$ could follow the definition a Poisson statistic (a number of independent events during an interval t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would imply that there is a relationship between the mean and the variance in the number of infected person.\n",
    "\n",
    "Consequently, this does not fit well with the different hypothesis we have seen before. \n",
    "But you can already see what kind of trick we could use to get around it : we could log transform the n. \n",
    "\n",
    "This is really different from what we were doing when transforming the covariables before. This time it really is the response variable that we are going to fit, which is transformed. We are not transforming the $\\pmb X$ to increase our descriptional power of $\\pmb Y$, but transforming $\\pmb Y$, and by doing so, of course, transforming $\\pmb X$.\n",
    "\n",
    "In that case we end up with the following linear relationship:\n",
    "$$n(t)=n(0)e^{\\beta t} \\rightarrow$$\n",
    "$$log(n(t)) = log( n(0)e^{\\beta t} ) = log(n(0))+\\beta t$$\n",
    "\n",
    "Looking at the relationship between the mean and the variance of our newly transformed random variable log(n), we are not sure it could be considered homoscedastic but the variation are kind of small so we could try an OLS to model the log(n(t))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column with the log( number of infected ) \n",
    "df_corona_exp[\"log_infect\"]=np.log(0.1+df_corona_exp[\"Infections\"]) \n",
    "df_corona_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = smf.ols(\"log_infect ~ Days\" , data = df_corona_exp)# we create the least square fit object\n",
    "results = model.fit()#we do the actual fit\n",
    "\n",
    "res=results.summary()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLogInfections = results.params.Intercept + df_corona_exp[\"Days\"]*results.params.Days\n",
    "predictedInfections = np.exp(predictedLogInfections)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2,figsize=(12,6))\n",
    "ax[0].plot(list(df_corona_exp[\"Days\"]),list(df_corona_exp[\"Infections\"]),'b-',label='data')\n",
    "ax[0].plot(list(df_corona_exp[\"Days\"]),predictedInfections,'g--',label='prediction OLS transfo')\n",
    "ax[0].set_xlabel('Days')\n",
    "ax[0].set_ylabel('Infections number')\n",
    "ax[0].legend(loc='best',fontsize=12)\n",
    "\n",
    "ax[1].plot(list(df_corona_exp[\"Days\"]),list(df_corona_exp[\"log_infect\"]),'b-',label='data')\n",
    "ax[1].plot(list(df_corona_exp[\"Days\"]),predictedLogInfections,'g--',label='prediction OLS transfo')\n",
    "\n",
    "ax[1].set_xlabel('Days')\n",
    "ax[1].set_ylabel('log(Infections number)')\n",
    "ax[1].legend(loc='best',fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look at the predicted values versus the observed ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "ax.plot(predictedInfections , df_corona_exp[\"Infections\"] , 'bo')\n",
    "ax.plot( [0,max(predictedInfections)], [0,max(predictedInfections)] )\n",
    "ax.set_xlabel(\"predicted values\")\n",
    "ax.set_ylabel(\"observed values\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-exercice : given this model how would you deduce the doubling time (mean time it takes for the number of infected to double) ?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_02_mini.py\n",
    "import numpy as np\n",
    "\n",
    "# answer : the doubling time corresponds to the number of days d, \n",
    "# n0*exp(beta*d) = 2*n0*exp(beta*0)\n",
    "# --> exp(beta*d) = 2\n",
    "# --> beta*d == log(2)\n",
    "doubling_time = np.log(2)/ 0.112015\n",
    "doubling_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see later on how this transformation is already quite a significant first step into GLM,\n",
    "and how we could have go one step further to actually use a GLM. \n",
    "\n",
    "Because, yes, even though this already is quite a pretty good model, there is a more appropriate way of modelling this data. \n",
    "\n",
    "Not only is it more appropriate, it is also more accurate and you will see that it matters a lot when trying to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Annex : is the Pearson chi2 adapted for the infection data <a id='A2'></a>\n",
    "\n",
    "The theory goes that if the model is correct,and if the sample size is large enough, then the sum of the squares of the **pearson residuals** (residuals scaled by the standard deviation of the predicted values) should follow a chi-square distribution with a number of degrees of freedom equal to $n-p$ (number of points - number of parameters).\n",
    "\n",
    "`statsmodels` reports this sum of squares as `Pearson Chi2`.\n",
    "\n",
    "The problem is \"*the sample size is large enough*\" does not have a clear cut limit. It is quite dependent on the model type and the values of this parameter. So to test this hypothesis we propose a simulation strategy.\n",
    "\n",
    "The idea is that we take the model, and use it to simulate a lot of data-set, and then compute the Pearson Chi2 for each of this simulated data-set.\n",
    "If the sample size is large enough, then the simulated Pearson Chi2 will follow the expected Chi-square distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's simulate what happens with a correctly specified model \n",
    "# poisson model \n",
    "\n",
    "PEARSONCHI2 = []\n",
    "\n",
    "# parameters of our model :\n",
    "beta = 0.1923\n",
    "n0 = np.exp(-2.6891)\n",
    "\n",
    "N = 1000 # number of simulations we do\n",
    "n=50 # size of the sample\n",
    "\n",
    "for i in range(N):\n",
    "    x = np.random.randint(low=1,high=50,size=n)\n",
    "    \n",
    "    y_pred = n0 * np.exp( x * beta ) # value predicted by the model\n",
    "    y = np.random.poisson(lam= n0 * np.exp( x * beta ) ) # value simulated by the model\n",
    "\n",
    "    pearsonResiduals = (y_pred - y)/np.sqrt(y_pred) \n",
    "    #NB: this formula works for for poisson models because the variance of prediction \n",
    "    #   is equal to its mean\n",
    "    \n",
    "    pearsonChi2 = sum( np.power( pearsonResiduals , 2 ) )\n",
    "    \n",
    "    PEARSONCHI2.append( pearsonChi2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1,figsize=(8,5))\n",
    "ax.hist(PEARSONCHI2)\n",
    "ax.set_xlabel(\"simulated pearson chi2\")\n",
    "\n",
    "pvals = 1-stats.chi2.cdf(PEARSONCHI2,df=n-2)\n",
    "print( \"fraction of rejected tests at alpha=0.05 :\" , sum(pvals < 0.05)/len(pvals) )\n",
    "print(\"K-S test of observed distribution of deviance with a chi2 with n-2 df :\" , \n",
    "      stats.kstest( PEARSONCHI2 , lambda x : stats.chi2.cdf(x , df = n-2) ).pvalue )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it would appear that for this parameter set (n0 , beta, sample size) the pearson Chi2 does not follow the expected Chi-square distribution...\n",
    "\n",
    "This being said, we just simulated a large number of pearson Chi2 for our model. This could be used as a reference distribution instead of the chi-square distribution! (the whole goal of the Chi-square approximation is so that we don't have to do this sort of simulation approach to begin with...)\n",
    "\n",
    "On our infection data, the observed pearson Chi2 is around 2720. \n",
    "This is way bigger than what we get with the simulated values : it is likely that our Poisson model is not so good.\n",
    "\n",
    "In the next part, we will try another, more complex model on this data.\n",
    "\n",
    "> Feel free to play around with this simulations parameters. For instance, when `beta=0.5` , `n0 = 10` and `n=500`, the Pearson Chi2 actually seems to follow something close to the expected chi-square distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Annex : trying a negative binomial model on the infection data <a id='A3'></a>\n",
    "\n",
    "\n",
    "The poisson model arguably models the data better than a simple linear model on the log-transformed data.\n",
    "However, a poisson model present the strict hypothesis that there is a perfect relationship between the mean and the variance of the noise around the predicted values.\n",
    "\n",
    "This is a strong hypothesis, and maybe one we could want to relax : indeed in practise the data may be overdispersed (the variance is higher than the mean) or underdispersed (the variance is lower than the mean). \n",
    "\n",
    "\n",
    "One of the definition of the negative binomial model is as a poisson model with an additionnal parameter ($\\alpha$) that controls over/under-dispersion.\n",
    "\n",
    "Here, the idea is that Rather than $variance = mean$, we have\n",
    "\n",
    "$$Variance = mean + \\alpha * mean^2$$\n",
    "\n",
    "So to test our hypothesis, we will try to fit a negative binomial model on our data and compare the fit with the previous poisson model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying a negative binomial to account for overdispersion\n",
    "\n",
    "# from : https://dius.com.au/2017/08/03/using-statsmodels-glms-to-model-beverage-consumption/\n",
    "# using Cameron-Trivedi dispersion test to estimate alpha\n",
    "y=df_corona_exp[\"Infections\"]\n",
    "X=df_corona_exp[\"Days\"]#again making it an array \n",
    "X = sm.add_constant(X)#the model doesn't include an intercept automatically so we creat one\n",
    "model = sm.GLM(y, X, \n",
    "               family=sm.families.Poisson( link = sm.families.links.log() ))\n",
    "results = model.fit()#we do the actual fit\n",
    "\n",
    "\n",
    "MU = results.mu\n",
    "Y = y\n",
    "RESP = ( (Y-MU)**2 - MU ) / MU\n",
    "\n",
    "ct_results = sm.OLS( RESP , MU ).fit()\n",
    "#print(ct_results.summary())\n",
    "# Construct confidence interval for alpha, the coefficient of the mean in the mean-variance relationship\n",
    "alpha_ci95 = ct_results.conf_int(0.05).loc['x1']\n",
    "print('\\nC-T dispersion test: alpha = {:5.3f}, 95% CI = ({:5.3f}, {:5.3f})'.format(ct_results.params[0], alpha_ci95.loc[0], alpha_ci95.loc[1]))\n",
    "alpha = ct_results.params[0]\n",
    "##\n",
    "model = sm.GLM(y, X, \n",
    "               family=sm.families.NegativeBinomial( alpha = alpha , link = sm.families.links.log() ))\n",
    "results = model.fit()#we do the actual fit\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( ncols=2,nrows=2 ,figsize=(14,12))\n",
    "ax[0][0].plot(results.mu , results.resid_pearson , 'bo')\n",
    "ax[0][0].set_xlabel(\"predicted values\")\n",
    "ax[0][0].set_ylabel(\"pearson residuals\")\n",
    "\n",
    "ax[0][1].plot(results.mu , y , 'bo')\n",
    "ax[0][1].plot( [0,max(y)], [0,max(y)] )\n",
    "ax[0][1].set_xlabel(\"predicted values\")\n",
    "ax[0][1].set_ylabel(\"observed values\")\n",
    "\n",
    "ax[1][0].plot(list(df_corona_exp[\"Days\"]),list(df_corona_exp[\"Infections\"]),'bo',label='data')\n",
    "ax[1][0].plot(list(df_corona_exp[\"Days\"]),[np.exp(-2.6891)*np.exp(0.1923*t) for t in list(df_corona_exp[\"Days\"])],'r--',label='prediction GLM')\n",
    "ax[1][0].plot(list(df_corona_exp[\"Days\"]),[np.exp(0.448)*np.exp(0.1128*t) for t in list(df_corona_exp[\"Days\"])],'g--',label='prediction OLS transfo')\n",
    "ax[1][0].plot( df_corona_exp[\"Days\"] , results.mu , 'b--',label='GLM negative binom')\n",
    "\n",
    "\n",
    "ax[1][0].set_xlabel('Days')\n",
    "ax[1][0].set_ylabel('Infections number')\n",
    "ax[1][0].legend(loc='best',fontsize=12)\n",
    "\n",
    "ax[1][1].plot(list(df_corona_exp[\"Days\"]),list(df_corona_exp[\"log_infect\"]),'bo',label='data')\n",
    "ax[1][1].plot(list(df_corona_exp[\"Days\"]),[-2.6891+0.1923*t for t in list(df_corona_exp[\"Days\"])],'r--',label='prediction GLM')\n",
    "ax[1][1].plot(list(df_corona_exp[\"Days\"]),[0.448+0.1128*t for t in list(df_corona_exp[\"Days\"])],'g--',label='prediction OLS transfo')\n",
    "ax[1][1].plot( df_corona_exp[\"Days\"] , np.log(results.mu) , 'b--',label='GLM negative binom')\n",
    "\n",
    "ax[1][1].set_xlabel('Days')\n",
    "ax[1][1].set_ylabel('log(Infections number)')\n",
    "ax[1][1].legend(loc='best',fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "res=results.summary()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that metrics such as Deviance and Pearson Chi-square are better.\n",
    "\n",
    "However, something may not really be correct in that model. In fact, it is very likely that it is undergoing some form of autocorrelation , ie. the number of infected at time $t+1$ does not depend so much on the day, but rather on the number of infected at time $t$ (and/or $t-1$, $t-2$, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Annex : compute the confidence interval in GLMs - example with a gaussian distribution <a id='confint'></a>\n",
    "\n",
    "As a side note for now, but important for the rest:\n",
    "\n",
    "$$\\hat{\\pmb\\beta}=(\\pmb X^T \\pmb X)^{-1}\\pmb X^T \\pmb Y$$\n",
    "\n",
    "is the solution from both the maximum likelihood in the case of normally distributed noise, and the least square fit.\n",
    "\n",
    "As a quick proof, and also to get a feeling of how all of this is working we can take the simple case of $\\bar{y_i}=\\beta x_i=f(x_i,\\beta)$.\n",
    "\n",
    "In that case the loglikelihood look like \n",
    "\n",
    "$$l=\\sum_i -\\frac{1}{2}\\frac{(y_i-\\beta x_i)^2}{\\sigma^2} + constant $$\n",
    "\n",
    "The partial derivative of this likelihood with respect to the $\\beta$ parameter is :\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\beta}\\propto\\sum_i -2x_iy_i + 2\\beta x_i^2$$\n",
    "\n",
    "You want $\\frac{\\partial l}{\\partial \\beta}(\\hat{\\beta})=0$ as you are maximizing the likelihood so you end up with:\n",
    "\n",
    "$$\\hat{\\beta} \\sum_i x_i^2=\\sum_i x_iy_i$$\n",
    "\n",
    "Which in multivariate case actually corresponds to $(\\pmb X^T \\pmb X)\\hat{\\pmb\\beta}=\\pmb X^T \\pmb Y$\n",
    "\n",
    "The term $(\\pmb X^T \\pmb X)$ plays a special role, here and in the confidence interval seen before $\\beta_j \\in [\\hat{\\beta_j} \\pm z_{1-\\frac{\\alpha}{2}}\\sqrt{\\hat{\\sigma}^2 [(\\pmb X^T \\pmb X)^{-1}]_{j,j}}]$: \n",
    "\n",
    "$(\\pmb X^T \\pmb X)$ is generally related to the covariance of $\\pmb X$. But in that specific case (normality), it is also a matrix representing the curvature of the log likelihood function at $\\hat{\\pmb \\beta}$. You calculate the curvature of a function by calculating the matrix of second derivative called hessian. In our case you just differentiate one more time what we found before :\n",
    "\n",
    "$$\\frac{\\partial^2 l}{\\partial \\beta^2}\\propto\\sum x_i^2$$\n",
    "\n",
    "And indeed you find this $\\sum x_i^2$ term which in multivariate forms is $(\\pmb X^T \\pmb X)$.\n",
    "\n",
    "\n",
    "\n",
    "Calculating a confidence interval in the general case (in the case of the GLM that we will see later for example), involves inverting the curvature of the loglikelihood function at the estimated $\\hat{\\pmb \\beta}$ and put in a normal confidence interval as seen before. \n",
    "In statistic this curvature matrix (also called hessian in mathematic) is related to the Fisher Information, which we will use later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_introml2024)",
   "language": "python",
   "name": "conda_introml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
