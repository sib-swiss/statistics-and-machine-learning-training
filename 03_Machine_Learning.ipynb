{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 5, 5\n",
    "plt.rc(\"font\", size=10)\n",
    "\n",
    "\n",
    "plt.rc('xtick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('xtick.major', size=8, pad=12)\n",
    "plt.rc('xtick.minor', size=8, pad=12)\n",
    "\n",
    "plt.rc('ytick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('ytick.major', size=8, pad=12)\n",
    "plt.rc('ytick.minor', size=8, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have fitted our curves and doing so we have found the  best model explaining the point that we had for the fitting.\n",
    "\n",
    "We also saw that we could choose different models according to how much the improvement obtained was worth the complexification of the model. But again we did it on the whole data that we had. We never really check how well our data was generalizing to points never seen before, or by how much the model we found was subject to outliers.\n",
    "\n",
    "Machine learning procedures allow us to take those considerations into account. After highlighting the few caveats of the procedures we have used in the former notebook, we will introduce the foundation of the machine learning way to model.\n",
    "\n",
    "More particularly we will see that the machine learning paradigm modifies the function to optimize that we have seen before by adding a penalty to covariables that generalize badly. We will also see that in a machine learning procedure, the generalization is approached by fitting and evaluating mutliple times your model on subset of your data.\n",
    "\n",
    "**The machine learning paradigm emphasizes the importance of building a general model that will be good at dealing with future, unknown, data points rather than being the best model on the data that we have now.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Of Content: <a id=\"toc\"></a>\n",
    "\n",
    "\n",
    "* [**motivating example**](#motivation)\n",
    "\n",
    "* [**Linear regression**](#linear)\n",
    "    * [approach 1: a simple linear regression](#linear-1)\n",
    "    * [approach 2: adding regularization and validation set](#linear-2)\n",
    "    * [approach 3 : k-fold cross-validation](#linear-3)\n",
    "    * [approach 4 : a \"classical\" ML pipeline](#linear-4)\n",
    "    \n",
    "* [**Logistic regression**](#toy-example-lr)\n",
    "    * [Exercise: Logistic regression to detect breast cancer malignancy](#LR-exercise)\n",
    "\n",
    "* [**Imbalanced dataset**](#imbalanced)\n",
    "* [**A few VERY IMPORTANT words on leakage.**](#leakage)\n",
    "\n",
    "* [**Support Vector Machine**](#svm)\n",
    "    * [SVM for Classification](#svm-c)\n",
    "        * [introduction](#formal-svm-c)\n",
    "        * [Toy example to visualize SVMC.](#toy-example-svm-c)\n",
    "        * [SVM Classifier pipeline.](#svm-c-pipeline)\n",
    "    * [SVM for Regression.](#svm-r)\n",
    "    \n",
    "* [**Decision tree modeling .**](#decision-tree)\n",
    "    * [Simple decision tree for classification.](#simple-tree-c)\n",
    "        * [Toy example to visualize decision tree.](#toy-decision-tree)\n",
    "        * [Single decision tree pipeline.](#single-tree-pipeline)\n",
    "    * [Random Forest in classification](#rf-c)\n",
    "        * [Exercise: Random Forest on the breast cancer dataset](#rf-exo)\n",
    "        * [RF annex 1: too many features](#rf-a1)\n",
    "    * [Random Forest in regression](#rf-r)\n",
    "    \n",
    "* [**Conclusion**](#conclusion)\n",
    "\n",
    "\n",
    "* [**Classification exercise**](#exo-classif)\n",
    "\n",
    "* [**Regression Exercise**](#exo-regression)\n",
    "* [**Annexes**](#annex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it seems that we fare much better in the cubic scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# motivating example <a id=\"motivation\"></a>\n",
    "\n",
    "[Acharjee et al.2016](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1043-4) propose several -omic dataset which they used to predict and gain knowledge on various phenotypic traits in potatos.\n",
    "\n",
    "Here, we will concentrate on the their transcriptomics dataset and the phenotypic trait of the potato coloration.\n",
    "\n",
    "We have pre-selected and normalized the 200 most promising genes (out of ~15 000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_metadata = \"data/potato_data.phenotypic.csv\"\n",
    "file_data = \"data/potato_data.transcriptomic.top200norm.csv\"\n",
    "\n",
    "df = pd.read_csv( file_metadata , index_col=0 )\n",
    "dfTT = pd.read_csv( file_data , index_col=0)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of our story, we will imagine that out of the 86 potatos in the data, we have only 73 at the time of our experiment.\n",
    "\n",
    "We put aside the rest for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = df.index[:73]\n",
    "i2 = df.index[73:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfTT.loc[i1 , :]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[i1 , \"Flesh Colour\"]\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "# Linear regression  <a class=\"anchor\" id=\"linear\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## approach 1: a simple linear regression <a class=\"anchor\" id=\"linear-1\"></a>\n",
    "\n",
    "Let's fit a simple linear model with our gene expression values, and see what happens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "## importing scoring functions\n",
    "from sklearn.metrics import r2_score , mean_squared_error\n",
    "\n",
    "Xc = sm.add_constant(X)## adding the intercept to the model\n",
    "\n",
    "model = sm.OLS( y , Xc ) \n",
    "model = model.fit() \n",
    "\n",
    "# predict\n",
    "y_pred = model.predict( Xc )\n",
    "\n",
    "# evaluate the prediction\n",
    "print(f\"R-squared score: { r2_score( y , y_pred ) :.2f}\")\n",
    "print(f\"mean squared error: { mean_squared_error( y , y_pred ) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Wow!!** this is a perfect fit.\n",
    "\n",
    "But if you know anything about biology, or data analysis, then you likely suspect something wrong is happening.\n",
    "\n",
    "\n",
    "Indeed, at the moment, our claim is that our model can predict flesh color perfectly (RMSE=0.0) from the normalized expression of these 200 genes.\n",
    "\n",
    "But, say we now have some colleagues who come to us with some new potato data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## now we use the leftover data points:\n",
    "Xnew = dfTT.loc[i2 , :]\n",
    "ynew = df.loc[i2 , \"Flesh Colour\"]\n",
    "\n",
    "## apply the model on the new data\n",
    "ynew_pred = model.predict( sm.add_constant(Xnew) )\n",
    "\n",
    "# evaluate the prediction\n",
    "print(f\"new data R-squared score: { r2_score( ynew , ynew_pred ) :.2f}\")\n",
    "print(f\"new data mean squared error: { mean_squared_error( ynew , ynew_pred ) :.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( y , y_pred , label = 'training data' )\n",
    "plt.scatter( ynew , ynew_pred , label = 'new data' )\n",
    "plt.xlabel('observed values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the performance on the new data is not as good as with the data we used to train the model.\n",
    "\n",
    "We have **overfitted** the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, before we process with how we can try to avoid these problems,\n",
    " we are going to switch to `scikit-learn` objects. This will make our life easier down the line.\n",
    " \n",
    "---\n",
    " \n",
    "Doing the thing we just did with `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we import elements from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score , mean_squared_error\n",
    "\n",
    "\n",
    "# create the regression object\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# fit it with our data\n",
    "lin_reg.fit(X,y)\n",
    "\n",
    "# predict\n",
    "y_pred = lin_reg.predict( X )\n",
    "\n",
    "# evaluate the prediction\n",
    "print(f\"R-squared score: { r2_score( y , y_pred ) :.2f}\")\n",
    "print(f\"mean squared error: { mean_squared_error( y , y_pred ) :.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we use the leftover data points:\n",
    "Xnew = dfTT.loc[i2 , :]\n",
    "ynew = df.loc[i2 , \"Flesh Colour\"]\n",
    "\n",
    "## apply the model on the new data\n",
    "ynew_pred = lin_reg.predict( Xnew )\n",
    "\n",
    "# evaluate the prediction\n",
    "print(f\"new data R-squared score: { r2_score( ynew , ynew_pred ) :.2f}\")\n",
    "print(f\"new data mean squared error: { mean_squared_error( ynew , ynew_pred ) :.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( y , y_pred , label = 'training data' )\n",
    "plt.scatter( ynew , ynew_pred , label = 'new data' )\n",
    "plt.xlabel('observed values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NB: the models are not perfectly equivalent here. This is because (a) statsmodels does some normalization under the hood which sklearn doesn't (b) there are several ways to overfit this data and the two library use two different ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNEX : \"generic\" sklearn usage \n",
    "\n",
    "The main library we will be using for machine learning is scikit-learn.\n",
    "\n",
    "It should go without saying that if you have any questions regarding its usage and capabilities, your first stop should be their [website](https://scikit-learn.org/stable/),\n",
    "especially since it provides plenty of [examples](https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html#sphx-glr-auto-examples-ensemble-plot-voting-decision-regions-py), [guides](https://scikit-learn.org/stable/user_guide.html), and [tutorials](https://scikit-learn.org/stable/tutorial/index.html#tutorial-menu).\n",
    "\n",
    "Nevertheless, we introduce here the most common behavior of sklearn object.\n",
    "\n",
    "Indeed, sklearn implement machine learning algorithms (random forest, clustering algorithm,...), as well as all kinds of preprocessers (scalin, missing value imputation,...) with a fairly consistent interface.\n",
    "\n",
    "Most methods must first be instanciated as an object from a specific class:\n",
    "\n",
    "```python\n",
    "## import the class, here RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## instanciate the class object:\n",
    "my_clf = RandomForestClassifier()\n",
    "```\n",
    "\n",
    "As it stands, the object is just a \"naive\" version of the algorithm.\n",
    "\n",
    "The next step is then to feed the object data, so it can learn from it. This is done with the `.fit` method:\n",
    "\n",
    "```python\n",
    "my_clf.fit( X , y )\n",
    "```\n",
    "> In this context, `X` is the data and `y` is the objective to attain. When the object is not an ML algorithm but a preprocessor, you only give the `X`\n",
    "\n",
    "Now that the object has been trained with your data, you can use it. For instance, to:\n",
    "* `.transform` your data (typically in the case of a preprocessor)\n",
    "* `.predict` some output from data (typically in the case of an ML algorithm, like a classifier)\n",
    "\n",
    "```python\n",
    "y_predicted = clf.predict(X)  # predict classes of the training data\n",
    "\n",
    "## OR \n",
    "\n",
    "X_scaled = myScaler.transform(X)  # apply a transformation to the data\n",
    "```\n",
    "\n",
    "Last but not least, it is common in example code to \"fit and transform\" a preprocesser in the same line using `.fit_transform`\n",
    "\n",
    "```python\n",
    "X_scaled = myNaiveScaler.fit_transform(X)  # equivalent to myNaiveScaler.fit(X).transform(X)\n",
    "```\n",
    "\n",
    "That's the basics. You will be able to experiment at length with this and go well beyond it.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Anyhow, here we could still use the model that we have created, \n",
    "but we would agree that reporting the perfect performance we had with our training data would be misleading.\n",
    "\n",
    "To honestly report the performance of our model, we measure it on a **set of data that has not been used at all to train it: the *test set*.**\n",
    "\n",
    "\n",
    "\n",
    "To that end, we typically begin by dividing our data into :\n",
    "\n",
    " * **train** set : find the best model\n",
    " * **test** set  : give an honest evaluation of how the model perform on completely new data.\n",
    "\n",
    "![train_test](image/train_test.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = Xnew\n",
    "y_test = ynew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## approach 2: adding regularization and validation set <a class=\"anchor\" id=\"linear-2\"></a>\n",
    "\n",
    "In the case of a Least Square fit, the function you are minimizing looks like:\n",
    "\n",
    "$\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2$\n",
    "\n",
    ", so the sum of squared difference between the observation and the predictions of your model.\n",
    "\n",
    "\n",
    "**Regularization** is a way to reduce overfitting, and in the case of the linear model\n",
    "we do so by adding to this function a **penalization term which depends on coefficient weights**.\n",
    "\n",
    "In brief, the stronger the coefficient, the higher the penalization. So only coefficients which bring more fit than penalization will be kept.\n",
    "\n",
    "\n",
    "> Note : we report here the formulas used in `scikit-learn` functions. Other libraries may have a different parameterization, but the concepts stay the same\n",
    "\n",
    "$\\frac{1}{2n}\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 + \\alpha\\sum_{j}|\\beta_{j}|$ , **l1 regularization** (Lasso) $\\alpha$ being the weight that you put on that regularization \n",
    "\n",
    "$\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 + \\alpha\\sum_{j}\\beta_{j}^{2}$ , **l2 regularization** (Ridge) \n",
    "\n",
    "$\\frac{1}{2n}\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 + \\alpha\\sum_{j}(\\rho|\\beta_{j}|+\\frac{(1-\\rho)}{2}\\beta_{j}^{2})$ , **elasticnet**\n",
    "\n",
    "\n",
    "For a deeper understanding of those notions, you may look at :\n",
    "\n",
    " * https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net\n",
    "\n",
    " * https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\n",
    "\n",
    "\n",
    "\n",
    "> NB: Regularization generalize to maximum likelihood contexts as well)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "logalphas = []\n",
    "\n",
    "coef_dict = {'name' : [],\n",
    "             'coefficient' : [],\n",
    "             'log-alpha' : []}\n",
    "r2 = []\n",
    "\n",
    "for alpha in np.logspace(-2,2,50):\n",
    "\n",
    "    reg = SGDRegressor( penalty='l1' , alpha = alpha )\n",
    "    reg.fit( X , y )\n",
    "    \n",
    "    logalphas.append(np.log10(alpha))\n",
    "    r2.append( r2_score( y , reg.predict(X) ) )\n",
    "    \n",
    "    coef_dict['name'] += list( X.columns )\n",
    "    coef_dict['coefficient'] += list( reg.coef_ )\n",
    "    coef_dict['log-alpha'] += [np.log10(alpha)]* len(X.columns )\n",
    "\n",
    "coef_df = pd.DataFrame(coef_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (14,7))\n",
    "\n",
    "ax[0].plot(logalphas , r2)\n",
    "ax[0].set_xlabel(\"log10( alpha )\")\n",
    "ax[0].set_ylabel(\"R2\")\n",
    "\n",
    "sns.lineplot( x = 'log-alpha' , y='coefficient' , hue = 'name' , data= coef_df , ax = ax[1] ,legend = False)\n",
    "\n",
    "fig.suptitle(\"regression of potato data with an L1 regularization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Micro-exercise:** adapt the code above to generate this plot with an l2 penalty. How do you interpret the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_mini1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but how do we choose which level of regularization we want ?\n",
    "\n",
    "It is a general rule that **as you decrease $\\alpha$, the $R^2$ on the data used for the fit increase**, i.e. you risk overfitting.\n",
    "\n",
    "Consequently, we cannot choose the value of $\\alpha$ parameter from the data used to fit alone; we call such a parameter an **hyper-parameter**.\n",
    "\n",
    "**Question:** what are other hyper-parameters we could optimize at this point?\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "In order to find the optimal value of an hyper-parameter, we can separate our training data into:\n",
    " * a **train set** : used to fit the model\n",
    " * a **validation set** : used to evaluate how our model perform on new data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## adding a validation set\n",
    "\n",
    "# we will use 60 points to train the model\n",
    "# and we will use the rest to evaluate the model \n",
    "I = list( range(X.shape[0]))\n",
    "np.random.shuffle( I ) \n",
    "\n",
    "I_train = I[:60]\n",
    "I_valid = I[60:]\n",
    "\n",
    "X_train = X.iloc[ I_train , : ] \n",
    "y_train = y.iloc[ I_train ]\n",
    "\n",
    "# we will use the rest to evaluate the model\n",
    "X_valid = X.iloc[ I_valid , : ] \n",
    "y_valid = y.iloc[ I_valid ]\n",
    "\n",
    "\n",
    "logalphas = []\n",
    "\n",
    "r2_train = []\n",
    "r2_valid = []\n",
    "\n",
    "for alpha in np.logspace(-3,2,200):\n",
    "\n",
    "    reg = SGDRegressor( penalty='l1' , alpha = alpha  )\n",
    "    reg.fit( X_train , y_train )\n",
    "    \n",
    "    logalphas.append(np.log10(alpha))\n",
    "    r2_train.append( r2_score( y_train , reg.predict(X_train) ) )\n",
    "    r2_valid.append( r2_score( y_valid , reg.predict(X_valid) ) )\n",
    "    \n",
    "## plotting and reporting \n",
    "bestI = np.argmax(r2_valid)\n",
    "bestLogAlpha = logalphas[bestI]\n",
    "bestR2_valid = r2_valid[bestI]\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (10,5))\n",
    "fig.suptitle(\"best alpha : {:.2f} - validation R2 : {:.2f}\".format(10**bestLogAlpha , bestR2_valid))\n",
    "ax.plot( logalphas, r2_train , label='train set' )\n",
    "ax.plot( logalphas, r2_valid , label='validation set' )\n",
    "ax.scatter( [bestLogAlpha] , [bestR2_valid]  , c='red')\n",
    "ax.set_xlabel(\"log10( alpha )\")\n",
    "ax.set_ylabel(\"R2\")\n",
    "ax.set_ylim(-0.1,1.1)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, with the help of a validation set, we can clearly see the phases :\n",
    " * **underfitting** : for high $\\alpha$, the performance is low for both the train and the validation set\n",
    " * **overfitting** : for low $\\alpha$, the performance is high for the train set, and low for the validation set\n",
    " \n",
    "We want the equilibrium point between the two where performance is ideal for the validation set.\n",
    "\n",
    "**Problem :** if you run the code above several time, you will see that the optimal point varies due to the random assignation to train or validation set. \n",
    "\n",
    "There exists a myriad of possible strategies to deal with that problem, such as repeating the above many times and taking the average of the results for instance.\n",
    "Note also that this problem gets less important as the validation set size increases.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Anyhow, on top of our earlier regression model, we have added :\n",
    "\n",
    " * an **hyper-parameter** : $\\alpha$, the strength of the regularization term\n",
    " * a **validation strategy** for our model in order to avoid overfitting\n",
    "\n",
    "<br>\n",
    "\n",
    "That's it, we are now in the world of Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we go any further, let's see how this modified model performs on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg = SGDRegressor( penalty='l1' , alpha = 10**bestLogAlpha  )\n",
    "reg.fit( X , y )\n",
    "\n",
    "y_pred = reg.predict( X )\n",
    "print(f\"train data R-squared score: { r2_score( y , y_pred ) :.2f}\")\n",
    "print(f\"train data mean squared error: { mean_squared_error(  y , y_pred ) :.2f}\")\n",
    "\n",
    "\n",
    "y_test_pred = reg.predict( X_test )\n",
    "\n",
    "print(f\" test data R-squared score: { r2_score( y_test , y_test_pred ) :.2f}\")\n",
    "print(f\" test data mean squared error: { mean_squared_error(  y_test , y_test_pred ) :.2f}\")\n",
    "\n",
    "\n",
    "plt.scatter( y , y_pred , label = 'training data' )\n",
    "plt.scatter( y_test , y_test_pred , label = 'new data' )\n",
    "plt.xlabel('observed values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things to observe:\n",
    " * we still see better performance on the train data than on the test data (generally always the case)\n",
    " * the performance on the test set has improved: our model is less overfit and more generalizable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "##  approach 3 : k-fold cross-validation <a class=\"anchor\" id=\"linear-3\"></a>\n",
    "\n",
    "In the previous approach, we have split our training data into a train set and a validation set.\n",
    "\n",
    "This approach works well if you have enough data for your validation set to be representative.\n",
    "\n",
    "Often, we unfortunately do not have enough data for this.\n",
    "\n",
    "Indeed, we have seen that if we run the code above several time, we see that the optimal point varies due to the random assignation to train or validation set. \n",
    "\n",
    "\n",
    "**K-fold cross validation** is one of the most common strategy to try to mitigate this randomness with a limited amount of data.\n",
    "\n",
    "![k-fold validation](images/kfold.png)\n",
    "\n",
    "In k-fold cross-validation, you split you data in $k$ subpart, called fold.\n",
    "\n",
    "Then, for a given hyper-parameter values combination, you actually train $k$ model: each time you use a different fold for validation (and the remaining $k-1$ folds for training).\n",
    "\n",
    "You then compute the average performance across all fold : this is the **cross-validated performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "We are going to do a simple k-fold manually once, to explore a bit how it works, but in practice you will discover that it is mostly automatized with some of scikit-learn's recipes and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kfold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5 , shuffle=True , random_state=734)\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={valid_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logalphas = np.linspace(-2,1,200)\n",
    "\n",
    "kf = KFold(n_splits=5 , shuffle=True , random_state=6581) ## try changing the random state\n",
    "\n",
    "fold_r2s = [ [] for i in range(kf.n_splits) ] ## for each fold\n",
    "cross_validated_r2 = [] # average across folds\n",
    "   \n",
    "for j,alpha in enumerate( 10**logalphas ) :\n",
    "\n",
    "    cross_validated_r2.append(0)\n",
    "    \n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(X)):\n",
    "\n",
    "        ## split train and validation sets\n",
    "        X_train = X.iloc[ train_index , : ]\n",
    "        X_valid = X.iloc[ valid_index , : ]\n",
    "\n",
    "        y_train = y.iloc[ train_index ]\n",
    "        y_valid = y.iloc[ valid_index ]\n",
    "\n",
    "        ## fit model for that fold\n",
    "        reg = SGDRegressor( penalty='l1' , alpha = alpha  )\n",
    "        reg.fit( X_train , y_train )\n",
    "\n",
    "        ## evaluate for that fold\n",
    "        fold_score = r2_score( y_valid , reg.predict(X_valid) )\n",
    "        \n",
    "        ## keeping in the curve specific to this fold\n",
    "        fold_r2s[i].append( fold_score )\n",
    "        \n",
    "        ## keeping a tally of the average across folds\n",
    "        cross_validated_r2[-1] += fold_score/kf.n_splits\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "bestI = np.argmax(cross_validated_r2)\n",
    "bestLogAlpha = logalphas[bestI] \n",
    "bestR2_valid = cross_validated_r2[bestI]\n",
    "\n",
    "\n",
    "## plotting\n",
    "fig,ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "ax.plot( logalphas, cross_validated_r2 , label='cross-validated r2' )\n",
    "ax.scatter( [bestLogAlpha] , [bestR2_valid]  , c='red')\n",
    "\n",
    "for i,scores in enumerate(fold_r2s):\n",
    "    ax.plot( logalphas , scores , label = f'fold {i}' , linestyle='dotted' )\n",
    "\n",
    "ax.set_xlabel(\"log10( alpha )\")\n",
    "ax.set_ylabel(\"R2\")\n",
    "ax.set_ylim(-0.1,1.1)\n",
    "ax.legend()\n",
    "\n",
    "fig.suptitle(\"best alpha : {:.2f} - cross-validated R2 : {:.2f}\".format(10**bestLogAlpha , bestR2_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**micro-exercise**: re-fit a model with the alpha we found and check the performance with the *test* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_mini2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, you can realize that now, for each possible value of our hyper-parameter we fit and evaluate not 1, but $k$ models, here 4.\n",
    "\n",
    "So, for 200 values of $\\alpha$, that means 200x5 = 1000 models to fit and evaluate.\n",
    "\n",
    "Now, consider that we have other hyper-parameters, such as the type of regularization (L1 or L2),\n",
    "or how we perform scaling, or whether we consider interactions, and now you understand why Machine Learning can quickly become  computationnaly intensive. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## approach 4 : a \"classical\" ML pipeline <a class=\"anchor\" id=\"linear-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start back from scratch to recapitulate what we've seen and use scikit-learn to solve the potato problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full dataset\n",
    "X = dfTT\n",
    "y = df[ \"Flesh Colour\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by splitting our data in a train and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train, y_test = train_test_split(X,y , test_size=0.2 )\n",
    "\n",
    "print('train set size:',len(y_train))\n",
    "print(' test set size:',len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a model while optimizing some hyper-parameters.\n",
    "\n",
    "On top of what we've done before, I add a scaling phase, and test l1 or l2 penalties.\n",
    "\n",
    "Scikit-learn's `GridSearchCV` is useful to explore these more \"complex\" hyper-parameter spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "## our pipeline will have 2 consecutive steps\n",
    "##   * standard scaling : set mean of each feature at 0 and (optionally) standard dev at 1 \n",
    "##   * linear regression with some regularization\n",
    "pip_reg = Pipeline([('scaler',StandardScaler()),\n",
    "                    ('model',SGDRegressor())])\n",
    "\n",
    "\n",
    "\n",
    "# define the hyperparameters you want to test\n",
    "# with the range over which you want it to be tested.\n",
    "# \n",
    "# They are given in a dictionary with the structure:\n",
    "#      pipelineStep__parameter : [set of values to explore]\n",
    "#                  ^^\n",
    "#                  note the double underscore _\n",
    "grid_values = {'scaler__with_std' : [ True , False ],\n",
    "               'model__penalty':[ 'l1' , 'l2' ],\n",
    "               'model__alpha':np.logspace(-2,2,200)}\n",
    "\n",
    "\n",
    "# Feed the pipeline and set of values to the GridSearchCV with the \n",
    "# score over which the decision should be taken (here, R^2).\n",
    "# and the cross-validation scheme, here the number of fold in a stratified k-fold strategy\n",
    "grid_reg = GridSearchCV(pip_reg, \n",
    "                        param_grid = grid_values, \n",
    "                        scoring='r2', \n",
    "                        cv = 5,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "# When the actual fit happens\n",
    "#  the gridSearchCV object will go through each hyperparameter value combination\n",
    "#  and fit + evaluate each fold, and averages the score across each fold.\n",
    "#\n",
    "#  It then finds the combination that gave the best score and\n",
    "#  use it to re-train a model with the whole train data\n",
    "grid_reg.fit(X_train, y_train)\n",
    "\n",
    "# get the best cross-validated score \n",
    "print(f'Grid best score ({grid_reg.scoring}): {grid_reg.best_score_:.3f}')\n",
    "\n",
    "# print the best parameters\n",
    "print('Grid best parameter :')\n",
    "for k,v in grid_reg.best_params_.items():\n",
    "    print(' {:>20} : {}'.format(k,v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gridSearch CV fits a new estimator with the best hyperparameter values\n",
    "reg = grid_reg.best_estimator_\n",
    "\n",
    "y_pred = reg.predict( X_train )\n",
    "print(f\"train data R-squared score: { r2_score( y_train , y_pred ) :.2f}\")\n",
    "print(f\"train data mean squared error: { mean_squared_error(  y_train , y_pred ) :.2f}\")\n",
    "\n",
    "\n",
    "y_test_pred = reg.predict( X_test )\n",
    "\n",
    "print(f\" test data R-squared score: { r2_score( y_test , y_test_pred ) :.2f}\")\n",
    "print(f\" test data mean squared error: { mean_squared_error(  y_test , y_test_pred ) :.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter( y_train , y_pred , label = 'training data' )\n",
    "plt.scatter( y_test , y_test_pred , label = 'new data' )\n",
    "plt.xlabel('observed values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also access the best model parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_reg.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coefficient of the linear model\n",
    "grid_reg.best_estimator_['model'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to ToC](#toc)\n",
    "\n",
    "# Logistic regression <a class=\"anchor\" id=\"toy-example-lr\"></a>\n",
    "\n",
    "Let's imagine a simple case with 2 groups, and a single feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.concatenate( [ np.random.randn(300) , np.random.randn(300)+4 ])\n",
    "y = np.array( [0]*300 + [1]*300 )\n",
    "\n",
    "sns.histplot( x=X1,hue = y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a logistic regression to model the relationship between the class and the feature.\n",
    "\n",
    "Remember : **Logistic regression does not model the class directly, but rather model the class probabilities** (through the logit transform)\n",
    "\n",
    "Let's see how regularization affect the class probabilities found by our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# do not forget to scale the data\n",
    "X1_norm = StandardScaler().fit_transform(X1.reshape( X1.shape[0] , 1 ))\n",
    "\n",
    "fig,ax = plt.subplots( figsize = (10,5) )\n",
    "\n",
    "ax.scatter( X1_norm , y , c = y )\n",
    "\n",
    "for alpha in [0.01,0.1,1,10]:\n",
    "    \n",
    "    # this implementation does not take alpha but rather C = 1/alpha\n",
    "    C = 1/alpha\n",
    "    lr = LogisticRegression( penalty = 'l2' , C = C )\n",
    "    lr.fit(X1_norm , y)\n",
    "    \n",
    "    proba = lr.predict_proba(np.linspace(-2,2,100).reshape(-1, 1))\n",
    "    ax.plot( np.linspace(-2,2,100) , proba[:,1] , label = 'alpha = {}'.format(alpha) )\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **when $\\alpha$ grows the probabilities evolve more smoothly** ie. we have more regularization.\n",
    "\n",
    "> However, note that all the curves meet at the same point, corresponding to the 0.5 probability.\n",
    "\n",
    "This is nice, but **our end-goal is to actually be able to predict the classes**, and not just the probabilities.\n",
    "\n",
    "Our task is not regression anymore, but rather **classification**.\n",
    "\n",
    "So here, we do not evaluate the model using $R^2$ or log-likelihood, but a classification metric.\n",
    "\n",
    "we will discuss a few of these metrics, and we will begin by the most common: **Accuracy**\n",
    "\n",
    "\n",
    "The Accuracy is the proportion of samples which were correctly classified (as either category).\n",
    "\n",
    "More mathematically:\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP+FP+FN+TN}$$\n",
    "\n",
    "![image/TPFP.png](image/TPFP.png)q\n",
    "Image credit wikipedia user Sharpr for svg version. original work by kakau in a png. Licensed under the [Creative Commons Attribution-Share Alike 3.0 Unported license](https://creativecommons.org/licenses/by-sa/3.0/deed.en).\n",
    "\n",
    "* TP : True Positive\n",
    "* FP : False Positive\n",
    "* TN : True Negative\n",
    "* FN : False Negative\n",
    "\n",
    "So you can see that accuracy forces us to make a choice about the **probability threshold we use predict categories**.\n",
    "\n",
    "0.5 is a common choice, and the default of the `predict` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_predicted = lr.predict(X1_norm)\n",
    "\n",
    "print( f\"Accuracy with a threshold of 0.5 : {accuracy_score(y,y_predicted):.2f}\"  )\n",
    "\n",
    "pd.crosstab( y , y_predicted , rownames = ['observed'] , colnames = ['predicted'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be useful to remember that this is only 1 choice among many:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "y_predicted = lr.predict_proba(X1_norm)[:,1] > threshold\n",
    "print( f\"Accuracy with a threshold of {threshold} : {accuracy_score(y,y_predicted):.2f}\"  )\n",
    "pd.crosstab( y , y_predicted , rownames = ['observed'] , colnames = ['predicted'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro-exercise :** modify the threhold in the code above. \n",
    " * in which direction should the threshold move to limit the number of False Positive ?\n",
    " * for which application could that be useful ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to ToC](#toc)\n",
    "\n",
    "## Exercise: Logistic regression to detect breast cancer malignancy <a class=\"anchor\" id=\"LR-exercise\"></a>\n",
    "\n",
    "Let's build a logistic regression model that will be able to predict if a breast tumor is malignant or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "#loading the dataset which is comprised in scikit learn already\n",
    "data = load_breast_cancer()\n",
    "\n",
    "## we reduce the features because otherwise this problem is a bit too easy ;-)\n",
    "m = list( map( lambda x : x.startswith(\"mean \") , data[\"feature_names\"] ) )\n",
    "\n",
    "\n",
    "X_cancer=data['data'][:,m]\n",
    "y_cancer= 1-data['target']\n",
    "\n",
    "#making it into a dataframe\n",
    "breast_cancer_df=pd.DataFrame(X_cancer,\n",
    "    columns=data[\"feature_names\"][m])\n",
    "\n",
    "breast_cancer_df[\"target\"]=y_cancer\n",
    "\n",
    "breast_cancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "breast_cancer_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 357 benign samples\n",
    "* 212 malignant samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all these covariables / features are defined on very different scales, for them to be treated fairly in their comparison you need to take that into account by scaling.\n",
    "\n",
    "\n",
    "\n",
    "**task 1:** split the data into a train and a test dataset\n",
    "\n",
    "Complete the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split your data\n",
    "\n",
    "# stratify is here to make sure that you split keeping the repartition of labels unaffected\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = ...\n",
    "\n",
    "\n",
    "print(\"fraction of class malignant in train\",sum(y_train_cancer)/len(y_train_cancer))\n",
    "print(\"fraction of class malignant in test \",sum(y_test_cancer)/len(y_test_cancer) )\n",
    "print(\"fraction of class malignant in full \",sum(y_cancer)/len(y_cancer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task 2:** design your pipeline and grid search\n",
    "\n",
    "Complete the following cell to perform hyper-parameter search with the following specification:\n",
    "\n",
    "* hyperparameters:\n",
    "    * scaler : no hyper-parameters for the scaler (ie, we will keep the defaults)\n",
    "    * logistic regression : test different values for `C` and `penalty` \n",
    "* score: 'accuracy'\n",
    "* cross-validation: use 10 folds\n",
    "\n",
    "\n",
    "> If you want to test the elasticnet penalty, you will also have to adapt the `solver` parameter (cf. the [LogisticRegression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline_lr_cancer=Pipeline([('scaler',StandardScaler()),\n",
    "                             ('model',LogisticRegression(solver = 'liblinear'))])\n",
    "\n",
    "#define the hyper-parameter space to explore\n",
    "grid_values = { ... }\n",
    "\n",
    "#define the GridSearchCV object\n",
    "grid_cancer = GridSearchCV( ... )\n",
    "\n",
    "#train your pipeline\n",
    "grid_cancer.fit( ... )\n",
    "\n",
    "\n",
    "#get the best cross-validated score \n",
    "print(f'Grid best score ({grid_cancer.scoring}): {grid_cancer.best_score_:.3f}')\n",
    "# print the best parameters\n",
    "print('Grid best parameter :')\n",
    "for k,v in grid_cancer.best_params_.items():\n",
    "    print(' {:>20} : {}'.format(k,v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task 1:** split the data into a train and a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r -13 solutions/solution_03_LR_cancer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task 2:** design your pipeline and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 14- solutions/solution_03_LR_cancer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there we can explore the model a bit further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then access the coefficient of the model, to assess the importance of the different parameters:\n",
    "\n",
    "w_lr_cancer = grid_cancer.best_estimator_['model'].coef_[0]\n",
    "\n",
    "sorted_features=sorted( zip( breast_cancer_df.columns , w_lr_cancer  ),\n",
    "                       key= lambda x : np.abs( x[1] ) , # sort by absolute value\n",
    "                       reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for f,ww in sorted_features:\n",
    "    print('{:>25}\\t{:.3f}'.format(f,ww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_cancer_test_pred = grid_cancer.predict(X_test_cancer)\n",
    "\n",
    "# get the confusion matrix:\n",
    "confusion_m_cancer = confusion_matrix(y_test_cancer, y_cancer_test_pred)\n",
    "\n",
    "## recipe to plot the confusion matrix : \n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_m_cancer, annot=True)\n",
    "plt.title(f'Accuracy:{accuracy_score(y_test_cancer,y_cancer_test_pred):.3f}')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with its default threshold of 0.5, this model tends to produce more False Positive (ie. benign cancer seen as malignant), than False Negative (ie. malignant cancer seen as benign).\n",
    "\n",
    "Depending on the particular of the problem we are trying to solve, that may be a desirable outcome.\n",
    "\n",
    "Whatever the case, it is always interesting to explore a bit more : we will plot how each possible threshold affect the True Positive Rate and the False Positive Rate (**TPR and FPR**) : this is the Receiver Operating Characteristic c urve (**ROC curve**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# 1. the predict_proba gives you the probability in each class.\n",
    "#       we want to have the probability of being of class 1 (malignant), so we take the second column\n",
    "y_proba_lr_cancer = grid_cancer.predict_proba(X_test_cancer)[:,1]\n",
    "\n",
    "#   for the logistic regression, that score is the logit, so can convert it back to \n",
    "#   probabilities with the expit function (which is the inverse of the logit)\n",
    "#y_proba_lr_cancer = expit( y_score_lr_cancer )\n",
    "\n",
    "# 2. this calculates the ROC curve : TPR and FPR for each threshold of score\n",
    "fpr_lr_cancer, tpr_lr_cancer, threshold_cancer = roc_curve(y_test_cancer, y_proba_lr_cancer)\n",
    "\n",
    "# we find the point corresponding to a 0.5 theshold\n",
    "keep = np.argmin( np.abs( threshold_cancer - 0.5 ) )\n",
    "\n",
    "# we compute the area under the ROC curve\n",
    "roc_auc_lr_cancer = auc( fpr_lr_cancer, tpr_lr_cancer )\n",
    "\n",
    "# 3. plotting \n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_lr_cancer, tpr_lr_cancer, lw=3, label='LogRegr ROC curve\\n (area = {:0.2f})'.format(roc_auc_lr_cancer))\n",
    "plt.plot(fpr_lr_cancer[keep], tpr_lr_cancer[keep],'ro',label='threshold=0.5')\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve (logistic classifier)', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with this ROC curve, we can see how the model would behave on different thresholds.\n",
    "\n",
    "**Question:** we have marked the 0.5 threshold on the plot. Where would a higher threshold be on the curve?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can see that when plotting the ROC curve, I have also computed its \"Area Under the Curve\" : \n",
    "indeed ROC AUC is another common metric when doing classification.\n",
    "\n",
    "\n",
    "For now, let's put this aside briefly to explore a very common problem in classification : imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Imbalanced dataset <a class=\"anchor\" id=\"imbalanced\"></a> \n",
    "\n",
    "Let's use the same small example as before, but now instead of 300 sample of each class, imagine we only have 30 of class 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.concatenate( [ np.random.randn(300) , np.random.randn(30)+2 ])\n",
    "y = np.array( [0]*300 + [1]*30 )\n",
    "\n",
    "# do not forget to scale the data\n",
    "X1_norm = StandardScaler().fit_transform(X1.reshape( X1.shape[0] , 1 ))\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize = (14,5) )\n",
    "\n",
    "sns.histplot( x=X1,hue = y , ax =ax [0])\n",
    "\n",
    "\n",
    "ax[1].scatter( X1_norm , y , c = y )\n",
    "\n",
    "for alpha in [0.01,0.1,1,10]:\n",
    "    \n",
    "    # this implementation does not take alpham but rather C = 1/alpha\n",
    "    C = 1/alpha\n",
    "    lr = LogisticRegression( penalty = 'l2' , C = C )\n",
    "    lr.fit(X1_norm , y)\n",
    "    \n",
    "    proba = lr.predict_proba(np.linspace(-2,3,100).reshape(-1, 1))\n",
    "    ax[1].plot( np.linspace(-2,3,100) , proba[:,1] , label = 'alpha = {}'.format(alpha) )\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that now the point where the probability curves for different alpha converge is not 0.5 anymore...\n",
    "\n",
    "Also, the probability says fairly low even at the right end of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = lr.predict(X1_norm)\n",
    "print( f\"Accuracy with a threshold of 0.5 : {accuracy_score(y,y_predicted):.2f}\"  )\n",
    "pd.crosstab( y , y_predicted )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, most of the class 1 samples are miss-classified (22/30), but we still get a very high accuracy...\n",
    "\n",
    "That is because, by contruction, both the **logistic regression and accuracy score do not differentiate False Positive and False Negative**.\n",
    "\n",
    "And the problem gets worse the more imbalance there is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "## RECALL = TP / (TP + FN)\n",
    "\n",
    "recall_list = []\n",
    "acc_list = []\n",
    "\n",
    "imbalance_list = np.linspace(0,1,50)\n",
    "\n",
    "alpha = 1\n",
    "for imbalance in imbalance_list:\n",
    "\n",
    "    n0 = 300\n",
    "    n1 = int( n0 * (1 - imbalance) )\n",
    "    if n1 == 0:\n",
    "        n1 = 1\n",
    "    \n",
    "    X1 = np.concatenate( [ np.random.randn(n0) , np.random.randn(n1)+2 ])\n",
    "    y = np.array( [0]*n0 + [1]*n1 )\n",
    "\n",
    "    X1_norm = StandardScaler().fit_transform(X1.reshape( X1.shape[0] , 1 ))\n",
    "    \n",
    "    C = 1/alpha\n",
    "    lr = LogisticRegression( penalty = 'l2' , C = C )\n",
    "    lr.fit(X1_norm , y)\n",
    "    \n",
    "    y_predicted = lr.predict(X1_norm)\n",
    "    \n",
    "    recall_list.append( recall_score( y , y_predicted ) )\n",
    "    acc_list.append( accuracy_score(y,y_predicted) )\n",
    "\n",
    "        \n",
    "fig,ax=plt.subplots(figsize = (10,4))\n",
    "ax.plot( imbalance_list , acc_list , label='accuracy' )\n",
    "ax.plot( imbalance_list , recall_list , label='recall' )\n",
    "ax.set_xlabel(\"imbalance\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not only does the precision get worse, the **accuracy actually gets higher as there is more imbalance!**\n",
    "\n",
    "So the problem here may be 2-fold:\n",
    " * imbalance in our dataset skews the **logistic regression** toward a particular outcome\n",
    " * **accuracy** is not able to differenciate between False Positive and False Negative, and so it is **blind to imbalance**\n",
    "\n",
    "Consequently, the solutions will have to come both from the model, and from the metric we are using.\n",
    "\n",
    "\n",
    "**For the logistic regression**:\n",
    " * we will re-weight sample according to their class frequency, so that they are more important during the fitting.\n",
    " * in sklearn : `LogisticRegression( ... , class_weight='balanced')`\n",
    " \n",
    "<br> \n",
    "\n",
    "**For the metric**, there exists several metrics which are sensitive to imbalance problems. \n",
    "Here we will introduce the **[balanced accuracy](https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score)**:\n",
    "\n",
    "$$balanced\\_accuracy = 0.5*( \\frac{TP}{TP+FN} + \\frac{TN}{TN+FP} )$$\n",
    "\n",
    "> Other scores you may want to look-up : [average-precision score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score), and [f1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score), which are both linked to the precision/recall curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "\n",
    "def check_imbalance_effect( imbalance_list , class_weight = None):\n",
    "    \n",
    "    recall_list = []\n",
    "    balanced_acc_list = []\n",
    "    acc_list = []\n",
    "    for imbalance in imbalance_list:\n",
    "\n",
    "        n0 = 300\n",
    "        n1 = int( n0 * (1 - imbalance) )\n",
    "        if n1 == 0:\n",
    "            n1 = 1\n",
    "\n",
    "        X1 = np.concatenate( [ np.random.randn(n0) , np.random.randn(n1)+2 ])\n",
    "        y = np.array( [0]*n0 + [1]*n1 )\n",
    "\n",
    "        X1_norm = StandardScaler().fit_transform(X1.reshape( X1.shape[0] , 1 ))\n",
    "\n",
    "        # LR\n",
    "        lr = LogisticRegression( penalty = 'l2' , C = 1 , class_weight=class_weight)\n",
    "        lr.fit(X1_norm , y)\n",
    "\n",
    "        y_predicted = lr.predict(X1_norm)\n",
    "\n",
    "        recall_list.append( recall_score( y , y_predicted )  )\n",
    "        acc_list.append( accuracy_score(y,y_predicted) )\n",
    "        balanced_acc_list.append( balanced_accuracy_score(y,y_predicted) )\n",
    "\n",
    "    return recall_list , acc_list , balanced_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_list = np.linspace(0,1,50)\n",
    "\n",
    "### first, we see what happens without class_weight=None\n",
    "\n",
    "recall_list , acc_list , balanced_acc_list = check_imbalance_effect( imbalance_list , \n",
    "                                                               class_weight = None)\n",
    "    \n",
    "        \n",
    "fig,ax=plt.subplots(1,2,figsize = (12,4))\n",
    "ax[0].plot( imbalance_list , acc_list , label='accuracy - class_weight=None' )\n",
    "ax[0].plot( imbalance_list , recall_list , label='recall - class_weight=None' )\n",
    "ax[0].plot( imbalance_list , balanced_acc_list , label='balanced_accuracy - class_weight=None' )\n",
    "ax[0].set_xlabel(\"imbalance\")\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[0].legend()\n",
    "## now, with class weight \n",
    "\n",
    "recall_list , acc_list , balanced_acc_list = check_imbalance_effect( imbalance_list , \n",
    "                                                               class_weight = 'balanced')\n",
    "            \n",
    "ax[1].plot( imbalance_list , acc_list , label='accuracy - class_weight=balanced' )\n",
    "ax[1].plot( imbalance_list , recall_list , label='recall - class_weight=balanced' )\n",
    "ax[1].plot( imbalance_list , balanced_acc_list , label='balanced_accuracy - class_weight=balanced' )\n",
    "ax[1].set_xlabel(\"imbalance\")\n",
    "ax[1].set_ylim(0,1)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the **balanced accuracy** is able to detect an imbalance problem.\n",
    "\n",
    "Setting `class_weight='balanced'` in our logistic regression fixes the imbalance at the level of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**micro-exercise**:  re-explore the hyper-parameters of the logisitic regression for the cancer data-set, but this time, account for the imbalance between malignant and benign samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**If you want to use a GLM other than the logistic regression:**\n",
    "[GLM in sklearn](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# A few VERY IMPORTANT words on leakage. <a class=\"anchor\" id=\"leakage\"></a>\n",
    "\n",
    "The most important part in all of the machine learning jobs that we have been presenting above, is that **the data set on which you train and the data set on which you evaluate your model should be clearly separated**(either the validation set when you do hyperparameter tunning, or test set for the final evaluation). \n",
    "\n",
    "No information directly coming from your test or your validation should pollute your train set. If it does you **loose your ablity to have a meaningful evaluation power.** \n",
    "\n",
    "In general **data leakage** relates to every bits of information that you should not have access to in a real case scenario, being present in your training set.\n",
    "\n",
    "Among those examples of data leakage you could count : \n",
    "* **using performance on the test set to decide which algorithm/hyperparameter to use**\n",
    "* doing imputation or scaling before the train/test split\n",
    "* inclusion of future data points in a time dependent or event dependent model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Support Vector Machine <a class=\"anchor\" id=\"svm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for Classification <a class=\"anchor\" id=\"svm-c\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduction <a class=\"anchor\" id=\"formal-svm-c\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The basic principle of SVM is pretty simple. SVM aims at finding the 'good' threshold (hyperplane) to separate data from different classes. Conceptually it is very different from logistic regression where you maximize the log likelihood of the log odds function. **With SVM you really look for an hyperplane that separates data and that's it : there is no underlying hypothesis about probability distribution or anything else. It is very geometrical.**\n",
    "\n",
    "So what's a good threshold? Again it is going to depend on the metric you are interested in. But at least a good threshold should be linked to this biais variance trade off in the sense that it should offer flexibility to your model.\n",
    "\n",
    "You can imagine that there is a quite a lot of hyperplanes separating data in your training set. You could stick your threshold right where the class 0 point closest to class 1 lies. But in that case it will be very far from the other class 0 points, which can be a problem. **You could decide that your threshold should be right between the two closest extreme of your classes but that is going to be very sensitive to missclassified data or extreme events... Those points choosen as a reference to put your threshold are called support vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10815657)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,7))\n",
    "\n",
    "# case 1 \n",
    "norm1=0.2*np.random.randn(100)-2\n",
    "norm2=0.8*np.random.randn(100)+2.5\n",
    "ax[0].plot(norm1,[1]*100,'ro',markersize=10)\n",
    "ax[0].plot(norm2,[1]*100,'bo')\n",
    "\n",
    "min2 = min( norm2 )\n",
    "max1 = max( norm1 )\n",
    "\n",
    "ax[0].axvline(min2, color='k', linestyle='--', label='defined by the most extreme blue point')\n",
    "ax[0].axvline( (min2+max1)/2,color='k',linestyle='-.',label='middle of extreme of two classes')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "\n",
    "# case 2\n",
    "cauch=0.8*np.random.standard_cauchy(10)-2\n",
    "norm=1*np.random.randn(100)+2.5\n",
    "\n",
    "ax[1].plot(cauch,[1]*10,'ro',markersize=10)\n",
    "ax[1].plot(norm,[1]*100,'bo')\n",
    "\n",
    "min2 = min( norm )\n",
    "max1 = max( cauch )\n",
    "\n",
    "ax[1].axvline(min2, color='k', linestyle='--', label='defined by the most extreme blue point')\n",
    "ax[1].axvline( (min2+max1)/2,color='k',linestyle='-.',label='middle of extreme of two classes')\n",
    "ax[1].legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left panel, the two hyperplanes are valid separation but you can imagine that the plane defined by the most extreme blue point doesn't leave much space for generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data are not linearly separable, like in the right panel, you need to be able to choose support vectors that are going to do some misclassification but for the greater good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, once again, you are confronted to a compromise. You should place your threshold somwhere that is globally best even though that would mean some miss-classification. We are back to our regularization problem and of course **Support vector machine has a regularization parameter : C**. The game now becomes placing your threshold right in the middle of points (support vectors) from  each classes that you have \\\"chosen\\\" to be general points for decision making : **they don't need to be the two closest points from different classes anymore. They need to be points where your hyperplane makes the least error differentiating classes.**\n",
    "\n",
    "\n",
    "![svm](image/1920px-SVM_margin.png)\n",
    "\n",
    "Image source : image by wikipedia user Larhmam, distributed under a [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/deed.en).\n",
    "\n",
    "\n",
    "So you want to maximize the margin separating the two classes. This margin is $\\frac{2}{||\\pmb{w}||}$. So you want to minimize $||\\pmb{w}||$. The SVM loss function we want to minimize with respect to $\\pmb{w}$ and $b$ is:\n",
    "\n",
    "$C\\cdot\\Sigma^{N}_{i=1}\\zeta_i + \\frac{1}{2}||\\pmb{w}||^{2}$ subject to $\\zeta_i \\ge 0$ and $y_{i}(w^{T}x_{i}-b) \\ge 1-\\zeta_i$, where $\\zeta_i = \\Sigma^{N}_{i=1}max(0,1-y_{i}(\\pmb{w}\\cdot\\pmb{x}_i-b))$\n",
    " * $y_i$ is $-1$ or $1$ depending on the class of the point $i$\n",
    " * the class of point $\\pmb{x}$ is determined by the SVM using the sign of $(\\pmb{w}\\cdot\\pmb{x}-b)$ (ie, on which side of the $(\\pmb{w}\\cdot\\pmb{x}-b)$ hyperplane we are).\n",
    "\n",
    "\n",
    "\n",
    "Note that you could also use a L1 regularization but it is not implemented in the function we are going to use.\n",
    "\n",
    "Indeed if most of the data points are well separated in term of class on each side of the hyperplane then\n",
    "* most of the time $y_{k}(w^{T}x_{k}-b) \\geq 1$ and so $max(0,1-y_{k}(w^{T}x_{k}-b)=0$ (that's good for minimizing our loss function), \n",
    "* and a few times $y_{k}(w^{T}x_{k}-b) \\leq -1$ and so $max(0,1-y_{k}(w^{T}x_{k}-b) \\geq 2$ (which is polluting our minimization of the loss function).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can see that there is a [dot product](https://en.wikipedia.org/wiki/Dot_product) involved : in the case of a linear hyperplane this dot product is just the cartesian dot product that you probably use all the time. It allows you to calculate distances between points in that cartesian space or between points and hyperplanes. But you might be familiar with other scalar product : like for example when you proceed to a Fourier decomposition of a function. This particular scalar product acts on functions and so is not really of interest for us... But others exist.\n",
    "\n",
    "**So in principle you could use other definitions of distance between points to answer that classification question**. This is what non-linear SVM does and this is why you can choose different so called kernels as hyperparameters as we will see below :\n",
    "\n",
    "$\\overrightarrow{x_{i}}.\\overrightarrow{x_{j}}$ : cartesian\n",
    "\n",
    "$(\\overrightarrow{x_{i}}.\\overrightarrow{x_{j}})^{d}$ : polynomial degree d\n",
    "\n",
    "$exp(-\\gamma||\\overrightarrow{x_{i}}-\\overrightarrow{x_{j}}||^{2})$ : gaussian radial basis\n",
    "\n",
    "$tanh(\\kappa\\overrightarrow{x_{i}}.\\overrightarrow{x_{j}}+c)$ : hyperbolic tangent\n",
    "\n",
    "**This is really powerful for classification but going non-linear by using a kernel trick prevents you from interpreting how your features are massaged to create this classifier... So, if you want interpretability and do science rather than engineering : keep it linear.**\n",
    "\n",
    "![3d_svm](image/3d_svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even though SVM as nothing to do with probablities, we are going to transform the results of our classifier back to probabilities (using logistic regression...) to be able to draw a ROC curve. But again I insist, those are just useful transformations but has actually nothing to do with the technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the toc](#toc)\n",
    "\n",
    "### Toy example to visualize SVMC <a class=\"anchor\" id=\"toy-example-svm-c\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X2, y2 = make_blobs(n_samples=(250,250), centers=[[-1,-1],[1,1]], random_state=6)\n",
    "plt.scatter(X2[:,0],X2[:,1],c=y2)\n",
    "plt.xlim(min(X2[:,1])-0.5,max(X2[:,1])+0.5)\n",
    "plt.ylim(min(X2[:,1])-0.5,max(X2[:,1])+0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import contour_SVM\n",
    "\n",
    "contour_SVM(X2,y2,c=100,ker='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_SVM(X2,y2,c=0.01,ker='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_SVM(X2,y2,c=1,ker='rbf',gam=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_SVM(X2,y2,c=1,ker='rbf',gam=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_SVM(X2,y2,c=1,ker='poly',deg=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the toc](#toc)\n",
    "\n",
    "### SVM Classifier pipeline. <a class=\"anchor\" id=\"svm-c-pipeline\"></a>\n",
    "\n",
    "In sklearn SVM classifier is implemented in a single `sklearn.svm.SVC` class,\n",
    "but as we have seen depending on the kernel you have different parameters.\n",
    "\n",
    "That means that when you specify the grid of hyper-parameters to explore, you could write something like this:\n",
    "\n",
    "```python\n",
    "grid_values = {\"model__kernel\": ['linear', 'rbf', 'poly'],\n",
    "                 \"model__C\":np.logspace(-2, 2, 10),\n",
    "                 \"model__degree\":[2,3,4,5],\n",
    "                 \"model__gamma\": np.logspace(-2,1,10)}\n",
    "```\n",
    "\n",
    "That would work, however that means that even though parameter `gamma` is useless when the kernel is `'linear'`, the GridSearchCV will still test all the different combination of values.\n",
    "On this example, rather than testing :\n",
    " * 10 combinations for linear kernel (C)\n",
    " * 10*4 = 40 combinations for poly kernel (C and degree)\n",
    " * 10*10 = 100 combinations for rbf kernel (C and gamma)\n",
    "\n",
    "= 150 combinations to test.\n",
    "\n",
    "You would test $3*10*4*10 = 1200$ combinations...\n",
    "\n",
    "Let's see how we can handle this smartly to avoid these useless computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "## set up the pipeline as usual\n",
    "pipe = Pipeline([('scalar',StandardScaler()),\n",
    "                 (\"classifier\", SVC())])\n",
    "\n",
    "# the grid of parameter values is not a dictionnary, but now a list \n",
    "#  of dictionnaries : smaller grid to explore independently\n",
    "\n",
    "# for each of these grid, we can re-define locally the classifier\n",
    "grid_param = [  {\"classifier\": [SVC(class_weight='balanced', probability=True, kernel='linear')],\n",
    "                 \"classifier__C\":np.logspace(-2, 2, 10)},\n",
    "                {\"classifier\": [SVC(class_weight='balanced', probability=True, kernel='rbf')],\n",
    "                 \"classifier__C\":np.logspace(-2, 2, 10),\n",
    "                 \"classifier__gamma\": np.logspace(-2,1,10)},\n",
    "                {\"classifier\": [SVC(class_weight='balanced', probability=True, kernel='poly')],\n",
    "                 \"classifier__C\":np.logspace(-2, 2, 10),\n",
    "                 \"classifier__degree\":[2,3,4,5]} ]\n",
    "## NB : we use probability = True in order to make ROC auc computation possible later on\n",
    "\n",
    "grid_svm = GridSearchCV(pipe, \n",
    "                        grid_param, \n",
    "                        cv=5, \n",
    "                        verbose=0,\n",
    "                        n_jobs=-1,\n",
    "                        scoring='accuracy') \n",
    "\n",
    "grid_svm.fit(X_train_cancer, y_train_cancer)\n",
    "\n",
    "#get the best cross-validated score \n",
    "print('Grid best score ('+grid_svm.scoring+'): ',\n",
    "      grid_svm.best_score_)\n",
    "# print the best parameters\n",
    "print('Grid best kernel    :' , grid_svm.best_params_[\"classifier\"].kernel )\n",
    "print('Grid best parameter :')\n",
    "for k,v in grid_svm.best_params_.items():\n",
    "    print(' {:>20} : {}'.format(k,v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the same as above could be done with multiple separate grid searchs. We would **compare them by their cross-validated score** : `grid.best_score_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "scoring_metric = \"accuracy\"\n",
    "\n",
    "# linear kernel\n",
    "pipe1 = Pipeline([('scalar',StandardScaler()),\n",
    "                 (\"classifier\", SVC(class_weight='balanced', probability=True, kernel='linear'))])\n",
    "grid_param1 = { \"classifier__C\":np.logspace(-2, 2, 10)}\n",
    "\n",
    "grid_svm_linear = GridSearchCV(pipe1, grid_param1, \n",
    "                        cv=5, verbose=0, n_jobs=-1, scoring=scoring_metric) \n",
    "\n",
    "# rbf kernel\n",
    "pipe2 = Pipeline([('scalar',StandardScaler()),\n",
    "                 (\"classifier\", SVC(class_weight='balanced', probability=True, kernel='rbf'))])\n",
    "grid_param2 = { \"classifier__C\":np.logspace(-2, 2, 10) , \n",
    "               \"classifier__gamma\": np.logspace(-2,1,10)}\n",
    "\n",
    "grid_svm_rbf = GridSearchCV(pipe2, grid_param2, \n",
    "                        cv=5, verbose=0, n_jobs=-1, scoring=scoring_metric) \n",
    "\n",
    "# poly kernel\n",
    "pipe3 = Pipeline([('scalar',StandardScaler()),\n",
    "                 (\"classifier\", SVC(class_weight='balanced', probability=True, kernel='poly'))])\n",
    "grid_param3 = { \"classifier__C\":np.logspace(-2, 2, 10) , \n",
    "               \"classifier__degree\": [2,3,4,5]}\n",
    "\n",
    "grid_svm_poly = GridSearchCV(pipe3, grid_param3, \n",
    "                        cv=5, verbose=0, n_jobs=-1, scoring=scoring_metric) \n",
    "\n",
    "\n",
    "\n",
    "grid_svm_linear.fit(X_train_cancer, y_train_cancer)\n",
    "grid_svm_rbf.fit(X_train_cancer, y_train_cancer)\n",
    "grid_svm_poly.fit(X_train_cancer, y_train_cancer)\n",
    "\n",
    "\n",
    "print('linear Grid best score ('+grid_svm_linear.scoring+'): ',grid_svm_linear.best_score_)\n",
    "print('   rbf Grid best score ('+grid_svm_rbf.scoring+'): '   ,grid_svm_rbf.best_score_)\n",
    "print('  poly Grid best score ('+grid_svm_poly.scoring+'): '  ,grid_svm_poly.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end we get the same result.\n",
    "\n",
    "> This is also the approach we would take to decide between logistic regression, SVM, or other models.\n",
    "\n",
    "Anyhow, let's look at the best model performance in more depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cancer_test_score=grid_svm.score(X_test_cancer,y_test_cancer)\n",
    "\n",
    "print('Grid best parameter (max.'+grid_svm.scoring+') model on test: ', y_cancer_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cancer_pred_test=grid_svm.predict(X_test_cancer)\n",
    "\n",
    "confusion_m_cancer_SVMC = confusion_matrix(y_test_cancer, y_cancer_pred_test)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_m_cancer_SVMC, annot=True)\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title(\"test {} : {:.3f}\".format(grid_svm.scoring , y_cancer_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_cancer_score_SVMC = grid_svm.decision_function(X_test_cancer)\n",
    "fpr_SVMC_cancer, tpr_SVMC_cancer, thre_SVMC_cancer = roc_curve(y_test_cancer, y_cancer_score_SVMC)\n",
    "roc_auc_SVMC_cancer = auc(fpr_SVMC_cancer, tpr_SVMC_cancer)\n",
    "\n",
    "proba=expit(thre_SVMC_cancer)\n",
    "for i in range(len(proba)):\n",
    "    if abs(proba[i]-0.5)<0.1:\n",
    "        keep=i\n",
    "        break\n",
    "        \n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_SVMC_cancer, tpr_SVMC_cancer, lw=3, label='SVC ROC curve\\n (area = {:0.2f})'.format(roc_auc_SVMC_cancer))\n",
    "plt.plot(fpr_SVMC_cancer[keep], tpr_SVMC_cancer[keep],'ro',label='threshold=0.5')\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve (SVM classifier)', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "#plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here the kernel is RBF, so we have no coefficients to grab from the model for interpretation.\n",
    "\n",
    "But, we still have some options.\n",
    "\n",
    "For instance, we present here [permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html): it is the **decrease in a model score when a single feature value is randomly shuffled**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "r = permutation_importance(grid_svm.best_estimator_, X_test_cancer, y_test_cancer,\n",
    "                            n_repeats=1000,\n",
    "                            random_state=132987)\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        print(f\"{breast_cancer_df.columns[i]:>25} : \"\n",
    "              f\"{r.importances_mean[i]:.3f}\"\n",
    "              f\" +/- {r.importances_std[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods exists to help analyse black-boxxy models, such as [SHAP](https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Iris%20classification%20with%20scikit-learn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## SVM for Regression <a class=\"anchor\" id=\"svm-r\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the same algorithm for regression, with the only difference that this time instead of finding the hyperplane that is the farthest from the support, we find the hyperplane that is the closest from those support. \n",
    "\n",
    "For example, on our diabete data set, just by replacing SVC by SVR. \n",
    "We just use the `'rbf'` kernel (the linear and poly have already been tested with our OLS model earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_potato_train = dfTT.loc[i1 , :]\n",
    "y_potato_train = df.loc[i1 , \"Flesh Colour\"]\n",
    "\n",
    "X_potato_test = dfTT.loc[i2 , :]\n",
    "y_potato_test = df.loc[i2 , \"Flesh Colour\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVR\n",
    "## SVR for regression\n",
    "\n",
    "pipeline_SVR=Pipeline([('scalar',StandardScaler()),\n",
    "                       ('model',SVR(kernel='rbf'))])\n",
    "\n",
    "grid_values = {\"model__C\":np.logspace(-0, 4, 25),\n",
    "               \"model__gamma\": np.logspace(-6,-4,25)}\n",
    "\n",
    "## don't forget to change the metric to one adapted for regression:\n",
    "grid_SVR = GridSearchCV(pipeline_SVR, \n",
    "                                 param_grid = grid_values, \n",
    "                                 scoring='r2', n_jobs=-1)\n",
    "\n",
    "grid_SVR.fit(X_potato_train, y_potato_train)\n",
    "\n",
    "print('Grid best score ('+grid_SVR.scoring+'): ', grid_SVR.best_score_)\n",
    "print('Grid best parameter (max.'+grid_SVR.scoring+'): ', grid_SVR.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember, our OLS model was able to get an $R^2$ of $~0.592$.\n",
    "\n",
    "So we do not gain any $R^2$, and also we loose interpretability ... not the best trade here.\n",
    "\n",
    "Let's still have a look at the model predictions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_potato_test_score=grid_SVR.score(X_potato_test,y_potato_test)\n",
    "print('Grid best parameter (max.'+grid_SVR.scoring+') model on test: ', y_potato_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train prediction\n",
    "y_train_pred = grid_SVR.predict(X_potato_train)\n",
    "## test prediction\n",
    "y_test_pred  = grid_SVR.predict(X_potato_test)\n",
    "\n",
    "\n",
    "plt.scatter( y_train_pred , y_potato_train , c=\"blue\" , label='train set'  )\n",
    "plt.scatter( y_test_pred , y_potato_test , c=\"red\" , label='test set'  )\n",
    "m,M = min(y_train_pred) , max(y_train_pred)\n",
    "plt.plot( [m,M] , [m,M] , 'k--' )\n",
    "plt.xlabel( 'predicted values' ) \n",
    "plt.ylabel( 'real values' ) \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<!-- Exo : try a  linear kernel. Can you say anything about feature importance? How would you compare this new model to the former. %load  solutions/solution_03_mini.py -->\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Decision tree modeling : a (new?) loss function and new ways to do regularization. <a class=\"anchor\" id=\"decision-tree\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple decision tree for classification <a class=\"anchor\" id=\"simple-tree-c\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple **decision tree** reduces your problem into a **hierarchichal sequence of questions** on your features that can be answered by yes or no and which subdivides the data into 2 subgroups on which a new question is asked, and so on and so on.\n",
    "![tree_ex](image/tree_ex.png)\n",
    "\n",
    "Ok, but a huge number of trees can actually be built just by considering the different orders of questions asked. How does the algorithm deals with this?\n",
    "\n",
    "Quite simply actually: it **tests all the features and chooses the most discriminative** (with respect to your target variable) : the feature where a yes or no question divides the data into 2 subsets which minimizes an **impurity measure**.\n",
    "\n",
    "Imagine you have a dataset with feature color (red or blue) and feature shape (square or circle), and 2 target classes : 1 and 2.\n",
    "\n",
    "\n",
    "![tree](image/Tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking `\"feature color is red\"` gives you the following subgroups:\n",
    " * 10 class 1, and 1 class 2 (`\"feature color is red\" == True`)\n",
    " * 2 class 1, and 11 class 2 (`\"feature color is red\" == False`)\n",
    "\n",
    "Asking `\"feature shape is square\"` gives you:\n",
    " * 5 class 1, and 7 class 2 (`True`) \n",
    " * 7 class 1 and 5 class 2 (`False`)\n",
    " \n",
    " So, you will prefer asking `\"feature color is red?\"` over `\"feature shape is square?\"`: `\"feature color is red?\"` is more discriminative.\n",
    "\n",
    "For **categorical variables, the questions test for a specific category**.\n",
    "For **numerical variables, the questions use a threshold** to as a yes/no question.  \n",
    "\n",
    "The **threshold is, again, chosen to minimize impurity**. And in turn the best threshold for a variable is used to estimate the discriminativeness of that variable.\n",
    "\n",
    "Of course, you will have to compute this threshold at each step of your tree since at each step you are considering different subdatasets.\n",
    "\n",
    "---\n",
    "The **impurity is related to how much your feature splitting is still having mixed classes**. So the impurity ends up giving a score: either it is a simple [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) or it is a [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient).\n",
    "\n",
    "#### Shannon Entropy\n",
    "\n",
    "$$Entropy = - \\sum_{j} p_j log_2(p_j)$$\n",
    "\n",
    "This measure is linked to information theory, where the information of an event occuring is the $log_2$ of this event's probability of occuring.\n",
    "For purity, **0 is the best possible score, and 1 the worst**.\n",
    "\n",
    "#### Gini coefficient\n",
    "\n",
    "$$Gini = 1- \\sum_{j} p_j^2$$\n",
    "\n",
    "The idea is to measure the **probability that a dummy classifier mislabels your data**.\n",
    "**0 is best, 1 is worst.**\n",
    "\n",
    "---\n",
    "Before going further, just a little bit of vocabulary: \n",
    "* **Trees** are made of **nodes** (where the question is asked and where the splitting occurs). \n",
    "* A **branch** is the outcome of a splitting. \n",
    "* A **leaf** is the last node on a branch (no more splitting).\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "### Toy example to visualize decision tree. <a class=\"anchor\" id=\"toy-decision-tree\"></a>\n",
    "\n",
    "Let explore some hyperparameters of this method that, you will see in those examples, act like a regularization:\n",
    "- **Max Tree depth**: the maximum number of consecutive questions to ask\n",
    "- **Min Splitting of nodes**: minimum number of points to consider to make a new rule, outside of the leaves\n",
    "- **Min Splitting of leaves**: minimum number of points to consider to make a new rule, at the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3, y_3 = make_blobs(n_samples=120, centers=3,cluster_std=[[1,3],[1,3],[1,3]], random_state=6)\n",
    "plt.scatter(X_3[:,0],X_3[:,1],c=y_3,cmap=plt.cm.coolwarm,edgecolors='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "## creating a decision tree with 1 parameter changed (more on that later)\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X_3, y_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab( tree.predict( X_3 ) , y_3 , rownames=['truth'] , colnames=['prediction'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "fig,ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "_ = plot_tree( tree , feature_names=['x','y'] , \n",
    "               fontsize=14 , filled=True , impurity=False , precision=3, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import contour_tree\n",
    "contour_tree(X_3, y_3,\n",
    "              crit = 'entropy',\n",
    "              maxd = None,\n",
    "              min_s = 2,\n",
    "              min_l = 1,\n",
    "              max_f = None)\n",
    "#You can see that there are 5 hyperparameters here. Let's see what they do and what they mean.\n",
    "#I bet you can already guess it is going to be related to regularization....\n",
    "# After X,y you have \n",
    "# * crit = 'entropy' which is one way to calculate impurity (you could also put gini here)\n",
    "# * maxd : the max depth of your tree\n",
    "# * min_s : the number of points that should be concerned by the making of a new rule (splitting of the nodes)\n",
    "# * min_l : #of points that should be considered to make a final leaf classification\n",
    "# * max_f maximum number of features to consider for making a new rule..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an incredibly complex model. \n",
    "\n",
    "Please, note that since every node is a question asked on one particular feature and features are never directly compared, you don't need scaling! This observation that each question always involves one feature at a time can be also seen in the way the boundaries between classes are made in the graph : there is no diagonal boundaries. You can only see lines parallel to the plot axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using another impurity measurement\n",
    "contour_tree(X_3, y_3,\n",
    "              crit = 'gini',\n",
    "              maxd = None,\n",
    "              min_s = 2,\n",
    "              min_l = 1,\n",
    "              max_f = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still some overfitting but it is nice to see that the boundaries are different and that impurity calculations, even if very similar, are making a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imposing a limit for the depth of the tree : how many questions you ask (here set to 4)\n",
    "contour_tree(X_3, y_3,\n",
    "              crit = 'entropy',\n",
    "              maxd = 4,\n",
    "              min_s = 2, min_l = 1, max_f = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_tree(X_3, y_3,\n",
    "              crit = 'entropy',\n",
    "              maxd = None,\n",
    "              min_s = 2,\n",
    "              min_l = 4,\n",
    "              max_f = None)\n",
    "# min_samples_leaf : \n",
    "#     it sets the minimal number of data points that the chain of rules should concern. \n",
    "\n",
    "# eg. Do you really wish to create a whole new set of rules to explain \n",
    "# only one particular data point? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_tree(X_3, y_3,\n",
    "              crit = 'entropy',\n",
    "              maxd = None,\n",
    "              min_s = 20,\n",
    "              min_l = 1,\n",
    "              max_f = None)\n",
    "# Here it is the same as before but this time it applies to nodes instead of leaves\n",
    "# This parameter is called min_samples_split and is set to 20 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 main advantages to this kind of methods:\n",
    "* it works with all types of feature\n",
    "* you don't need to rescale\n",
    "* it already includes non linear fitting\n",
    "\n",
    "**Moreover it is 'easy' to interpret.**\n",
    "\n",
    "But....(yes there is a but, there is no free lunch)\n",
    "\n",
    "Even with all of those hyperparamaters **they are still not great on new data (inaccuracy...).** \n",
    "\n",
    "We will see that in the real data example below and we will see more powerful technics based on decision tree that are more costly but generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "### Single decision tree pipeline. <a class=\"anchor\" id=\"single-tree-pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the different hyper parameters on the decision tree are quite related to the dataset size, \n",
    "##  both in number of columns (for max depth)\n",
    "##  and in number of rows (for min sample split and min sample leaf)\n",
    "\n",
    "X_train_cancer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "grid_values = {'criterion': ['entropy','gini'],\n",
    "               'max_depth':np.arange(2,10),\n",
    "               'min_samples_split':np.arange(2,12,2)\n",
    "              }\n",
    "\n",
    "grid_tree_cancer = GridSearchCV(DecisionTreeClassifier(class_weight=\"balanced\"), \n",
    "                                param_grid = grid_values, \n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-1)\n",
    "grid_tree_cancer.fit(X_train_cancer, y_train_cancer)\n",
    "\n",
    "print(f'Grid best score (accuracy): {grid_tree_cancer.best_score_:.3f}')\n",
    "print('Grid best parameter :')\n",
    "\n",
    "for k,v in grid_tree_cancer.best_params_.items():\n",
    "    print('{:>25}\\t{}'.format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cancer_test_score=grid_tree_cancer.score(X_test_cancer,y_test_cancer)\n",
    "\n",
    "print('Grid best parameter (max. accuracy) model on test: ', y_cancer_test_score)\n",
    "\n",
    "y_cancer_pred_test = grid_tree_cancer.predict(X_test_cancer)\n",
    "\n",
    "confusion_m_cancer_tree = confusion_matrix(y_test_cancer, y_cancer_pred_test)\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(confusion_m_cancer_tree, annot=True)\n",
    "plt.title('test {} : {:.3f}'.format( grid_tree_cancer.scoring , y_cancer_test_score ))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance can be retrieved from the tree: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tree=grid_tree_cancer.best_estimator_.feature_importances_\n",
    "\n",
    "sorted_features=sorted([[breast_cancer_df.columns[i],abs(w_tree[i])] for i in range(len(w_tree))],\n",
    "                       key=itemgetter(1),reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for f,w in sorted_features:\n",
    "    print('{:>25}\\t{:.3f}'.format(f,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can even plot the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "fig,ax = plt.subplots(figsize=(25,10))\n",
    "plot_tree( grid_tree_cancer.best_estimator_ , \n",
    "          feature_names=breast_cancer_df.columns , \n",
    "          ax=ax , fontsize=14 , filled=True , impurity=False , precision=3)\n",
    "ax.set_title('best single decision tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Random Forest in classification. <a class=\"anchor\" id=\"rf-c\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Random Forest algorithm relies on two main concepts : \n",
    "1. **randomly producing/training many different trees**\n",
    "2. **agglomerating the predictions** of all these trees (mainly averaging)\n",
    "\n",
    "\n",
    "The randomness between trees concerns:\n",
    "* **[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) of the training dataset**\n",
    "* using only a **random subset of features**\n",
    "\n",
    "\n",
    "**Bootstrapping:** sampling methods in which you randomly draw a subsample from your data, *with replacement*. The created replicate is the same size as the original distribution.\n",
    "\n",
    "I am sure you can see intuitively how that is going to help generalization of our model.\n",
    "\n",
    "So now on top of all the parameters seen before to create each individual trees of the forest, you also have a parameter controlling the number of trees in your forest.\n",
    "\n",
    "![RF](image/RF.png)\n",
    "**In the following plots I am plotting the result for a random forest algorithm and compare it to a single decision tree sharing the same hyperparameters value than the one used in the random forest**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import contour_RF\n",
    "\n",
    "contour_RF(X_3,y_3,n_tree = 200, \n",
    "            crit = 'gini', maxd = 4,min_s = 5, min_l = 5, max_f = 'sqrt')\n",
    "#Same as for decision tree except that we have here one more hyperparameter, here\n",
    "# put to 100 and that represents the number of bootstraps \n",
    "# (number of trees trained and then participating to the vote)\n",
    "\n",
    "# also, we restrict the number of variables given to each tree to \n",
    "# the square root of the original number of variables ->  max_f = 'sqrt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "\n",
    "### Exercise: Random Forest on the breast cancer dataset <a class=\"anchor\" id=\"rf-exo\"></a>\n",
    "\n",
    "Train a random forest on the breast cancer dataset.\n",
    "\n",
    "Use an hyper-parameter space similar to the one we used for single decision trees with the number of trees (`n_estimator`) added to it.\n",
    "\n",
    "**computational considerations**: to limit the training time to around 1 minute, only test 5 values of `n_estimators`, all below 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_RF_cancer.py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "grid_values = {'n_estimators' : [10,50,100,150,200], \n",
    "               'criterion': ['entropy','gini'],\n",
    "               'max_depth':np.arange(2,10), ## I reduce the search space in the interest of time too\n",
    "               'min_samples_split':np.arange(2,12,2)}\n",
    "\n",
    "grid_tree = GridSearchCV(RandomForestClassifier(class_weight='balanced'), \n",
    "                                param_grid = grid_values, \n",
    "                                scoring='roc_auc',\n",
    "                                cv = 5,\n",
    "                                n_jobs=-1)\n",
    "grid_tree.fit(X_train_cancer, y_train_cancer)\n",
    "\n",
    "print(f'Grid best score (accuracy): {grid_tree.best_score_:.3f}')\n",
    "print('Grid best parameter :')\n",
    "\n",
    "for k,v in grid_tree.best_params_.items():\n",
    "    print('{:>25}\\t{}'.format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a Receiver-Operator curve from this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_test_score=grid_tree.predict_proba(X_test_cancer)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_cancer, y_test_score)\n",
    "\n",
    "keep = sum( thresholds > 0.5 ) - 1 # trick to find the index of the last threshold > 0.5\n",
    "\n",
    "y_test_roc_auc = grid_tree.score(X_test_cancer,y_test_cancer)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr, tpr, lw=3)\n",
    "plt.plot(fpr[keep], tpr[keep],'ro',label='threshold=0.5')\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title(f'ROC AUC (Decision tree) {y_test_roc_auc:.3f}', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees do not have coefficients like the logistic regression, but they still have a feature importance metric which is computed from how much each feature reduce impurity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tree=grid_tree.best_estimator_.feature_importances_\n",
    "\n",
    "sorted_features=sorted([[breast_cancer_df.columns[i],abs(w_tree[i])] for i in range(len(w_tree))],\n",
    "                       key=lambda x : x[1],\n",
    "                       reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for f,w in sorted_features:\n",
    "    if w == 0:\n",
    "        break\n",
    "    print('{:>25}\\t{:.4f}'.format(f,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = grid_tree.best_estimator_.feature_importances_\n",
    "\n",
    "## by gathering the importance accross each individual tree, we can access \n",
    "## the standard deviation of this importance\n",
    "feature_importance_std = np.std([tree.feature_importances_ for tree in grid_tree.best_estimator_.estimators_], \n",
    "                       axis=0)\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "\n",
    "plt.barh(pos, feature_importance[sorted_idx],xerr=feature_importance_std[sorted_idx][::-1], align='center')\n",
    "plt.yticks(pos, breast_cancer_df.columns[sorted_idx])\n",
    "plt.title('Feature Importance (MDI)',fontsize=10)\n",
    "plt.xlabel(\"Mean decrease in impurity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "### RF annex 1: too many features <a class=\"anchor\" id=\"rf-a1\"></a>\n",
    "\n",
    "Modern biological dataset using high-throughput technologies can now provide us with measurements for hundreds or even thousands of features (e.g., proteomics, RNAseq experiments).\n",
    "But it is often the case that among these thousands of features, only a handful are truly informative (the so-called biomarkers for example).\n",
    "\n",
    "While they generally are very good methods, Random Forest can sometime struggle in this context. \n",
    "Let's try to understand why with a synthetic example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple case : 2 categories, perfectly separable using 2 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "    \n",
    "# 120 points, 2 blobs/clusters with some spread=3\n",
    "blob_centers = np.array([[0,0],[8,4]])\n",
    "blob_stds = [[2,2],[2,2]]\n",
    "X_2, y_2 = make_blobs(n_samples = 120, \n",
    "                      centers = blob_centers,\n",
    "                      cluster_std = blob_stds, random_state = 42)\n",
    "\n",
    "plt.scatter(X_2[:,0],X_2[:,1],c=y_2,cmap=plt.cm.coolwarm,edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a single decision tree and a random forest do in this situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"decision tree cross-validated accuracy:\" , cross_val_score( dt , X_2 , y_2 ) )\n",
    "print(\"random forest cross-validated accuracy:\" , cross_val_score( rf , X_2 , y_2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that they both perform very well, perhaps even better in the case of the random forest (it is able to find more generalizable separation rules thanks to the ensembling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to add many new features filled with random data (imagine they are the 99% of genes which are not biomarkers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_noise = 10**4\n",
    "X_2_noise = np.concatenate( [ X_2 , np.random.randn( X_2.shape[0],nb_noise ) ] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"decision tree cross-validated accuracy:\" , cross_val_score( dt , X_2_noise , y_2 ) )\n",
    "print(\"random forest cross-validated accuracy:\" , cross_val_score( rf , X_2_noise , y_2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the **single decision tree is unchanged**,\n",
    "\n",
    "but the **Random Forest performs way worse**!\n",
    "\n",
    "\n",
    "**Question:** how can we explain this difference?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Solution and discussion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_RF_skb.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Random Forest in regression. <a class=\"anchor\" id=\"rf-r\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the standpoint of tree, the only difference is that now, instead of the entropy or Gini criterion, **the decision which variable to use at any node is made using a regression metric**, such as squared error for example.\n",
    "\n",
    "For example, consider this example of [regression with a single tree](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html), adapted from the sklearn website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16)) # adding additional noise to some of the points\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize = (14,6))\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "plot_tree( regr_1 , \n",
    "          ax=ax , fontsize=10 , filled=True , impurity=False , precision=3)\n",
    "ax.set_title('best single decision tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course with a single tree you do not get very far, unless the tree becomes absolutely huge. \n",
    "\n",
    "But with a random forest you can aggregate the estimate from many trees to get somewhere nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RFReg = RandomForestRegressor(n_estimators=100 )\n",
    "RFReg.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_rf = RFReg.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize = (14,6))\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_rf, color=\"yellowgreen\", label=\"RF\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a bit of leg-work, we can even grab the inidividual trees predictions to build an interval around the random forest prediction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## collecting prediction from all individual trees in a big list\n",
    "y_pred = []\n",
    "x_pred = []\n",
    "for tree in RFReg.estimators_ :\n",
    "    y_pred += list( tree.predict(X_test) )\n",
    "    x_pred += list(X_test[:,0])\n",
    "\n",
    "\n",
    "plt.figure(figsize = (14,6))\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_rf, color=\"yellowgreen\", label=\"RF\", linewidth=2)\n",
    "sns.lineplot(x=x_pred , y=y_pred , color=\"yellowgreen\" , errorbar = 'sd') \n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try on the potato data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full dataset\n",
    "X = dfTT\n",
    "y = df[ \"Flesh Colour\"]\n",
    "\n",
    "#We start by splitting our data in a train and a test set\n",
    "\n",
    "\n",
    "X_train , X_test , y_train, y_test = train_test_split(X,y , test_size=0.2)\n",
    "\n",
    "print('train set size:',len(y_train))\n",
    "print(' test set size:',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "## when it comes to criterion, we can now choose:\n",
    "# * “squared_error” (default) for the mean squared error, minimizes the L2 loss\n",
    "#                                           using the mean of each terminal node,\n",
    "# * “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits\n",
    "# * “absolute_error” for the mean absolute error, which minimizes the L1 loss\n",
    "#                                           using the median of each terminal node,\n",
    "# * “poisson” which uses reduction in Poisson deviance to find splits.\n",
    "#\n",
    "# let's try squared error and absolute error\n",
    "\n",
    "grid_values = {'criterion': ['squared_error' , 'absolute_error'],\n",
    "               'n_estimators':[500], \n",
    "               'max_depth':[10,15,20],\n",
    "               'min_samples_split':np.arange(2,12,2)}\n",
    "\n",
    "grid_RF = GridSearchCV(RandomForestRegressor(),\n",
    "                        param_grid = grid_values, \n",
    "                        scoring='r2',n_jobs=-1,cv=5)\n",
    "\n",
    "grid_RF.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f'Grid best score (r2): {grid_RF.best_score_:3f}')\n",
    "print('Grid best parameter (max. r2): ')\n",
    "\n",
    "for k , v in grid_RF.best_params_.items():\n",
    "    print(f'{k:>20} : {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Grid best parameter (max. r2) model on test: {grid_RF.score(X_test,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance = grid_RF.best_estimator_.feature_importances_\n",
    "\n",
    "sorted_features=sorted([[X_train.columns[i],abs(feature_importance[i])] for i in range(len(feature_importance))],\n",
    "                       key= lambda x : x[1],\n",
    "                       reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for f,w in sorted_features:\n",
    "    print('{:>20}\\t{:.3f}'.format(f,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based techniques are interesting because:\n",
    " * they do not necessitate scaling\n",
    " * they give interpretable models and results\n",
    " * they model arbitrary non-linear problems\n",
    " \n",
    "However as you have seen they tend to take longer to train..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the ToC](#toc)\n",
    "    \n",
    "# Conclusion <a id='conclusion'></a>\n",
    "\n",
    "During this notebook we have only given a whirlwind tour of what ML is and what is it about.\n",
    "\n",
    "We have of course only mentionned a handful of the numerous algorithms that can be used, both for [classification and for regression](https://scikit-learn.org/stable/supervised_learning.html) (NB: this link is not an exhaustive list, just what has been implemented in the sklearn library).\n",
    "\n",
    "However, more than a collection of algorithm, Machine Learning should also be seen as a set of methods to solve some important statistical problems :\n",
    " * **regularization** parameters (such as l1 or l2 norm, or max depth), to handle **overfitting**\n",
    " * **cross-validation** strategies, to detect **overfitting** and handle **model-selection**\n",
    " * **adapted metrics** to handle the specific of our goal and our data (handle imbalance for example).\n",
    "   * [classification metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "   * [regression metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Classification exercise : predicting heart disease on the framingham data-set <a class=\"anchor\" id=\"exo-classif\"></a>\n",
    "\n",
    "Use everything you have learned to model and predict the column `'TenYearCHD'` (dependent variable : ten year risk of coronary heart disease).\n",
    "\n",
    " * choose a metric appropriate to the data/question at hand\n",
    " * choose :\n",
    "     * the models which you want to test (don't forget eventual pre-processing steps)\n",
    "     * the hyper-parameters and their values which you want to test\n",
    " * use cross-validation pick the best model and hyper-parameter sets\n",
    " * evaluate your best model on the test set\n",
    " \n",
    "> NB: with the complement of models you have, it is unlikely you will get a very high performance here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heart = pd.read_csv(\"data/framingham.csv\")\n",
    "df_heart = df_heart.dropna() # dropping the rows containing some missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##separation in X and y\n",
    "X_heart = df_heart.drop( columns = \"TenYearCHD\" )\n",
    "y_heart = df_heart[ \"TenYearCHD\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting in train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r -7 solutions/solution_03_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 9-34 solutions/solution_03_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 34-47 solutions/solution_03_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 48-65 solutions/solution_03_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the best model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 67-91 solutions/solution_03_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 93-118 solutions/solution_03_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 119-145 solutions/solution_03_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionnal little diagnostic plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 146- solutions/solution_03_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Additionnal Regression exercise : predicting daily maximal temperature <a class=\"anchor\" id=\"exo-regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('data/One_hot_temp.csv')\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " * year: 2016 for all data points\n",
    " * month: number for month of the year\n",
    " * day: number for day of the year\n",
    " * week: day of the week as a character string\n",
    " * temp_2: max temperature 2 days prior\n",
    " * temp_1: max temperature 1 day prior\n",
    " * average: historical average max temperature\n",
    " * actual: max temperature measurement\n",
    " * friend: your friend’s prediction, a random number between 20 below the average and 20 above the average\n",
    " \n",
    "\n",
    "Additionally, all the features noted forecast are weather forecast given by some organisation for that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We want to predict `actual`, th actual max temperature of a day.\n",
    "\n",
    "Use a random forest to do so. You can inspire yourself from the examples of code above.\n",
    "\n",
    "Here are a couple of plots to get you started with the data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "feature_list=list(features.columns)\n",
    "labels=features[\"actual\"]\n",
    "# Dates of training values\n",
    "months = np.array(features)[:, feature_list.index('month')]\n",
    "days = np.array(features)[:, feature_list.index('day')]\n",
    "years = np.array(features)[:, feature_list.index('year')]\n",
    "\n",
    "# List and then convert to datetime object\n",
    "dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "\n",
    "# Dataframe with true values and dates\n",
    "true_data = pd.DataFrame(data = {'date': dates, 'actual': labels})\n",
    "\n",
    "\n",
    "plt.xlabel('Date'); \n",
    "plt.ylabel('Maximum Temperature (F)')\n",
    "\n",
    "# Plot the actual values\n",
    "plt.plot(true_data['date'], true_data['actual'], 'b-', label = 'actual')\n",
    "plt.xticks(rotation = 45.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "feature_list=list(features.columns)\n",
    "labels=features[\"average\"]\n",
    "# Dates of training values\n",
    "months = np.array(features)[:, feature_list.index('month')]\n",
    "days = np.array(features)[:, feature_list.index('day')]\n",
    "years = np.array(features)[:, feature_list.index('year')]\n",
    "\n",
    "# List and then convert to datetime object\n",
    "dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "\n",
    "# Dataframe with true values and dates\n",
    "true_data = pd.DataFrame(data = {'date': dates, 'average': labels})\n",
    "\n",
    "\n",
    "plt.xlabel('Date'); \n",
    "plt.ylabel('Maximum Temperature (F)')\n",
    "\n",
    "# Plot the average values\n",
    "plt.plot(true_data['date'], true_data['average'], 'b-', label = 'average')\n",
    "plt.xticks(rotation = 45. )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - Read in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-5 solutions/solution_03_02.py\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "## Read in data as pandas dataframe \n",
    "features = pd.read_csv('data/One_hot_temp.csv')\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 8-17 solutions/solution_03_02.py\n",
    "## train/test split\n",
    "y = np.array(features['actual'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "X= features.drop(['Unnamed: 0', 'year', 'month', 'day',\n",
    "       'actual', 'week_Fri', 'week_Mon', 'week_Sat', 'week_Sun', 'week_Thurs',\n",
    "       'week_Tues', 'week_Wed'], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - setup and fit pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 19-34 solutions/solution_03_02.py\n",
    "## setup and fit pipeline\n",
    "grid_values = {'criterion': ['squared_error'],\n",
    "               'n_estimators':[300,600,900], \n",
    "               'max_depth':[2,5,7],\n",
    "               'min_samples_split':[4],\n",
    "              'min_samples_leaf':[2]}# define the hyperparameters you want to test\n",
    "#with the range over which you want it to be tested.\n",
    "\n",
    "grid_tree_acc = GridSearchCV(RandomForestRegressor(), param_grid = grid_values, scoring='r2',n_jobs=-1)#Feed it to the GridSearchCV with the right\n",
    "#score over which the decision should be taken\n",
    "\n",
    "grid_tree_acc.fit(X_train, y_train)\n",
    "\n",
    "print('Grid best parameter (max. r2): ', grid_tree_acc.best_params_)#get the best parameters\n",
    "print('Grid best score (r2): ', grid_tree_acc.best_score_)#get the best score calculated from the train/validation\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 36-40 solutions/solution_03_02.py\n",
    "## evaluate the model on the test set\n",
    "# get the equivalent score on the test dataset : again this is the important metric\n",
    "y_decision_fn_scores_acc=grid_tree_acc.score(X_test,y_test)\n",
    "print('Grid best parameter (max. r2) model on test: ', y_decision_fn_scores_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - get the feature importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 41-49 solutions/solution_03_02.py\n",
    "## get the feature importances \n",
    "w=grid_tree_acc.best_estimator_.feature_importances_#get the weights\n",
    "\n",
    "sorted_features=sorted([[list(X.columns)[i],abs(w[i])] for i in range(len(w))],key=itemgetter(1),reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for f,w in sorted_features:\n",
    "    print('{:>20}\\t{:.3f}'.format(f,w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - using permutation to get the importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 50-73 solutions/solution_03_02.py\n",
    "## using permutation to get the importances\n",
    "from sklearn.inspection import permutation_importance\n",
    "feature_importance = grid_tree_acc.best_estimator_.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in grid_tree_acc.best_estimator_.estimators_], axis=0)\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx],xerr=std[sorted_idx][::-1], align='center')\n",
    "plt.yticks(pos, np.array(list(X.columns))[sorted_idx])\n",
    "plt.title('Feature Importance (MDI)',fontsize=10)\n",
    "\n",
    "result = permutation_importance(grid_tree_acc.best_estimator_, \n",
    "                                X_test, y_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(result.importances[sorted_idx].T,\n",
    "            vert=False, labels=np.array(list(X.columns))[sorted_idx])\n",
    "plt.title(\"Permutation Importance (test set)\",fontsize=10)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - BONUS - re-thinking the splitting strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RF](image/TimeSeriesSplit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_02ter.py\n",
    "## Our splitting strategy doesn't seem to represent the reality of the process....\n",
    "## inspired from https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/\n",
    "\n",
    "import scipy as sc\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "y = np.array(features['actual'])\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "X= features.drop(['Unnamed: 0', 'year', 'month', 'day',\n",
    "       'actual', 'forecast_noaa', 'forecast_acc', 'forecast_under',\n",
    "       'week_Fri', 'week_Mon', 'week_Sat', 'week_Sun', 'week_Thurs',\n",
    "       'week_Tues', 'week_Wed'], axis = 1)\n",
    "\n",
    "## the train data is the 75% most ancient data, the test is the 25% most recent\n",
    "X_train=np.array(X)[:int(len(X.index)*0.75),:]                                                                           \n",
    "X_test=np.array(X)[int(len(X.index)*0.75):,:]\n",
    "y_train=np.array(y)[:int(len(X.index)*0.75)]\n",
    "y_test=np.array(y)[int(len(X.index)*0.75):]\n",
    "\n",
    "grid_values = {'criterion': ['squared_error'],\n",
    "               'n_estimators':[300,600,900], \n",
    "               'max_depth':[2,5,7],\n",
    "               'min_samples_split':[4],\n",
    "              'min_samples_leaf':[2]}# define the hyperparameters you want to test\n",
    "\n",
    "#with the range over which you want it to be tested.\n",
    "tscv = TimeSeriesSplit()\n",
    "    \n",
    "#Feed it to the GridSearchCV with the right\n",
    "#score over which the decision should be taken    \n",
    "grid_tree_acc = GridSearchCV(RandomForestRegressor(), \n",
    "                            param_grid = grid_values, \n",
    "                            scoring='r2',\n",
    "                            cv=tscv,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_tree_acc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print('Grid best parameter (max. r2): ', grid_tree_acc.best_params_)#get the best parameters\n",
    "print('Grid best score (r2): ', grid_tree_acc.best_score_)#get the best score calculated from the train/validation\n",
    "#dataset\n",
    "\n",
    "\n",
    "\n",
    "y_decision_fn_scores_acc=grid_tree_acc.score(X_test,y_test)\n",
    "print('Grid best parameter (max. r2) model on test: ', y_decision_fn_scores_acc)# get the equivalent score on the test\n",
    "#dataset : again this is the important metric\n",
    "\n",
    "\n",
    "## feature importances\n",
    "RF = grid_tree_acc.best_estimator_\n",
    "W=RF.feature_importances_#get the weights\n",
    "\n",
    "sorted_features=sorted([[list(X.columns)[i],abs(W[i])] for i in range(len(W))],key=itemgetter(1),reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for f,w in sorted_features:\n",
    "    print('{:>20}\\t{:.3f}'.format(f,w))\n",
    "    \n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "feature_importance = RF.feature_importances_#get the weights\n",
    "std = np.std([tree.feature_importances_ for tree in grid_tree_acc.best_estimator_.estimators_], axis=0)\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx],xerr=std[sorted_idx][::-1], align='center')\n",
    "plt.yticks(pos, np.array(list(X.columns))[sorted_idx])\n",
    "plt.title('Feature Importance (MDI)',fontsize=10)\n",
    "\n",
    "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(result.importances[sorted_idx].T,\n",
    "            vert=False, labels=np.array(list(X.columns))[sorted_idx])\n",
    "plt.title(\"Permutation Importance (test set)\",fontsize=10)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## plotting the fit\n",
    "plt.plot(y,RF.predict(X),'ro')\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title(str(sc.stats.pearsonr(y,RF.predict(X))[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - BONUS - an even better splitting strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RF](image/BlockedTimeSeriesSplit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_02quat.py\n",
    "## Even better splitting strategy\n",
    "\n",
    "\n",
    "# we define our own splitter class\n",
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]\n",
    "            \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "y = np.array(features['actual'])\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "X= features.drop(['Unnamed: 0', 'year', 'month', 'day',\n",
    "       'actual', 'forecast_noaa', 'forecast_acc', 'forecast_under',\n",
    "       'week_Fri', 'week_Mon', 'week_Sat', 'week_Sun', 'week_Thurs',\n",
    "       'week_Tues', 'week_Wed'], axis = 1)\n",
    "\n",
    "\n",
    "X_train=np.array(X)[:int(len(X.index)*0.75),:]                                                                           \n",
    "X_test=np.array(X)[int(len(X.index)*0.75):,:]\n",
    "y_train=np.array(y)[:int(len(X.index)*0.75)]\n",
    "y_test=np.array(y)[int(len(X.index)*0.75):]\n",
    "grid_values = {'criterion': ['squared_error'],\n",
    "                'max_depth':[2,5,7],\n",
    "               'min_samples_split':[4],\n",
    "              'min_samples_leaf':[2]}\n",
    "#with the range over which you want it to be tested.\n",
    "tscv = BlockingTimeSeriesSplit(n_splits=5)\n",
    "\n",
    "\n",
    "    \n",
    "#Feed it to the GridSearchCV with the right\n",
    "#score over which the decision should be taken    \n",
    "grid_tree_acc = GridSearchCV(RandomForestRegressor(), \n",
    "                            param_grid = grid_values, \n",
    "                             scoring='r2',\n",
    "                             cv=tscv, \n",
    "                             n_jobs=-1)\n",
    "\n",
    "grid_tree_acc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print('Grid best parameter (max. r2): ', grid_tree_acc.best_params_)#get the best parameters\n",
    "print('Grid best score (r2): ', grid_tree_acc.best_score_)#get the best score calculated from the train/validation\n",
    "#dataset\n",
    "\n",
    "y_decision_fn_scores_acc=grid_tree_acc.score(X_test,y_test)\n",
    "print('Grid best parameter (max. r2) model on test: ', y_decision_fn_scores_acc)# get the equivalent score on the test\n",
    "#dataset : again this is the important metric\n",
    "\n",
    "## looking at feature importance \n",
    "RF = grid_tree_acc.best_estimator_\n",
    "W=RF.feature_importances_#get the weights\n",
    "\n",
    "sorted_features=sorted([[list(X.columns)[i],abs(W[i])] for i in range(len(W))],key=itemgetter(1),reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "print(sorted_features)\n",
    "\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "feature_importance = RF.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in grid_tree_acc.best_estimator_.estimators_], axis=0)\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx],xerr=std[sorted_idx][::-1], align='center')\n",
    "plt.yticks(pos, np.array(list(X.columns))[sorted_idx])\n",
    "plt.title('Feature Importance (MDI)',fontsize=10)\n",
    "\n",
    "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(result.importances[sorted_idx].T,\n",
    "            vert=False, labels=np.array(list(X.columns))[sorted_idx])\n",
    "plt.title(\"Permutation Importance (test set)\",fontsize=10)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## plotting the fit\n",
    "plt.plot(y,RF.predict(X),'ro')\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title(str(sc.stats.pearsonr(y,RF.predict(X))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Annexes <a id='annex'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sns.load_dataset(\"iris\")\n",
    "# Here we use the data loader from seaborn but such data loaders also exist with scikit-learn and are more generally delt\n",
    "#with the dataframe handler pandas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue=\"species\")\n",
    "# Seaborn allows you to 'split' your data according to a chosen parameter hue. Here I chose to color split the data according\n",
    "#to the target\n",
    "#description diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you get from the plots above?\n",
    "\n",
    "Looking at the diagonal of these plots : petal features separate the species more efficiently than sepal features.\n",
    "\n",
    "There is a very strong correlation between `petal_length` and `petal_width` : those two features are probably so similar that keeping them both could be redundant.\n",
    "\n",
    "The least correlation visible seems to be between `sepal_width` and all the others.\n",
    "\n",
    "By itself `sepal_width` is not good at differentiating species but associated with other features we can already see groups forming by species. And since they are very much non-colinear I would say that, in dimension two, `petal_length` and `sepal_width` are already a good pick for low dimensions models.\n",
    "\n",
    "You can actually quantify the correlation between features by calling the `corr()` function in pandas. You would prefer (and sometime is requiered) having a subset of features that are not correlated to each others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()\n",
    "\n",
    "sns.clustermap(df_corr,\n",
    "               figsize=(8,8),\n",
    "               z_score=None,\n",
    "               row_cluster=True,\n",
    "               col_cluster=True,\n",
    "               method='ward',\n",
    "               cmap='coolwarm',vmax=1,vmin=-1, \n",
    "               annot=True, annot_kws={\"size\": 13},cbar_kws={\"label\": 'Pearson\\ncorrelation'})\n",
    "## sns allows you to do a hierarchical clustering that simply\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification\n",
    "One thing (among others) that you can do is to look for a **subset of features that seems to be important to describe the target class**. It's like the pairplots above but instead of just looking at it you choose the features you want to keep.\n",
    "\n",
    "You can choose different metrics for 'how important to describe the class' a feature is. \n",
    "Many of those metrics utilize concepts that we haven't introduced yet, in contexts that we haven't seen yet, so I will introduce two metrics for classification that don't need too much of *a priori* knowledge. \n",
    "\n",
    "`Scikit-learn` lets you specify a threshold on the features are kept, either as:\n",
    "* a direct number: `SelectKBest`.\n",
    "* important features from a percentile of your top importance score: `SelectPercentile`.\n",
    "* an error type: `SelectFpr` or `SelectFdr` (see course 2 logistic regression part).\n",
    "\n",
    "\n",
    "`Scikit-learn` offers you different scores to calculate the importance of your features.\n",
    "\n",
    "* **ANOVA-F** : F=$\\frac{Var_{feature\\_i}(Between\\_class)}{Var_{feature\\_i}(Within\\_class)}$. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "**F** itself gives you how much a feature $i$ variance is different between classes, normalized by the intrinsic variance of that feature per class. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "So if **F** is big it means that the variation that you observe between classes is big compared to the variance of this feature : \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "it behaves differently for different classes so it it is a good feature to keep for the classification. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "To this **F** is associated a **p-value** that you would use for scoring.\n",
    "\n",
    "\n",
    "* **Chi2** ($\\chi^{2}$) test. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "You suppose the null hypothesis that this feature $i$ is homogenously distributed among classes\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "and so you are expecting that its representation in different classes should be very similar to what you can calculate for the bulk data\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    " i.e. $\\frac{\\Sigma^{n\\_points} feature_{i}}{n\\_points}$.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "You then compare the actual distribution of this feature in different classes to your null model predictions. If this **sum of square differences**: \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\Sigma^{n\\_class}_{k}\\frac{(expected\\_form\\_null\\_hypothesis_{k}-observed_{k})^{2}}{observed}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "is big then the null hypothesis has to be rejected and this feature is significant for classifying. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "The sum of these square quantities over the different classes asymptotically follows a $\\chi^{2}$ \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "distribution and thus you have access to a **p-value for scoring**.\n",
    "\n",
    "\n",
    "Another score would be to use the amount of [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) shared between a feature and our target. \n",
    "\n",
    "The way this mutual information is caclulated is out of the scope of this class as it is a bit technical.\n",
    "\n",
    "##### For regression just use correlation or Mutual Iformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "skb = SelectKBest(chi2, k=2)#creating the object SelectKBest and settling for 2 best features (k=2) in term of chi2 score\n",
    "skb.fit(df[list(df.columns)[:-1]], df[list(df.columns)[-1]])#calculating the chi2 for each features\n",
    "\n",
    "dico_pval={df.columns[i]:v for i,v in enumerate(skb.pvalues_)}\n",
    "print(\"features Chi2 scores (p-values):\")#all the features and the chi2 pvalues associated. use .pvalues_\n",
    "for feature,pval in dico_pval.items() :\n",
    "    print('\\t',feature , ':' , pval )\n",
    "\n",
    "X_new=skb.transform(df[list(df.columns)[:-1]])# keep only the k=2 best features according to the score\n",
    "\n",
    "print(\"New data with only the k=2 best features kept :\")\n",
    "print(X_new[:5,]) #printing only the 5 first entries\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3, y3 = make_blobs(n_samples=120, centers=3,cluster_std=3, random_state=6)# 120 points, 3 blobs/clusters with some spread=3\n",
    "#Random_state is here just to be sure that every time you will get the same blobs. If you change the random_state or do not\n",
    "#specify it then you will get a new plot every time you call the function (random seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course all of that can be applied to a multi-classes classification. How is it tipically done?\n",
    "\n",
    "There are many different ways of tackling the problem, that end up being a combination of these 4 elements :\n",
    "\n",
    "- **Either you treat the problem as one class vs one class**.\n",
    "\n",
    "- **Or you treat the problem as a one class vs the rest : you subdivide the problem into three different problems either your are class 1 and you consider the other classes as being one big class \"non 1\", and you do the same for the other class**.\n",
    "- **You change your loss function to a multinomial one : softmax intead of a sigmoid.** \n",
    "\n",
    "In any case you need to decide **how you are going to agglomerate those different statistics (different ROC curves for example)**:\n",
    "\n",
    "- **micro average** : pull all raw numbers together (eg. number of FP, TP), group them and then calculate your overall statistic (eg. TPR)\n",
    "- **macro average** : calculate each statistics separately and then do the average.\n",
    "\n",
    "Think about the differences induced by those metrics. Why should you use one more than the other? Or maybe you should always use all of them?\n",
    "\n",
    "Spoiler it has to do with overall separability and balance between the different class.\n",
    "\n",
    "\n",
    "What strategy your logistic regression uses so you can plot the right curves, is a tricky question. For a first pass on your data always set the multiclasses method to be ovr (one vs rest) : understanding the hyperplanes relation to decision probability and the ROC curve is more intuitive that way, and I believe less sensitive to imbalance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import contour_lr_more \n",
    "#one vs rest implementation\n",
    "contour_lr_more('l2',X3,y3,10,'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import contour_lr_more \n",
    "contour_lr_more('l2',X3,y3,10,'multinomial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_introml2024)",
   "language": "python",
   "name": "conda_introml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
