{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content <a id='toc'></a>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Convention and definition of the problem for linear model](#0)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1. Least Square Method](#1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.1 Simple representation of the problem and how it is solved](#2)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.2.Underlying hypotheses of the Least Square fit](#3)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.3. Goodness of fit](#4)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.4. Confidence interval and test statistics](#5)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.5. building and fitting OLS models with `statsmodels`](#4b)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise on a real dataset : model the effect of park frequentation on sparrow nest number with a cubic function](#6)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.6. Model choosing](#8)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.7 What to do when some hypothesis about OLS are not true](#7)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise : model choosing on the sparrow nuisance dataset](#9)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Going further : interaction effects](#11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "#matplotlib.use('nbagg') # enables interactive figures, where we can zoom and move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a couple more configuration of the plotting engine\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 4, 4\n",
    "plt.rc(\"font\", size=10)\n",
    "\n",
    "\n",
    "plt.rc('xtick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('xtick.major', size=8, pad=12)\n",
    "plt.rc('xtick.minor', size=8, pad=12)\n",
    "\n",
    "plt.rc('ytick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('ytick.major', size=8, pad=12)\n",
    "plt.rc('ytick.minor', size=8, pad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// prevents output into scrollable frames\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Convention and definition of the problem for linear model <a id='0'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we are interested in is the following:\n",
    "\n",
    "We have measured a bunch of variables per individual, for $n$ individuals. \n",
    "We are interested in the relationship between one of this variable that we will call the **response variable** \n",
    "and the other variables that we will call **covariables**.\n",
    "\n",
    "Let's illustrate this with the \"diabetes\" dataset, grabbed from Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499. (https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf).\n",
    "\n",
    "It is described as : \n",
    "\"Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "#this is our response variable\n",
    "response=pd.DataFrame(diabetes['target'],columns=['disease_progression'])\n",
    "response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is our covariable\n",
    "covar=pd.DataFrame(diabetes['data'],columns=diabetes['feature_names'])\n",
    "covar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From a mathematical standpoint, we note the response variable vector $\\pmb{y}$, and $\\pmb{y}_i$ is a single element of that vector (*ie.* the reponse variable measured on a single individual).\n",
    "\n",
    "Similarly, $\\pmb X$ is the matrix of shape $(n,p)$ which contains the values of covariables. \n",
    "$\\pmb{X}_i$ is a row in this matrix, which corresponds to the measures of the $p$ covariables for individual $i$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb X = \n",
    "\\begin{bmatrix}\n",
    "X_{1,1} & X_{1,2} & ... & X_{1,p} \\\\\n",
    "X_{2,1} & X_{2,2} & ... & X_{2,p} \\\\\n",
    " ... \\\\\n",
    "X_{n,1} & X_{i,2} & ... & X_{n,p} \\\\\n",
    "\\end{bmatrix}\n",
    " ,\\pmb y = \n",
    "\\begin{bmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    " ... \\\\\n",
    "y_{n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Furthermore, we will consider our response variable to actually be a random variable.\n",
    "So each $y_i$ is in fact a realisation of a random variable $\\pmb Y_i$.\n",
    "\n",
    "Indeed our measurements are not perfect so there is some noise associated to it ($\\epsilon$) : this why we decided to consider our response variable as a random variable. In mathematical term we are interested in a class of problem that we can write as :\n",
    "\n",
    "$$\\pmb{Y}_i=f(\\pmb{X}_i)+\\epsilon$$\n",
    "\n",
    "The function $f$ is called the regression function, and today we will be interested in looking at a particular form of those function: **linear combination**.\n",
    "\n",
    "A particular case of linear combination would be a single covariable with an intercept like :\n",
    "\n",
    "$$y_i=\\beta X_{i,1}+c$$\n",
    "\n",
    "A more general case would have more covariables and would be written like:\n",
    "\n",
    "$$f(\\textbf{X}_i,\\pmb{\\beta})=\\sum_{j} \\beta_j X_{i,j}= \\textbf{X}_{i}^{T}\\pmb{\\beta}$$\n",
    "\n",
    "Where *$\\pmb X_i$* is a vector of $p$ covariables associated to point individual $i$. And $\\pmb{\\beta}$ is a vector of $p$ coefficients (one for each covariable).\n",
    "\n",
    "Note that for now nothing is said about the nature of the $X_{i,j}$. For example, some could be constant instead of being a variable (in that specific case they may as well be integrated in the $c$ constant).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our diabetes example, the response variable is $diseaseprogression$ and the covariables are \n",
    "$age , \tsex , bmi , bp, s1 , s2 , s3 , s4 , s5 , s6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the end, \n",
    "we are interested in finding a set of coefficients $\\pmb \\beta$ so that:\n",
    "\n",
    "$$disease progression_0=\\beta_1 age_0 + \\beta_2 sex_0+\\beta_3 bmi_0+\\beta_4 bp_0+\\beta_5 s1_0 +\\beta_6 s2_0 + \\beta_7 s3_0+\\beta_8 s4_0+\\beta_9 s5_0+\\beta_10 s6_0+\\beta_0$$\n",
    "\n",
    "$$disease progression_1=\\beta_1 age_1 + \\beta_2 sex_1+\\beta_3 bmi_1+\\beta_4 bp_1+\\beta_5 s1_1 +\\beta_6 s2_1 + \\beta_7 s3_1+\\beta_8 s4_1+\\beta_9 s5_1+\\beta_10 s6_1+\\beta_0$$\n",
    "\n",
    "$$......$$\n",
    "\n",
    "$$disease progression_n=\\beta_1 age_n + \\beta_2 sex_n+\\beta_3 bmi_n+\\beta_4 bp_n+\\beta_5 s1_n+\\beta_6 s2_n + \\beta_7 s3_n+\\beta_8 s4_n+\\beta_9 s5_n+\\beta_10 s6_n+\\beta_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say for now we are only interest in $bmi$ (Body Mass Index) to predict disease progression. \n",
    "Then the kind of outcome we want of this analysis is our ability to understand the data linearly like that, which can be summarized by our ability to produce this kind of plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling all the data together in the same dataFrame\n",
    "df_diabetes = pd.concat( [covar , response] , axis = 1 ) \n",
    "\n",
    "x = df_diabetes['bmi'] # covariable bmi\n",
    "y = df_diabetes['disease_progression'] # response variable disease progression\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( figsize = (4,4)) #setup graphical windows\n",
    "sns.scatterplot(x=x,y=y) # plot x versus y\n",
    "\n",
    "#linear regression explaining disease progression thanks to bmi\n",
    "slope , intercept , r , pval , stderr = stats.linregress(x,y)\n",
    "##don't mind how we did the regression for now, this is just for showcasing\n",
    "print(\"slope also called beta in our notation= \",slope)\n",
    "print(\"intercept also called c in our notation= \",intercept)\n",
    "\n",
    "#now that we have the outcome iof the regression which is in this case a slope and\n",
    "#an intercept we can calulate what the model will predict as a disease progression given a bmi value\n",
    "yPredict = x * slope + intercept \n",
    "\n",
    "ax.plot( x , yPredict , color = 'red')#the outcome of the regression is this red line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So of course now the game becomes how to choose the best vector of parameters $\\pmb{\\beta}$. For that we will discuss two main methods (sorry Bayesian people...):\n",
    "- Least Square fit\n",
    "- Maximum Likelihood\n",
    "\n",
    "It is important to realize that these two are just **methods to find a good fit**,\n",
    "they are somewhat disconnected from the actual model we use to represent the data.\n",
    "\n",
    "\n",
    "\n",
    "Indeed, underlying those different methods there are many possible models. We will discuss:\n",
    "\n",
    "- Linear Models\n",
    "- Generalized Linear Models\n",
    "\n",
    "As their name entails, **Generalized Linear Models** (GLMs for short) are just a generalization of the simple linear models, where we can perform a transformation on our linear combination (such as a logarithm for instance), and specify which diststribution to use to generate the \"noise\" (such as a poisson distribution for instance).\n",
    "\n",
    "But, we will se that later. For now, let's focus on the simple **linear model**, which we can solve with the **least square fit** method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# 1. Least Square Method <a id='1'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the least square method we are interested in making the smallest overall square error between our model and the response variable.\n",
    "\n",
    "Typically we want to find the vector of parameters $\\beta$ that minimizes the objective function :\n",
    "\n",
    "$$S(\\pmb\\beta)=\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2=\\sum_i \\epsilon_i^2$$\n",
    "\n",
    "Here the sum is over $n$, which counts the number of individuals.\n",
    "\n",
    "in mathematical terms you are looking for :\n",
    "\n",
    "$$\\hat{\\pmb\\beta}=\\text{arg min}_{\\pmb\\beta}S(\\pmb\\beta)$$\n",
    "\n",
    "\n",
    "> The hat $\\hat{.}$, is a notation we use to denote our estimate of the true value of something. So in that sense $\\hat{\\pmb\\beta}$ is the estimate of the \"real\" coefficient values, and $\\hat{Y}$ is the estimation of $Y$ given by our model (also called the model predictions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how that work on a toy example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.1 Simple representation of the problem and how it is solved <a id='2'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy example\n",
    "\n",
    "Let's play with a toy example where we know the solution. \n",
    "\n",
    "We simulate a response variable that can be written $y=3x$, so with a single covariable. We also add some gaussian noise on top of that. \n",
    "\n",
    "We are going to check if the Ordinary Least Square (OLS) method could actully find this coefficient of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import collections  as mc\n",
    "\n",
    "## let's create some data to play with \n",
    "slopeReal = 3 # the real slope is 3\n",
    "noise = 3 #noise strength\n",
    "\n",
    "x = np.arange(15) # 15 points for x\n",
    "y = slopeReal * x + noise * np.random.randn(len(x)) # y = beta * x + some noise (no intercept here)\n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "ax.plot(x,y, 'o' )\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The challenge of least square regression is to find the slope that minimizes the squared error\n",
    "let's try 3 possible values for the slope here : 2, 3 and 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimatedSlopes = [2,3,4]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(estimatedSlopes) , figsize = (14,7))\n",
    "\n",
    "for i,slopeEstimate in enumerate(estimatedSlopes): # for each slope we want to test\n",
    "    \n",
    "    yPredicted = slopeEstimate * x # prediction of y given the estimated slope and values of x\n",
    "\n",
    "    # error of the prediction -> sum of squared difference between observation and prediction\n",
    "    predictionSquaredError = sum( ( yPredicted - y )**2 ) \n",
    "\n",
    "    #base plot\n",
    "    ax[i].plot(x,y, 'o')\n",
    "    ax[i].set_xlabel(\"X\")\n",
    "    ax[i].set_ylabel(\"Y\")\n",
    "    \n",
    "    ax[i].plot(x,yPredicted, 'r--' , linewidth=2,label='Estimated') #plotting the prediction\n",
    "\n",
    "    # now, let's represent the fitting error as segments between real and estimated values\n",
    "    Real = [i for i in zip(x,y)]\n",
    "    Predicted = [i for i in zip(x,yPredicted)]\n",
    "    lc = mc.LineCollection(zip(Real,Predicted) , colors='black',linewidths=2,label='Noise + Fitting Error')\n",
    "    ax[i].add_collection(lc)\n",
    "    \n",
    "    ax[i].set_title('slope : {} \\n squared error : {:.2f}'.format(slopeEstimate,predictionSquaredError) ,\n",
    "                    fontsize=20)\n",
    "    ax[i].legend(fontsize=10)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**micro-exercise** : based on the 3 plot just above. Which one would you choose and why?\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We could try to generalize this approach and test a lot of possible slope and then choose the best one (the one giving the lowest SSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possibleSlopes = np.linspace(0,6,1001)# trying a hundred slopes between 0 and 6\n",
    "errors = []\n",
    "for sl in possibleSlopes: # we compute the sum of squared error for each slopes\n",
    "    yPred = sl*x\n",
    "    errors.append( sum( yPred - y )**2 )\n",
    "\n",
    "fitted_slope=possibleSlopes[ np.argmin( errors ) ] # we use np.argmin to find the best slope\n",
    "min_err=min(errors)\n",
    "\n",
    "#plotting \n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "ax.plot(possibleSlopes , errors )\n",
    "ax.plot(fitted_slope , min_err,'ro' ,label='Best estimated slope')\n",
    "ax.legend(loc='best',fontsize=10)\n",
    "\n",
    "plt.xlabel('estimated slope')\n",
    "plt.ylabel('sum of squared errors')\n",
    "print( 'slope estimate with the smallest squared error (noise + fitting error) : ', fitted_slope )\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-exercice : what are the defaults of this method to find the best $\\beta$ ?**\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "While we could use various optimization algorithms to find the best value for $\\beta$, \n",
    "when the system is overdetermined (*i.e.*, you have more points than coefficients $\\beta_i$) an analytical solution exists. It is of the form:\n",
    "\n",
    "$$\\hat{\\pmb\\beta}=(\\pmb X^T \\pmb X)^{-1}\\pmb X^T \\pmb Y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.2.Underlying hypotheses of the Least Square fit <a id='3'></a>\n",
    "\n",
    "There are a couple of important hypothesis behind this method:\n",
    "\n",
    "- **Correct specification** : have a good incentive for the function you use\n",
    "- **Strict exogeneity** : the errors are centered around 0\n",
    "- **No linear dependance** : you can not reconstruct one of your covariable by summing a subset of your covariables with some set of constant weights \n",
    "- **Spherical errors**: \n",
    "    - Homoscedasticity : the spread of the error is the same along the curve.\n",
    "    - No autocorrelation : error are not correlated along the curve.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "To illustrate **Strict exogeneity** and **Spherical errors**, here is a plot of the errors on the diabetes model with $bmi$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_diabetes['bmi'] # covariable bmi\n",
    "y = df_diabetes['disease_progression'] # response variable disease progression\n",
    "slope , intercept , r , pval , stderr = stats.linregress(x,y)\n",
    "fitted_y = (x*slope+intercept)\n",
    "errors = y - fitted_y\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize = (14,7)) #setup graphical windows\n",
    "sns.scatterplot(x=fitted_y,y=errors , ax=ax[0]) # plot the errors along x\n",
    "ax[0].axhline(0,color='red')\n",
    "ax[0].set_ylabel('errors')\n",
    "ax[0].set_xlabel('fitted values')\n",
    "sns.histplot(errors , ax=ax[1]) # plot an histogram of the errors\n",
    "ax[1].vlines( np.median( errors ) ,0,70 , color=\"red\", linewidth = 2)\n",
    "ax[1].set_xlabel('errors')\n",
    "\n",
    "print( \"error\" )\n",
    "print( \"mean    {:.3f}\".format(errors.mean()) )\n",
    "print( \"median  {:.3f}\".format(np.median( errors )) )\n",
    "print( \"Q1 : {:.3f} , Q3 : {:.3f}\".format(*np.quantile( errors , [0.25,0.75] ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors seems centered around 0 and there is no definite pattern that would allow us to reject outright the hypotheses of the Least Square fitting. We shall see later on how to actually test these using specific statistical tests.\n",
    "\n",
    "The right panel also seems to suggest that our errors could follow something that looks like a normal distribution.\n",
    "\n",
    "**Normality is not strictly needed for Least Square fitting, neither for the variables nor for their errors.** \n",
    "However you may need that hypothesis of normality of errors downstream in your analysis, in particular when using a test statistic to determinate the significance of your parameter\n",
    "\n",
    "\n",
    "Additionnaly, if your errors are normally distributed, then Least Square fitting and Maximum Likelihood are equivalent, showing that your method for choosing $\\pmb\\beta$ is efficient and sound.\n",
    "\n",
    "<br>\n",
    "\n",
    "A model where things go wrong, for instance where there is heteroskedascticity, could look something like that instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random(1000)\n",
    "y = 3 * x + x * np.random.randn(len(x))  # the noise strength now depends on x.\n",
    "slope , intercept , r , pval , stderr = stats.linregress(x,y)\n",
    "fitted_y = (x*slope+intercept)\n",
    "errors = y - fitted_y \n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "sns.scatterplot(x=fitted_y,y=errors , ax=ax) # plot the errors along x\n",
    "ax.axhline(0,color='red')\n",
    "ax.set_ylabel('errors')\n",
    "ax.set_xlabel('fitted values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>   \n",
    "\n",
    "---  \n",
    "\n",
    "<br>\n",
    "\n",
    "Finally, within that set of constraints and even if the method is called Linear Models, it is possible to **fit polynomials**. To do so you just have to precompute the monomials and add them to your set of covariables.\n",
    "\n",
    "For example :\n",
    "\n",
    "$y=\\beta x +c$ is a linear combination of x\n",
    "\n",
    "$y=\\beta_{1}x+\\beta_{2}x^{2}+\\beta_{3}x^{3}$ is still a linear combination of covariables x, $x^{2}$ and $x^{3}$, and **X** becomes {$x,x^2,x^3$\\}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.3. Goodness of fit <a id='4'></a>\n",
    "\n",
    "By definiton, the least square fit minimizes the Sum of Square Errors : $SSE=\\sum (y_i-\\hat{y_i})^2)$.\n",
    "\n",
    "It is still important to assess the goodness of fit of your model, for instance to get an idea if you capture most of the variance of your response variable or to compare models between one-another.\n",
    "\n",
    "The first metric use to do so is the **Mean Square Error** (MSE), which is defined as follow:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$MSE=\\frac{SSE}{n-2}$$ \n",
    "\n",
    "Why $n-2$? here are [elements of answer](https://stats.stackexchange.com/questions/378407/why-error-sum-of-squares-has-n-2-df-possibly-not-duplicate-please-read-on-r). Anyway this accounts for what your model is missing. \n",
    "That could be the simple inherent variance induced by the noise term or the noise term and a missing term that your model doesn't take into account. By its nature, this metric makes it hard to compare between different hypothetical fitting models or different dataset.\n",
    "\n",
    "A better normalized metric is the **adjusted coefficient of determination $\\pmb R^2_a$**. \n",
    "The adjusted part is very necessary when we work in the context of multiple linear regression (more than one covariable). \n",
    "\n",
    "Let's start by defining the coefficient of determination $\\pmb R^2$. \n",
    "This coefficient partitions the variance present in your data between what is taken into account by your model and what is not.\n",
    "\n",
    "$$R^2=1-\\frac{SSE}{SST}$$ <br>, where $SSE=\\sum_i (y_i-\\hat{y_i})^2$) and SST in the Sum of Squares Total ($\\sum_i (y_i-\\bar{y})^2$)\n",
    "\n",
    "For the adjusted coefficient of determination you have to take into account that SSE and SST don't have the same degree of freedom and you should adjust for that.\n",
    "\n",
    "$$R^2_a=1-\\frac{n-1}{n-p}(1-R^2)$$, with $p$ the number of covariables and $n$ the number of individuals.\n",
    "\n",
    "> Note : you can see that when there is only one covariable then $R^2_a = R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = sum( errors**2 )\n",
    "MSE = SSE/(len(y)-2)\n",
    "SST = sum( ( y - np.mean(y) )**2 )               # sum of square total\n",
    "R2 =  1 - SSE/SST                                # coefficient of determination\n",
    "R2a = 1 - ( (len(y)-1)/(len(y) - 2) ) * (1 - R2) # here, we have 2 parameters : intercept and slope for bmi\n",
    "print( \"\"\"bmi and disease progression model \n",
    " - SSE: {:.3f}\n",
    " - MSE: {:.3f}\n",
    " - R2:  {:.5f}\n",
    " - R2a: {:.5f}\"\"\".format(SSE,MSE,R2,R2a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.4. Confidence interval and test statistics <a id='5'></a>\n",
    "\n",
    "After your fitting, you would probably like to know the confidence interval for each of your estimated $\\beta$, as well as if they are truly necessary (significantly different from zero). \n",
    "For both **you can't truly do anything without making an hypothesis about the statistic of the noise** : here comes the part where assuming your noise to be normally distributed ($N(0,\\sigma^2)$) becomes important, but potentially wrong too.\n",
    "\n",
    "For the confidence interval, if you have an infinite amount of data it is demonstrated that the estimators are well described by a normal statistic. \n",
    "There is a convergence in the distribution so that $(\\hat{\\pmb\\beta}-\\pmb\\beta)\\rightarrow N(0,\\sigma^2 (\\pmb X^T \\pmb X)^{-1})$). \n",
    "\n",
    "So for a big amount of points relative to the number of estimated parameters, you are not making a big mistake by writing the confidence interval of $\\beta_i$ as:\n",
    "\n",
    "$$\\beta_j \\in [\\hat{\\beta_j} \\pm z_{1-\\frac{\\alpha}{2}}\\sqrt{\\hat{\\sigma}^2 [(\\pmb X^T \\pmb X)^{-1}]_{j,j}}]$$ \n",
    "\n",
    "So in this formula, \n",
    " * $j$ is indexing one of the covariable, \n",
    " * $z_{1-\\frac{\\alpha}{2}}$ would be 1.96 for a 95% confidence interval, as you usually do with a normal distribution\n",
    "\n",
    "If you don't have a huge amount of data you need to show that you have an incentive about your noise statistic to use these kind of confidence intervals.\n",
    "\n",
    "\n",
    "For the significance of the coefficients, **if you know that your noise is normally distributed then you can use a t-test**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's examine all these metrics for our simple model on the diabetes dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll be using the statsmodel package, which computes a lot of these metrics for you\n",
    "\n",
    "X = sm.add_constant(df_diabetes['bmi'])## adding the intercept to the model\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS( df_diabetes[ 'disease_progression' ], X)  ## defining an Ordinary Least Square variable\n",
    "results = model.fit() ## fitting it\n",
    "res=results.summary()\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go at these panels one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first panel gives you an overview of the fit quality:\n",
    "* You recognize the good old **$R^2$** and **$R_a^2$**\n",
    "* The **F-statistic** and its associated P-value (**Prob (F-statistic)**) tests the hypothesis that all the coefficients are 0 (under a normality assumption)\n",
    "* **Log-likelihood** (normality assumption + this is the next big part so keep it on a corner of your mind)\n",
    "* AIC and BIC, respectively **Aikike Information Criterion** and **Bayesian Information Criterion** are used (like the log-likelihood) for model comparison. You can use them to compare non nested models, although it is not always clear what constitutes a \"meaningful\" difference in AIC or BIC between two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second panel presents the coefficient values and associated statistics.\n",
    " * **const** is the intercept. It represents the value of the response variable when all covariable are at 0.\n",
    " * **bmi** is the coefficient of the $bmi$. It represents the increase of the response variable when $bmi$ increases by 1.\n",
    "\n",
    "Regarding the columns:\n",
    " * **std err** : standard errors of the estimated coefficients.\n",
    " * **t** : t-test statistics.\n",
    " * **P>|t|** : p-values of the t-test testing if the coefficient is significantly different from 0.\n",
    " * **\\[0.025 0975\\]** : 95% condidence interval aournd the coefficient\n",
    "\n",
    "Be careful with this **t-test which again makes the assumption that errors are normally distributed**, same for the standard error and the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.tables[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third panel is a summary of a few statistical tests that will give you a sense of how all of the hypothesis needed for OLS are plausible:\n",
    "* **Omnibus and Prob(omnibus)**: this is a test for normality of residuals. Low P-values means that your linear model is not adapted\n",
    "* **[Skew](https://en.wikipedia.org/wiki/Skewness)** : should be at 0\n",
    "* **[Kurtosis](https://en.wikipedia.org/wiki/Kurtosis)** : should be at 3\n",
    "* **Durbin-Watson** : tests autocorrelation in the error terms (2 is no autocorrelation, less than 1 is bad)\n",
    "* **Jarque-Bera** : tests if the [skewness](https://en.wikipedia.org/wiki/Skewness) and [kurtosis](https://en.wikipedia.org/wiki/Kurtosis) of your errors are looking like a normal distribution. If the Pvalue is high then they look normal.\n",
    "* **Condition Number** : tries to evaluate dependence between the co-variables. Close to infinity ($\\gt 10^{15}$) mean there is a linear relationship between some co-variables (you can read more on this [here](https://medium.com/@docintangible/multicollinearity-in-pythons-statsmodels-ols-vs-r-s-lm-6fc9a994154c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one important hypothesis of OLS which is missing from this summary : **homoskedasticity**.\n",
    "\n",
    "Thankfully, statsmodels provide several tests for this as well, we will use White's test, whose null hypothesis is homoscedasticity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "LMstat , LMpval , Fstat , Fpval = het_white( results.resid , X )\n",
    "print(\"White test for heteroscedasticity p-value:\" , LMpval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Providing a visual impression of the goodness of fit is also important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "O = np.argsort( results.fittedvalues )\n",
    "plt.plot( results.fittedvalues[O] ,\n",
    "gaussian_filter1d( results.resid[O] , sigma = 10 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "fig, ax = plt.subplots(ncols=3, figsize = (14,5)) #setup graphical windows\n",
    "# plot the residuals along fitted values\n",
    "sns.scatterplot(x=results.fittedvalues,\n",
    "                y=results.resid , \n",
    "                ax=ax[0]) \n",
    "# adding a smoothed average line\n",
    "O = np.argsort( results.fittedvalues ) ## sorting by X values\n",
    "ax[0].plot( results.fittedvalues[O] ,\n",
    "            gaussian_filter1d( results.resid[O] , sigma = 10 ) , color='orange',linestyle='dashed' )\n",
    "\n",
    "ax[0].axhline(0,color='red')\n",
    "ax[0].set_ylabel('errors')\n",
    "ax[0].set_xlabel('fitted values')\n",
    "\n",
    "# histogram of residuals\n",
    "sns.histplot(results.resid , ax=ax[1]) # plot an histogram of the errors\n",
    "ax[1].vlines( np.median( results.resid ) ,0,70 , color=\"red\", linewidth = 2)\n",
    "ax[1].set_xlabel('errors')\n",
    "\n",
    "\n",
    "sns.scatterplot(x=results.fittedvalues,\n",
    "                y=df_diabetes[\"disease_progression\"] , \n",
    "                ax=ax[2]) \n",
    "m,M = min(df_diabetes[\"disease_progression\"]) , max(df_diabetes[\"disease_progression\"])\n",
    "ax[2].plot([m,M],[m,M], color='red')\n",
    "ax[2].set_ylabel('real values')\n",
    "ax[2].set_xlabel('fitted values')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-exercise : what is your conclusion on the $bmi$ model based on these results ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.5. building and fitting OLS models with `statsmodels`  <a id='4b'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There  are two main approach to building a model with `statsmodels`.\n",
    "\n",
    "The first is the one we juste demonstrated: giving an iterable representing the target variable (`y`), and a 2D numpy array representing all covariables, including the intercept (`X`).\n",
    "\n",
    "The second is to use a *formula*. If you have been doing some regression in R, you would have come across these : instead of different dataframes for dependent variable covariables, on gives a single `DataFrame` along with a formula of the type `y ~ x`.\n",
    "\n",
    "As long as you have regrouped all your variables of interest in the same `DataFrame`, this second solution offers a fairly simple and elegant way of specifying your model, with an efficient handling of intercepts, as well as interaction effects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## option 1 : using different matrices for target / covariables\n",
    "\n",
    "y = df_diabetes['disease_progression'] # response variable disease progression\n",
    "X = sm.add_constant( df_diabetes[['bmi']] )## adding the intercept to the model\n",
    "\n",
    "model1 = sm.OLS(y, X)\n",
    "\n",
    "## option 2 : using a formula\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model2 = smf.ols(\"disease_progression ~ bmi\" , data = df_diabetes) \n",
    "## NB : the intercept is implicit here. To have a model without intercept: disease_progression ~ 0 + bmi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whichever method you end up using, you need to fit you model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = model1.fit()\n",
    "results2 = model2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## both method give the same result:\n",
    "print(\"method 1\")\n",
    "print(results1.params)\n",
    "print(\"method 2\")\n",
    "print(results2.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Going into details on these formula is beyond the scope of this course, but we encourage you to go though this [very well made tutorial](https://www.statsmodels.org/devel/example_formulas.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now proceed to an OLS fitting on a toy model produced thanks to the relation** $\\pmb Y=-3x+6x^3+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###making toy data\n",
    "nsample = 200\n",
    "x = np.linspace(0, 10, nsample)#this is our covariable\n",
    "X = np.column_stack((x, x**3)) #here with have only one real covariable, \n",
    "#but we will increase that number by also considering it's cube in our model\n",
    "\n",
    "\n",
    "#the true value of our parameters associated to our covariables \n",
    "# bonus : try 1,-100,2\n",
    "beta = np.array([1, -3, 6])\n",
    "\n",
    "#for now noise is low, but you can play with it by increasing the scale\n",
    "e = nsample*np.random.normal(size=nsample , scale=10)\n",
    "\n",
    "\n",
    "X = sm.add_constant(X)  ##adding the intercept\n",
    "y = np.dot(X, beta) + e ## making y=1-3x+6x^3 + noise\n",
    "y_true=np.dot(X, beta)\n",
    "\n",
    "plt.subplots(figsize = (9,6)) #setup graphical windows\n",
    "plt.plot(x,y,'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)  ##defining an Ordinary Least Square variable\n",
    "results = model.fit() ##fitting it\n",
    "\n",
    "res=results.summary()\n",
    "print(res)\n",
    "LMstat , LMpval , Fstat , Fpval = het_white( results.resid , X )\n",
    "print(\"\\n\\tWhite test for heteroscedasticity p-value:\" , LMpval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the result :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Plotting the fit\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "# we obtain the predicted values for our model, as well as their 95% intervals\n",
    "prstd, iv_l, iv_u = wls_prediction_std(results) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "ax.plot(x, y, 'o', label=\"data\")\n",
    "ax.plot(x, y_true, 'b-', label=\"with true coefficients\")\n",
    "ax.plot(x, results.fittedvalues, 'r', label=\"with OLS result\")\n",
    "ax.plot(x, iv_u, 'r--')\n",
    "ax.plot(x, iv_l, 'r--')\n",
    "ax.legend(loc='best',fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-exercise : given these results (plot + result summary), what is your conclusion on this model ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise on a real dataset : model the effect of park frequentation on sparrow nest number with a cubic function <a id='6'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using data from [TESTING THE RISK-DISTURBANCE HYPOTHESIS IN A FRAGMENTED LANDSCAPE: NONLINEAR RESPONSES OF HOUSE SPARROWS TO HUMANS\n",
    "FernÃ¡ndez-Juricic(2003),BioOne](https://bioone.org/journals/the-condor/volume-105/issue-2/0010-5422(2003)105%5b0316%3aTTRHIA%5d2.0.CO%3b2/TESTING-THE-RISK-DISTURBANCE-HYPOTHESIS-IN-A-FRAGMENTED-LANDSCAPE/10.1650/0010-5422(2003)105[0316:TTRHIA]2.0.CO;2.short)\n",
    "\n",
    "In this data set we would like to model the relation between the density of house sparrow nests in a park and how popular a park is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/Human_nuisance.csv\", index_col=0)\n",
    "df = df.rename( columns = {\"Breeding density(individuals per ha)\" : \"Breeding\" , \n",
    "            \"Number of pedestrians per ha per min\" : \"Number\" } )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot( df['Number'], df['Breeding'],'ro')\n",
    "ax.set_ylabel(\"Breeding density(individuals per ha)\")\n",
    "ax.set_xlabel(\"Number of pedestrians per ha per min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an OLS regression analysis on this data, to explain the swallow breeding density (response variable) with the number of pedestrian (covariable).\n",
    "\n",
    "> The relationship between the two variable implies at most a cubic (\\*\\*3) relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_01_01.py\n",
    "df=pd.read_csv(\"data/Human_nuisance.csv\", index_col=0)\n",
    "df = df.rename( columns = {\"Breeding density(individuals per ha)\" : \"Breeding\" , \n",
    "            \"Number of pedestrians per ha per min\" : \"Number\" } )\n",
    "\n",
    "## setup a variable to the second and third degrees\n",
    "df[\"Number2\"] = df.Number ** 2\n",
    "df[\"Number3\"] = df.Number ** 3\n",
    "df[\"Number4\"] = df.Number ** 4\n",
    "df[\"Number5\"] = df.Number ** 5\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup of the model - formula style\n",
    "model = smf.ols( 'Breeding ~ Number  + Number2 + Number3' , data=df)# we create the least square fit object\n",
    "results = model.fit()#we do the actual fit\n",
    "print(results.summary())\n",
    "LMstat , LMpval , Fstat , Fpval = het_white( results.resid , model.exog )\n",
    "print(\"\\n\\tWhite test for heteroscedasticity p-value:\" , LMpval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( results.fittedvalues , results.resid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## plotting results.\n",
    "\n",
    "# creating input for many different number of pedestrians in order to plot the curve\n",
    "xx=pd.DataFrame( {\"Number\" : np.linspace(min( df.Number ),max( df.Number),200) })\n",
    "xx[\"Number2\"] = xx[\"Number\"]**2\n",
    "xx[\"Number3\"] = xx[\"Number\"]**3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "ax.plot(df.Number, df.Breeding, 'o', label=\"observed points\")\n",
    "ax.plot(df.Number, results.fittedvalues , 'or', label=\"fitted points\")\n",
    "ax.plot(xx.Number, results.predict(xx), 'r')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.6. Model choosing <a id='8'></a>\n",
    "\n",
    "Most of the time you are not sure of the model you want to fit. You might have a broad idea of the different forms of the function but you don't really know for example what would be the best degree for your poynomial or if all the covariables are actually necessary. Of course you could say \"I am keeping the model that fit the best in term of $R^2$\". But the question really is : is that bunch of extra parameters that are complexifying my model worth the increase in $R^2$?\n",
    "\n",
    "We touched that question in 1.4 by asking about the significance of parameters values. Again if you are confident on the noise distribution you are dealing with (let's say it is normally distributed), and you have a function in mind but you don't know if you should include 1,2 or $p$ covariables then the problem is easy: you can use a log-likelihood ratio test.\n",
    "\n",
    "\n",
    "### Likelihood ratio test (LRT)\n",
    "\n",
    "We already mentioned the likelihood, or log-likelihood, several time. \n",
    "\n",
    "We will not spend too long here on its precise definition, in part because we will discuss it in some details in the next chapter. Suffice to know for now the **likelihood is a metric of goodness of fit of your model to your data** which possess interesting mathematical properties when it comes to comparing different models. \n",
    "\n",
    "Furthermore, it is computed for you by `statsmodels.OLS` and is part of the fit summary.\n",
    "\n",
    "\n",
    "To perform a Likelihood Ratio Test you just have to calculate the difference between the maximised log-likelihood of the two models you are comparing. \n",
    "You can estimate the significance of that difference either by using a test statistic (approximate method, which we will show) or by using simulations.\n",
    "\n",
    "LRT are to be used in the case of **nested function comparisons**. Nested functions are functions that have the same form but differ in the number of parameters used : for example comparing $y=\\beta_1 x_1 +c$ and $y=\\beta_1 x_1 +\\beta_2 x_2 +c$. In this course this will always be the case (but just remember that outside of this course you might want to do other comparison, so be careful).\n",
    "\n",
    "**Briefly :**\n",
    "\n",
    "You want to compare model $M_0$ and $M_1$, respectively having $\\{\\beta_{1,0}\\}$ and $\\{\\beta_{1,2},\\beta_{2,2}\\}$ as parameters. You want to see if adding this extra parameter $\\beta_{2,2}$ is worth it.\n",
    "\n",
    "The LRT statistics is :\n",
    "\n",
    "$$2*(l(X;\\hat{\\beta}_{1,2},\\hat{\\beta}_{2,2})-l(X;\\hat{\\beta}_{1,0}))$$\n",
    "\n",
    "Where the hat represents the maximum likelihood estimates. \n",
    "\n",
    "The LRT statistic asymptoptically, for your sample size going to infinity, follows a **chi-square distribution with a number of degree of freedom equal to the difference between the number of parameters in your models**. You have thus access to a P-value which will help you to decide if complexifying your model is worth it. \n",
    "\n",
    "Typically this LRT statistic could be viewed as a difference in terms of goodness of fit. If this difference is big compared to the goodness of fit of the model with fewer parameters then it is interesting to keep the complexified model. \n",
    "\n",
    "Indeed if this LRT statistic is big then the P-value will be low, and you will tend to reject the null hypothesis that the two models have the same (or non-significatively different) goodness of fit to your data.\n",
    "\n",
    "> Note how in the LRT statistic I have put the complex model first then the simple model.\n",
    "\n",
    "\n",
    "To calulate this P-value you can use $1-scipy.stats.chi2.cdf(LRT,df_{M_1}-df_{M_0})$, where $df$ is the number of degree of freedom of the models, equivalent to the number of parameters in our case.\n",
    "\n",
    "<br>\n",
    "\n",
    "From these conepts, many strategies can be devised to find the smallest set of features  which adequately describes your response variable.\n",
    "\n",
    "\n",
    "\n",
    "For example, we can use **Forward Sequential Feature Selection** : a greedy strategy which starts from an empty model and adds features 1 by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple forward Sequential Feature Selection implementation:\n",
    "def score_variables( data, response , current_covar , candidates ):\n",
    "    '''Takes:\n",
    "            - data: a pandas DataFrame containing the data\n",
    "            - response: column name of the response variable\n",
    "            - current_covar: list of the column names of the currently accepted co-variables (ie, part of the base model)\n",
    "            - candidates: list of the column names of the covariables to test\n",
    "    \n",
    "        Returns:\n",
    "            (dict): a dictionnary associating each candidate variable to the log-likelihood of\n",
    "                    a linear model with the corrent_covar and the candidate variable\n",
    "    '''\n",
    "\n",
    "    loglikelihood_dict = {}\n",
    "    \n",
    "    formula_base =response + ' ~ ' + ' + '.join(current_covar)\n",
    "    for c in candidates:\n",
    "        \n",
    "        model = smf.ols( formula_base + ' + ' + c , data = data )\n",
    "        results = model.fit() # fitting\n",
    "    \n",
    "        loglikelihood_dict[c] = results.llf\n",
    "    return loglikelihood_dict\n",
    "\n",
    "\n",
    "def LR_test( loglkh_ref , loglkh_candidate , ddf=1 ):\n",
    "    LRT = 2*( loglkh_candidate - loglkh_ref  )\n",
    "    return 1-stats.chi2.cdf( LRT , ddf )\n",
    "\n",
    "\n",
    "## setup the base model without any co-variable\n",
    "base_model = smf.ols( 'disease_progression ~ 1' , data = df_diabetes )\n",
    "base_model = base_model.fit()\n",
    "loglkh_ref = base_model.llf\n",
    "\n",
    "\n",
    "threshold = 10**-2 ## p-value threshold for the LRT\n",
    "accepted_covariables = []\n",
    "candidate_covariables = [c for c in df_diabetes.columns if c != 'disease_progression' ]\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    ## score each covariable with its log-likelihood\n",
    "    loglkh_dict = score_variables(df_diabetes , 'disease_progression' , accepted_covariables , candidate_covariables)\n",
    "\n",
    "    ## get the best likelihood\n",
    "    best_candidate = max( loglkh_dict , key= loglkh_dict.get )\n",
    "\n",
    "    ## compute LRT pv-alue\n",
    "    pval = LR_test( loglkh_ref , loglkh_dict[best_candidate] , ddf=1 )\n",
    "    \n",
    "    ## compare p-value to threshold\n",
    "    if pval > threshold :\n",
    "        print(\"did not accept co-variable {:>10}\\t\\tlog-likelihood: {:.2f}\\tLRT p-value: {:.1e}\".format(best_candidate,\n",
    "                                                                                            loglkh_dict[best_candidate],\n",
    "                                                                                            pval))\n",
    "        break\n",
    "    else:\n",
    "        ## accepted a new co-variable\n",
    "        accepted_covariables.append( best_candidate )\n",
    "        loglkh_ref = loglkh_dict[best_candidate]\n",
    "        candidate_covariables.remove(best_candidate)\n",
    "        print(\"accepted co-variable {:>10}\\t\\tlog-likelihood: {:.2f}\\tLRT p-value: {:.1e}\".format(best_candidate,\n",
    "                                                                                            loglkh_dict[best_candidate],\n",
    "                                                                                            pval))\n",
    "print('\\naccepted covariables:', accepted_covariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'disease_progression ~ ' + ' + '.join(accepted_covariables)\n",
    "model = smf.ols( formula , data = df_diabetes )\n",
    "res = model.fit()\n",
    "print('final model:')\n",
    "print( res.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.7. What to do when some hypothesis about OLS are not true <a id='7'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the homoscedasticity of your data is not true you have a few possibilities:\n",
    "- you can transform your data so your data become homoscedastic (for example you could use a variance stabilizing transformation, or a simple log transform or other...)\n",
    "- you can change $S(\\beta)$, your loss function, to reweight the different members of that equation by taking into account the discrepancy in terms of variance. That only works if there is no correlation between the error terms. In that case the method is called [**Weighted Least Square**](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.WLS.html#statsmodels.regression.linear_model.WLS) and the loss function becomes \n",
    "$$S(\\pmb\\beta)=\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 \\rightarrow S(\\pmb\\beta)=\\sum_i \\frac{1}{\\sigma_i^2} (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2$$\n",
    "- if there is a correlation between the different error terms then it becomes more complicated, but technics exist such as [Generalized Least Square model](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.GLS.html#statsmodels.regression.linear_model.GLS)\n",
    "\n",
    "Finally **if you know  what statistics your measurement follow**, you can bypass all of those problems (and encounter others :-)) by using a **Maximum Likelihood approach** rather than an Least-Square method. \n",
    "\n",
    "By doing so you will have to put yourself in the framework of Generalized Linear Models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise : model choosing on the sparrow nuisance dataset <a id='9'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come back to the sparrow dataset (`df_nuisance`).\n",
    "\n",
    "Find the best model possible (i.e., change the degree of the polynomial we try to fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r -13 solutions/solution_01_02.py\n",
    "df=pd.read_csv(\"data/Human_nuisance.csv\", index_col=0)\n",
    "df = df.rename( columns = {\"Breeding density(individuals per ha)\" : \"Breeding\" , \n",
    "            \"Number of pedestrians per ha per min\" : \"Number\" } )\n",
    "\n",
    "\n",
    "## let's build a reference model with only the number \n",
    "\n",
    "model = smf.ols( 'Breeding ~ Number' , data=df)\n",
    "results = model.fit()\n",
    "\n",
    "ref_model = 'Breeding ~ Number'\n",
    "ref_results = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 14- solutions/solution_01_02.py\n",
    "# incrementally adding columns of increasing power\n",
    "maxPow = 10\n",
    "threshold = 1.0\n",
    "coVariables = ['Number']\n",
    "for current_power in range( 2,maxPow ) :\n",
    "\n",
    "    # create the column we need\n",
    "    df[ 'Number'+str(current_power) ] = df.Number ** current_power\n",
    "    \n",
    "    coVariables.append('Number'+str(current_power))\n",
    "    \n",
    "    formula = 'Breeding ~ 0+' + '+'.join(coVariables)\n",
    "    \n",
    "    model = smf.ols( formula , data=df)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # LRT test\n",
    "    LRT = 2*(results.llf - ref_results.llf)\n",
    "    pval = 1-stats.chi2.cdf( LRT , 1 )\n",
    "    \n",
    "    print( '{:>20} -> pval: {:.3f}'.format(formula,pval) )\n",
    "    \n",
    "    if pval < threshold :\n",
    "        ## switch the reference \n",
    "        ref_model = formula\n",
    "        ref_results = results\n",
    "    \n",
    "    else:\n",
    "        break ## stop here\n",
    "\n",
    "print(\"Best model:\",ref_model)\n",
    "print(ref_results.summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Going further : interaction effects <a id='11'></a>\n",
    "\n",
    "So far the models we have created consider each co-variable effects independently. \n",
    "However it is legitimate to want to consider interactions effect between in certain cases.\n",
    "\n",
    "While you could add interaction effects yourselves by adding columns consisting of the multiplication of several covariables into your dataframe of covariables, we highly recommend you use the syntax offered by formulas. For instance,  a `*` or `:` instead of a `+` when specifying covariables will, respectively, add each variables and their interaction effect, or add only the interaction effect.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_diabetes \n",
    "\n",
    "model_noInteraction = smf.ols( 'disease_progression ~ bmi + sex' , data = df_diabetes )\n",
    "result_noInteraction = model_noInteraction.fit()\n",
    "print( result_noInteraction.summary() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes['sexI'] = (df_diabetes.sex > 0).astype(int)\n",
    "\n",
    "model_noInteraction = smf.ols( 'disease_progression ~ bmi * sexI' , data = df_diabetes )\n",
    "result_noInteraction = model_noInteraction.fit()\n",
    "print( result_noInteraction.summary() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes['sexI'] = (df_diabetes.sex > 0).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_introml2024)",
   "language": "python",
   "name": "conda_introml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
